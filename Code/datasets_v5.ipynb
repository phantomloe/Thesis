{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7067776c",
   "metadata": {},
   "source": [
    "# Table of contents \n",
    "- [Setup](#setup) \n",
    "    - [Purpose](#Purpose)\n",
    "    - [Libraries](#libraries)\n",
    "- [Ground truth URLs and sentences](#groundtruthURLsandsentences)\n",
    "- [URLs and sentences](#URLsandsentences)\n",
    "    - [Process URLs](#processURLs)\n",
    "    - [URLs in NeuroImage 2022 articles](#URLsinNeuroImage2022articles)\n",
    "- [Text classification using word embeddings](#textclassificationusingwordembeddings) \n",
    "    - [SciBERT](#scibert)\n",
    "    - [Class concepts and class labels](#classconceptsandclasslabels)\n",
    "        - [Word2Vec](#word2vec)\n",
    "        - [FastText](#fasttext)\n",
    "        - [GloVe](#glove)\n",
    "    - [Sentence classification](#sentenceclassification) \n",
    "- [Validate](#validate) \n",
    "- [Datasets](#datasets)\n",
    "- [References](#references) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af309d",
   "metadata": {},
   "source": [
    "<a name='setup'></a>\n",
    "# 0. Setup \n",
    "\n",
    "This notebook contains the code to extract the datasets used in the articles published in NeuroImage in 2022. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='purpose'></a> \n",
    "## 0.1. Purpose\n",
    "The purpose of this notebook is to locate and extract publicly available datasets used for analysis in the research articles published in NeuroImage in 2022. \n",
    "\n",
    "The overall steps are: \n",
    "\n",
    "- URLs and sentences: Use pypdf to locate and extract URLs and sentences containing URLs\n",
    "- Text classification using word embeddings: Use a fine-tuned Sci-BERT to identify the URLs linked to datasets.\n",
    "- \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='libraries'></a>\n",
    "## 0.2. Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json \n",
    "import os \n",
    "import re \n",
    "import io\n",
    "\n",
    "# Read PDFs\n",
    "import pypdf \n",
    "# Extract URLs from text \n",
    "import urlextract \n",
    "# Sentence - and thus URL - classification, and related imports \n",
    "import gensim.downloader as api\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_numeric, stem_text\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import torch\n",
    "# Random \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321e92f",
   "metadata": {},
   "source": [
    "# 1. Ground truth URLs and sentences \n",
    "<a name = 'groundtruthURLsandsentences'></a>\n",
    "\n",
    "Based on my exploration of ten randomly picked articles, 75% of the articles contained URLs - of the articles that did not contain any URLs, the majority either used self-collected data or no datasets at all. This means that extracting the URLs will also extract the datasets used for analysis in the article. \n",
    "\n",
    "\n",
    "I will test the functions using the groundtruth texts as my validation set. When manually extracting the datasets from the ten groundtruth texts, we should get the following datasets (NB! Currently, I have not distinguished between links that leads the reader to data and links that leads the reader to code - this will come later): \n",
    "\n",
    "\n",
    "Similar to my processing, I will perform the following manually: \n",
    "- For each article, save only unique URLs (i.e., if the same URL is mentioned more than one time, save all the sentences in one list) \n",
    "- Remove the following URLs: \n",
    "    - 'www.elsevier.com/locate/neuroimage'\n",
    "    - URLs containing the DOI of the article\n",
    "    - Creative Commons licenses\n",
    "- Columns: \n",
    "    - DOI: The article's DOI\n",
    "    - URL: The URL\n",
    "    - Sentence: The sentence(s) in which the URL appears. The only cleaning I did included removing extra spaces, e.g., 'under- lay' is changed to 'under-lay'. \n",
    "    - Data: True if the URL and sentence(s) point to and describe a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2ef338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of groundtruth DOI values to filter \n",
    "groundtruth_dois = [\n",
    "    '10.1016/j.neuroimage.2021.118839',\n",
    "    '10.1016/j.neuroimage.2021.118854',\n",
    "    '10.1016/j.neuroimage.2022.119030',\n",
    "    '10.1016/j.neuroimage.2022.119050',\n",
    "    '10.1016/j.neuroimage.2022.119240',\n",
    "    '10.1016/j.neuroimage.2022.119443',\n",
    "    '10.1016/j.neuroimage.2022.119526',\n",
    "    '10.1016/j.neuroimage.2022.119549',\n",
    "    '10.1016/j.neuroimage.2022.119646',\n",
    "    '10.1016/j.neuroimage.2022.119676',\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3a80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_urls = [\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2021.118839',\n",
    "        'URL': 'http://neuroimage.usc.edu/brainstorm',\n",
    "        'Sentence': ['Subsequently the results were loaded in a Matlab Tool Box, Brainstorm (Tadel et al. 2011), an accredited software freely available for download online under the GNU general public license (http://neuroimage.usc.edu/brainstorm).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2021.118854',\n",
    "        'URL': 'https://www.humanconnectome.org/study/hcp-young-adult/data-releases',\n",
    "        'Sentence': ['We applied our GFA extension to the publicly available resting-state functional MRI (rs-fMRI) and non-imaging measures (e.g., demograph-ics, psychometrics and other behavioural measures) obtained from 1003 subjects (only these had rs-fMRI data available) of the 1200-subject data release of the HCP (https://www.humanconnectome.org/study/hcp-young-adult/data-releases).'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2021.118854',\n",
    "        'URL': 'https://www.humanconnectome.org/study/hcp-young-adult/document/extensively-processed-fmri-data-documentation',\n",
    "        'Sentence': ['The data used in this study was downloaded from the Human Connectome Project website (https://www.humanconnectome.org/study/hcp-young-adult/document/extensively-processed-fmri-data-documentation).'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2021.118854',\n",
    "        'URL': 'https://github.com/ferreirafabio80/gfa',\n",
    "        'Sentence': ['The GFA models and experiments were implemented in Python 3.9.1 and are available here: https://github.com/ferreirafabio80/gfa.'],\n",
    "        'Label': 'Model'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119030',\n",
    "        'URL': 'marmosetbrainconnectome.org',\n",
    "        'Sentence': ['To accelerate such progress, we present the Marmoset Functional Brain Connectivity Resource (marmosetbrainconnectome.org), currently consisting of over 70 h of resting-state fMRI (RS-fMRI) data acquired at 500 μm isotropic resolution from 31 fully awake marmosets in a common stereotactic space.', 'To promote progress in understanding the functional organization of the marmoset brain, we present a resource that allows for online viewing and download of three-dimensional functional connectivity (FC) maps from over 70 h of RS-fMRI collected at ultra-high field from 31 fully awake adult marmosets: marmosetbrainconnectome.org.', 'A resampled ver-sion of this atlas (at 100 μm) allows for additional anatomical detail over the in vivo template but will still load sufficiently fast as an under-lay image on marmosetbrainconnectome.org.', 'Features of the web portal: marmosetbrainconnectome.org.', 'The Marmoset Functional Connectivity Resource is publicly accessi-ble at marmosetbrainconnectome.org.', 'This resource allows users to instantaneously view and use FC topologies from any gray matter voxel in the marmoset brain online (marmosetbrainconnectome.org; Fig. 1), offering a fine-grained (500 μm) insight into how the marmoset brain is functionally con-nected in any given region, utterly agnostic to structural nomenclature.', 'Tracer maps (B & E) were downloaded from marmosetbrain.org, and FC maps (A through H) were generated from marmosetbrainconnectome.org.', 'With all the publicly available data demo-graphics, users can also download specific demographics (e.g., heavy males in late life) to address their research questions. Individual-level topologies can be loaded via the marmosetbrainconnectome.org viewer without any analysis.'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119030',\n",
    "        'URL': 'https://www.marmosetbrainconnectome.org/download.html',\n",
    "        'Sentence': ['(B) The data download page (https://www.marmosetbrainconnectome.org/download.html) allows the user to download all raw (BIDS standard formated) (Gorgolewski et al., 2016) and pre-processed data.', 'Directing to https://www.marmosetbrainconnectome.org/download.html allows for download of the “raw” structural and functional images (3D Neu-roimaging Informatics Technology Initiative (NIfTI) format) contribut-ing to the FC maps shown in the resource – for convenience, these data are in a standard format (BIDS) (Gorgolewski et al., 2016).', 'All raw and preprocessed data are openly available for download at: https://www.marmosetbrainconnectome.org/download.html'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119030',\n",
    "        'URL': 'https://rii-mango.github.io/Papaya/',\n",
    "        'Sentence': ['The resource makes use of the Papaya viewer (https://rii-mango.github.io/Papaya/), with several additional features (illustrated in Fig. 1C & D), including (1) calculation of surface over-lay maps on-demand based on the threshold chosen in volume space, (2) the ability to display atlas borders in surface space, (3) support for rotating the underlying volume, overlaying functional connectiv-ity map, and atlas boundaries together – such obliquing of the images can be of utility for presurgical planning, and (4) the ability to choose between group- and subject-level topologies.'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119030',\n",
    "        'URL': 'https://gitlab.com/cfmm/marmoset',\n",
    "        'Sentence': ['The development of the Marmoset Functional Connectivity Resource is described in full detail at https://gitlab.com/cfmm/marmoset.', 'All code for the online viewer is available at: https://gitlab.com/cfmm/marmoset'],\n",
    "        'Label': 'Code'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119030',\n",
    "        'URL': 'https://gitlab.com/cfmm/marmoset-connectivity',\n",
    "        'Sentence': ['Users can also download all code used to generate the functional connectivity maps from https://gitlab.com/cfmm/marmoset-connectivity.', 'A 3D printed model is shown in stereotactic position (with skull cut away to expose the cortical surface) to demonstrate targeting based on the resource coordinates. (D) in method 2, the user employs the supplied code (downloaded from https://gitlab.com/cfmm/marmoset-connectivity) to transform the FC map to their native animals’ anatomical MRI space.', 'All code used for processing data is openly available at: https://gitlab.com/cfmm/marmoset-connectivity'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119030',\n",
    "        'URL': 'marmosetbrain.org',\n",
    "        'Sentence': ['Explicitly, we focused on a tracer map from an area 46 injection (left; CJ801-DY; marmosetbrain.org for notes on these injections).', 'To demonstrate the additional information offered by our resource, we systematically plotted connectivity across (within) area TE3 for com-parison with available tracer injections within that region (left; CJ180- CTBr and CJ180-DY; marmosetbrain.org for notes on these injections) (Majka et al., 2020).', 'As shown in Fig. 9, we systematically plotted connectivity across (within) area TE3 and compared available tracer injections within that region (left; CJ180-CTBr and CJ180-DY; marmosetbrain.org for notes on these injections) (Majka et al., 2020).', 'Green labeled ROIs indicate FC data, whereas purple labeled ROIs show where tracer data is publicly available within area TE3 from marmosetbrain.org', 'Accordingly, these re-sources can readily perform similar comparisons in any circuitry of inter-est Fig. 9. shows an example of how our brain-wide functional connec-tivity data can complement existing resources (e.g., marmosetbrain.org) (Majka et al., 2020), demonstrating a gradient of connectivity between cortical tracer injection sites.'],\n",
    "        'Label': 'Atlas/map'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119050',\n",
    "        'URL': 'http://audition.ens.fr/adc/NoiseTools/',\n",
    "        'Sentence': ['EEG analysis used FieldTrip (Oostenveld et al., 2011), Noise-Tools (De Cheveigne and Parra, 2014; http://audition.ens.fr/adc/NoiseTools/), and custom-written scripts in Matlab.'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119050',\n",
    "        'URL': 'zenodo.org',\n",
    "        'Sentence': ['Raw EEG data from all healthy individuals, as well as Matlab code, are publicly available on zenodo.org (doi:10.5281/zenodo.6110595).'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119240',\n",
    "        'URL': 'www.cni.stanford.edu',\n",
    "        'Sentence': ['MRI data were acquired on a 3T Discovery MR750 scanner (Gen-eral Electric Healthcare, Milwaukee, WI, USA) equipped with a 32-channel head coil (Nova Medical, Wilmington, MA, USA) at the Cen-ter for Cognitive and Neurobiological Imaging at Stanford Univer-sity (www.cni.stanford.edu).'],\n",
    "        'Label': 'Resource'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119240',\n",
    "        'URL': 'http://github.com/vistalab/vistasoft/mrDiffusion',\n",
    "        'Sentence': ['Diffusion weighted images were pre-processed with Vistasoft (http://github.com/vistalab/vistasoft/mrDiffusion), an open-source software package implemented in MATLAB R2012a (Mathworks, Natick, MA).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119240',\n",
    "        'URL': 'http://www.fil.ion.ucl.ac.uk/spm/',\n",
    "        'Sentence': ['Each diffusion weighted image was registered to the mean of the b=0 images and the mean b=0 image was registered automatically to the participant’s T1w image, using a rigid body transformation (imple-mented in SPM8, http://www.fil.ion.ucl.ac.uk/spm/; no warping was applied).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119240',\n",
    "        'URL': 'https://github.com/mezera/mrQ',\n",
    "        'Sentence': ['Quantitative T1 (relaxation time, seconds) maps were calculated us-ing mrQ, (https://github.com/mezera/mrQ), an open-source software package implemented in MATLAB R2012a (Mathworks, Natick, MA).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119240',\n",
    "        'URL': 'https://github.jyeatman/AFQ',\n",
    "        'Sentence': ['Automated Fiber Quantification (AFQ; https://github.jyeatman/ AFQ; (Yeatman, Dougherty, Myall, et al., 2012)), a software package implemented in MATLAB R2012a (Mathworks, Natick, MA), was used to isolate and characterize white matter metrics from three dorsal tracts (Arc-L and bilateral SLF) and four ventral white matter tracts (bilat-eral ILF and bilateral UF).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119443',\n",
    "        'URL': 'osf.io/gazx2/',\n",
    "        'Sentence': ['EEG datasets used to create the figure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/ and osf.io/bndjg/.'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119443',\n",
    "        'URL': 'osf.io/eucqf/',\n",
    "        'Sentence': ['EEG datasets used to create the figure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/ and osf.io/bndjg/.'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119443',\n",
    "        'URL': 'osf.io/thsqg/',\n",
    "        'Sentence': ['EEG datasets used to create the figure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/ and osf.io/bndjg/.'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119443',\n",
    "        'URL': 'osf.io/bndjg/',\n",
    "        'Sentence': ['EEG datasets used to create the figure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/ and osf.io/bndjg/.'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119443',\n",
    "        'URL': 'osf.io/guwnm/',\n",
    "        'Sentence': ['Code used to reproduce the plots in Fig. 1, as well as averaged ERP data, is available from osf.io/guwnm/.'],\n",
    "        'Label': 'Processed dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://db.humanconnectome.org/data/projects/HCP_1200',\n",
    "        'Sentence': ['200 unrelated subjects were selected from the Human Con-nectome Project (HCP) 1200 Subjects Data Release with avail-able resting (task-free) and task fMRI data from a 3T MRI scan-ner (https://db.humanconnectome.org/data/projects/HCP_1200).', 'Preprocessed task fMRI data for the four tasks from the HCP were analyzed (working memory, motor, language, emotion) (https://db.humanconnectome.org/data/projects/HCP_1200).'],\n",
    "        'Label': 'Dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://www.humanconnectome.org/study/hcp-young-adult/document/wu-minn-hcp-consortium-open-access-data-use-terms',\n",
    "        'Sentence': ['This study agreed to the Open Access Data Use Terms (https://www.humanconnectome.org/study/hcp-young-adult/document/wu-minn-hcp-consortium-open-access-data-use-terms) and was exempt from the UCSF IRB because investigators could not readily ascertain the identities of the individuals to whom the data belonged.'],\n",
    "        'Label': 'Resource'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/',\n",
    "        'Sentence': ['We used FSL (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/) and AFNI (https://afni.nimh.nih.gov/) for additional fMRI preprocessing.'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://afni.nimh.nih.gov/',\n",
    "        'Sentence': ['We used FSL (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/) and AFNI (https://afni.nimh.nih.gov/) for additional fMRI preprocessing.'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'http://www.brainnetome.org/',\n",
    "        'Sentence': ['Maps were averaged within 273 regions of interest by combining a parcella-tion of 210 cortical regions and 36 subcortical regions from the Brainnetome atlas (Fan et al., 2016) (http://www.brainnetome.org/) and 27 cerebellar regions from the SUIT atlas (Diedrichsen, 2006) (http://www.diedrichsenlab.org/imaging/suit.htm).'],\n",
    "        'Label': 'Atlas/map'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'http://www.diedrichsenlab.org/imaging/suit.htm',\n",
    "        'Sentence': ['Maps were averaged within 273 regions of interest by combining a parcella-tion of 210 cortical regions and 36 subcortical regions from the Brainnetome atlas (Fan et al., 2016) (http://www.brainnetome.org/) and 27 cerebellar regions from the SUIT atlas (Diedrichsen, 2006) (http://www.diedrichsenlab.org/imaging/suit.htm).'],\n",
    "        'Label': 'Atlas/map'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://www.fil.ion.ucl.ac.uk/spm/software/spm12/',\n",
    "        'Sentence': ['Task condition block regressors were convolved with a hemo-dynamic response function using the ‘spm_get_bf’ function in SPM12 (https://www.fil.ion.ucl.ac.uk/spm/software/spm12/).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://github.com/rmarkello/abagen',\n",
    "        'Sentence': ['We compared each gradient map to Allen Human Brain spatial gene expression patterns using the ‘abagen’ package (https://github.com/rmarkello/abagen) (Arnatkevici ̆ūtė et al., 2019; Hawrylycz et al., 2012).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://brainsmash.readthedocs.io/en/latest/',\n",
    "        'Sentence': ['These surrogate gradient maps were estimated using BrainSMASH (https://brainsmash.readthedocs.io/en/latest/).'],\n",
    "        'Label': 'Atlas/map'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://sites.google.com/site/bctnet/',\n",
    "        'Sentence': ['Graph the-ory analyses were run using the Brain Connectivity Toolbox (BCT; https://sites.google.com/site/bctnet/).'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'http://human.brain-map.org/',\n",
    "        'Sentence': ['Original data was obtained from the Human Connectome Project (1U54MH091657, PIs Van Essen and Ugurbil) and the Allen Hu-man Brain Atlas (http://human.brain-map.org/).'],\n",
    "        'Label': 'Atlas/map'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119526',\n",
    "        'URL': 'https://github.com/jbrown81/gradients',\n",
    "        'Sentence': ['All code (latent space derivation, dynamical system modeling, and gene expression corre-lation) and processed data (gradient maps/region weights, gradient timeseries, and region gene expression values) are available at https://github.com/jbrown81/gradients.', 'All code and processed data are available at https://github.com/jbrown81/gradients.'],\n",
    "        'Label': 'Processed dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119549',\n",
    "        'URL': np.nan,\n",
    "        'Sentence': np.nan,\n",
    "        'Label': np.nan\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119646',\n",
    "        'URL': np.nan,\n",
    "        'Sentence': np.nan,\n",
    "        'Label': np.nan\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119676',\n",
    "        'URL': 'https://www.shutterstock.com',\n",
    "        'Sentence': ['Both experiments employed static images (modified from Shutterstock, https://www.shutterstock.com).'],\n",
    "        'Label': 'Resource'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119676',\n",
    "        'URL': 'https://clippingmagic.com',\n",
    "        'Sentence': ['All image transformations were done with Clipping Magic (https://clippingmagic.com), ImageMagick, GIMP, Microsoft Paint, the MATLAB SHINE toolbox, and custom MATLAB code.'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119676',\n",
    "        'URL': 'http://www.nitrc.org/projects/jip',\n",
    "        'Sentence': ['Functional volumes were realigned and motion-corrected with the Statistical Parametric Mapping software (SPM12, RRID: SCR_007037), followed by non-rigid co-registration (using JIP, http://www.nitrc.org/projects/jip, RRID: SCR_009588) to the high-resolution anatomical template of the skull-stripped brain of each monkey.'],\n",
    "        'Label': 'Software'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119676',\n",
    "        'URL': 'https://caffe.berkeleyvision.org/model_zoo.html',\n",
    "        'Sentence': ['Another version of pre-trained AlexNet was im-ported from Caffe Model Zoo (https://caffe.berkeleyvision.org/model_zoo.html).'],\n",
    "        'Label': 'Model'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119676',\n",
    "        'URL': 'https://osf.io/b8pfa/?view_only=b6dbb5dd6a044989a7eecdc99facb43c',\n",
    "        'Sentence': ['Preprocessed fMRI data are available at https://osf.io/b8pfa/?view_only=b6dbb5dd6a044989a7eecdc99facb43c.'],\n",
    "        'Label': 'Preprocessed dataset'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119676',\n",
    "        'URL': 'https://github.com/Yozafirova/monkey-fMRI-codes',\n",
    "        'Sentence': ['Codes for the fMRI data analysis at https://github.com/Yozafirova/monkey-fMRI-codes and for the CNN data analysis at https://github.com/RajaniRaman/face_body_integration.'],\n",
    "        'Label': 'Analysis'\n",
    "    },\n",
    "    {\n",
    "        'DOI': '10.1016/j.neuroimage.2022.119676',\n",
    "        'URL': 'https://github.com/RajaniRaman/face_body_integration',\n",
    "        'Sentence': ['Codes for the fMRI data analysis at https://github.com/Yozafirova/monkey-fMRI-codes and for the CNN data analysis at https://github.com/RajaniRaman/face_body_integration.'],\n",
    "        'Label': 'Analysis'\n",
    "    },\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "manual_groundtruth_urls = pd.DataFrame(groundtruth_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19b2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the 'Data' directory\n",
    "data_dir = os.path.join(os.pardir, 'Data')\n",
    "\n",
    "# File path\n",
    "file_path = os.path.join(data_dir, 'articles_groundtruth_urls_and_sentences.csv')\n",
    "\n",
    "# Save the DataFrame to CSV, overwriting the file if it exists\n",
    "manual_groundtruth_urls.to_csv(file_path, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ff73e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOI         43\n",
       "URL         41\n",
       "Sentence    41\n",
       "Label       41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_groundtruth_urls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b477ddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 DOI  URL Sentence Label\n",
       "34  10.1016/j.neuroimage.2022.119549  NaN      NaN   NaN\n",
       "35  10.1016/j.neuroimage.2022.119646  NaN      NaN   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values in the 'URL' column\n",
    "manual_groundtruth_urls[manual_groundtruth_urls['URL'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b4daa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manual_groundtruth_urls[manual_groundtruth_urls['Label']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb6d1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOI\n",
       "10.1016/j.neuroimage.2022.119526    12\n",
       "10.1016/j.neuroimage.2022.119676     7\n",
       "10.1016/j.neuroimage.2022.119030     6\n",
       "10.1016/j.neuroimage.2022.119240     5\n",
       "10.1016/j.neuroimage.2022.119443     5\n",
       "10.1016/j.neuroimage.2021.118854     3\n",
       "10.1016/j.neuroimage.2022.119050     2\n",
       "10.1016/j.neuroimage.2021.118839     1\n",
       "10.1016/j.neuroimage.2022.119549     0\n",
       "10.1016/j.neuroimage.2022.119646     0\n",
       "Name: URL, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'DOI' and count the number of URLs, setting NaN counts to 0\n",
    "url_counts = manual_groundtruth_urls.groupby('DOI')['URL'].count().fillna(0)\n",
    "\n",
    "# Sort the counts in descending order\n",
    "url_counts = url_counts.sort_values(ascending=False)\n",
    "\n",
    "url_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d7cc2",
   "metadata": {},
   "source": [
    "A total of 41 links were extracted manually from the groundtruth articles. There are between one and twelve URLs in the articles. Two of the articles did not contain any URLs. 20 of the 41 links point of data/datasets, models pretrained with data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41159d0b",
   "metadata": {},
   "source": [
    "<a name='URLsandsentences'></a>\n",
    "# 2. URLs and sentences\n",
    "I use the work of Sourget (2023) to search the PDFs for their datasets: \n",
    "\n",
    "I use the Python library *urlextract* by Lipovský (2022) to extract the URLs. \n",
    "\n",
    "I perform some initial cleaning of the sentences extracted from the PDFs, specifically removing multiple spaces with a single space, removing all \\n characters, and remove leadning and trailing spaces after a number of special characters, incl. -, (, ), /, ., _ , and between : /.\n",
    "\n",
    "The functions: \n",
    "- *get_content* is losely interpreted from Soruget (2023) using the following breadcrumb in the github repository: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "- *clean_text* \n",
    "- *split_text_into_sentence* \n",
    "- *extract_links* uses the urlextract library (Lipovský 2022). \n",
    "- *get_urls_and_sentences* calls on *extract_links* and gets both URLs and sentences containing the URL. \n",
    "- *extract_and_transform_urls_from_dataframe* \n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "References: \n",
    "- Lipovský, J. (2022). urlextract: Collects and extracts URLs from given text. (1.8.0) [Python]. https://github.com/lipoja/URLExtract\n",
    "- Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657273d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(pdf_path, alt_pdf_path):\n",
    "    \"\"\"Get sentences that contain URLs. \n",
    "    This function is loosely interpreted from Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget\n",
    "    specifically: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "    \n",
    "    Parameters: \n",
    "    :param pdf_path (str): Path to the PDF file.\n",
    "    :param alt_pdf_path (str): Alternative path to the PDF file. \n",
    "    \n",
    "    Returns: \n",
    "    :return: Dataframe or 'Editorial board' if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_file = open(pdf_path, 'rb')\n",
    "        pdf_reader = pypdf.PdfReader(pdf_file)\n",
    "        pdf_text = \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "\n",
    "        # Extract sentences containing urls\n",
    "        df = get_urls_and_sentences(pdf_text)\n",
    "        pdf_file.close()\n",
    "        if df is not None:  # Check if a DataFrame is returned\n",
    "            return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            alternative_pdf_path = os.path.join(alt_pdf_path, os.path.basename(pdf_path))\n",
    "            pdf_file = open(alternative_pdf_path, 'rb')\n",
    "            return pd.DataFrame({\"url\": [np.nan], \"sentences\": [np.nan]})\n",
    "        except FileNotFoundError:\n",
    "            return pd.DataFrame({\"url\": [np.nan], \"sentences\": [np.nan]})\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    # If no URLs were found, return an empty DataFrame\n",
    "    return pd.DataFrame(columns=[\"url\", \"sentences\"])\n",
    "\n",
    "\n",
    "############### SENTENCES ################################################\n",
    "def clean_text(text): \n",
    "    \"\"\"This function performs a very simple initial cleaning of the extracted sentences. \n",
    "    This includes removing multiple spaces with a single space, removing all \\n characters, \n",
    "    and remove leadning and trailing spaces after a number of special characters, \n",
    "    incl. -, (, ), /, ., _ , and between : / \n",
    "    \"\"\"\n",
    "    return text.replace('   ', ' ').replace('  ', ' ').replace('\\n', '').replace('- ', '-').replace('( ', '(').replace(' )', ')').replace('/ ', '/').replace(' /', '/').replace(' .', '.').replace(': /', ':/').replace(' _ ', '_').replace(' _', '_').replace('_ ', '_') \n",
    "\n",
    "def get_sentences(text):\n",
    "    \"\"\"This function splits a given text into sentences based on a regular expression pattern. \n",
    "    It uses re.split() to identify sentence boundaries, considering common sentence-ending \n",
    "    punctuation like \".\", \"!\", or \"?\". It avoids splitting sentences if a digit immediately \n",
    "    follows the punctuation, e.g., 'Fig. 1'. \n",
    "    \n",
    "    Parameters: \n",
    "    :param text(str): \n",
    "    \n",
    "    Returns: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    sentence_pattern = r'(?<=[.!?])\\s+(?![0-9]+\\s)'\n",
    "    sentences = re.split(sentence_pattern, text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "############### LINKS ################################################\n",
    "def get_urls(text):\n",
    "    \"\"\"This function returns all unique urls in a text that have certain characters stripped from \n",
    "    the end of them (including ',', '.', and ')'). It uses the Python library URLExtract (Lipovský 2022).\n",
    "    \n",
    "    \"\"\"\n",
    "    # Instance of the URLExtract class\n",
    "    extractor = urlextract.URLExtract()\n",
    "    \n",
    "    # Create a set to store unique URLs\n",
    "    unique_urls = set()\n",
    "    \n",
    "    for url in extractor.gen_urls(text):\n",
    "        # Apply additional processing to the URL, e.g., removing characters at the end\n",
    "        processed_url = url.rstrip('.').rstrip(')').rstrip(',')\n",
    "        unique_urls.add(processed_url)\n",
    "    \n",
    "    # Convert the set back to a list if needed\n",
    "    unique_url_list = list(unique_urls)\n",
    "    \n",
    "    return unique_url_list\n",
    "\n",
    "def get_sentences_with_urls(sentences):\n",
    "    \"\"\"This function returns all text that contains URLs. It uses the Python library URLExtract (Lipovský 2022).\n",
    "    \"\"\"\n",
    "    # Instance of the URLExtract class\n",
    "    extractor = urlextract.URLExtract()\n",
    "    # Extract all sentences with URLs\n",
    "    sentences_with_urls = []\n",
    "    stop_processing = False  # Flag to stop processing when \"References\" is found\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if stop_processing:\n",
    "            break  # Stop processing when \"References\" is found\n",
    "        if extractor.has_urls(sentence):\n",
    "            sentences_with_urls.append(sentence)\n",
    "        if \"References\" in sentence:\n",
    "            stop_processing = True  # Set the flag to stop processing\n",
    "\n",
    "    return sentences_with_urls\n",
    "    \n",
    "def get_urls_and_sentences(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Lists to store the extracted URLs and their corresponding sentences\n",
    "    url_list = []\n",
    "    sentence_list = []\n",
    "    # Clean sentences \n",
    "    cleaned_text = clean_text(text)\n",
    "    # Extract links\n",
    "    links = get_urls(cleaned_text)\n",
    "    # Extract sentences \n",
    "    sentences = get_sentences(cleaned_text)\n",
    "    # Extract sentences with links \n",
    "    sentences_w_links = get_sentences_with_urls(sentences)\n",
    "\n",
    "    # Process each URL \n",
    "    for link in links:\n",
    "        sentences_for_url = [sentence for sentence in sentences_w_links if link in sentence]\n",
    "        if sentences_for_url:  # Only add the URL if there are associated sentences\n",
    "            url_list.append(link)\n",
    "            sentence_list.append(sentences_for_url)\n",
    "    \n",
    "    # If no URLs were found, return an empty DataFrame\n",
    "    if not url_list:\n",
    "        return pd.DataFrame(columns=[\"url\", \"sentences\"])\n",
    "\n",
    "    return pd.DataFrame({\"url\": url_list, \"sentences\": sentence_list})\n",
    "\n",
    "\n",
    "############### DATAFRAME WITH DOI, URL, SENTENCES ################################################\n",
    "def process_groundtruth_DOIs(groundtruth_dois, articles_directory, editorialboard_directory, json_file_path):\n",
    "    \"\"\"This function processes a list of DOIs, extracts urls and sentences from PDFs, \n",
    "    and create a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    :param groundtruth_dois (list): List of DOIs to process.\n",
    "    :param articles_directory (str): Path to the directory containing PDF articles.\n",
    "    :param editorialboard_directory (str): Path to the directory with editorial board articles.\n",
    "    :param json_file_path (str): Path to a JSON file.\n",
    "\n",
    "    Returns:\n",
    "    :return pd.DataFrame: A DataFrame containing processed information from the DOIs.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        for doi in groundtruth_dois:\n",
    "            doi_replaced = doi.replace('/', '.')\n",
    "            pdf_path = os.path.join(articles_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "            # Call the get_content function for each DOI\n",
    "            url_df = get_content(pdf_path, editorialboard_directory)\n",
    "\n",
    "            # Append the DOI to the URL DataFrame\n",
    "            url_df['DOI'] = doi\n",
    "            results_list.append(url_df)\n",
    "\n",
    "        \"\"\" TO PROCESS JUST ONE DOI: \n",
    "        doi = '10.1016/j.neuroimage.2022.119030'\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(articles_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        url_df = get_content(pdf_path, editorialboard_directory)\n",
    "        # Append the DOI to the URL DataFrame\n",
    "        url_df['DOI'] = doi\n",
    "        results_list.append(url_df)\n",
    "        \"\"\"\n",
    "            \n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    results_df = pd.concat(results_list, ignore_index=True)\n",
    "\n",
    "    # Rename the columns as needed\n",
    "    results_df.rename(columns={'url': 'URL', 'sentences': 'Sentences'}, inplace=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def process_DOIs(articles_directory, editorialboard_directory, json_file_path):\n",
    "    \"\"\"This function processes a list of DOIs, extracts urls and sentences from PDFs, \n",
    "    and create a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    :param articles_directory (str): Path to the directory containing PDF articles.\n",
    "    :param editorialboard_directory (str): Path to the directory with editorial board articles.\n",
    "    :param json_file_path (str): Path to a JSON file.\n",
    "\n",
    "    Returns:\n",
    "    :return pd.DataFrame: A DataFrame containing processed information from the DOIs.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        doi_data = json.load(json_file)\n",
    "        for doi in doi_data['DOIs']:\n",
    "            doi_replaced = doi.replace('/', '.')\n",
    "            pdf_path = os.path.join(articles_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "            # Call the get_content function for each DOI\n",
    "            url_df = get_content(pdf_path, editorialboard_directory)\n",
    "\n",
    "            # Append the DOI to the URL DataFrame\n",
    "            url_df['DOI'] = doi\n",
    "            results_list.append(url_df)\n",
    "            \n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    results_df = pd.concat(results_list, ignore_index=True)\n",
    "\n",
    "    # Rename the columns as needed\n",
    "    results_df.rename(columns={'url': 'URL', 'sentences': 'Sentences'}, inplace=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ecce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing PDFs\n",
    "articles_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_articles_doi/'\n",
    "editorialboard_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_editorialboard_doi/'\n",
    "\n",
    "# Path to the JSON file containing DOI values\n",
    "json_file_path = '../Data/ElsevierAPI/downloadedPDFs_info.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e42c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_groundtruth_df = process_groundtruth_DOIs(groundtruth_dois, articles_directory, editorialboard_directory, json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f2c6749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://doi.org/10.1016/j.neuroimage.2021.118839</td>\n",
       "      <td>[Corticospinal projections has been shown also...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 248 (2022) 118839 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://neuroimage.usc.edu/brainstorm</td>\n",
       "      <td>[2011), an accredited software freely availabl...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://github.com/Yozaﬁrova/monkey-fMRI-codes</td>\n",
       "      <td>[Codes for the fMRI data analysis at https://g...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://github.com/RajaniRaman/face_body_integ...</td>\n",
       "      <td>[Codes for the fMRI data analysis at https://g...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://clippingmagic.com</td>\n",
       "      <td>[NeuroImage 264 (2022) 119676 All image transf...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://caﬀe.berkeleyvision.org/model_zoo.html</td>\n",
       "      <td>[Another version of pre-trained AlexNet was im...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 264 (2022) 119676 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "0    https://doi.org/10.1016/j.neuroimage.2021.118839   \n",
       "1                  www.elsevier.com/locate/neuroimage   \n",
       "2   http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "3                http://neuroimage.usc.edu/brainstorm   \n",
       "4         http://creativecommons.org/licenses/by/4.0/   \n",
       "..                                                ...   \n",
       "65     https://github.com/Yozaﬁrova/monkey-fMRI-codes   \n",
       "66  https://github.com/RajaniRaman/face_body_integ...   \n",
       "67                          https://clippingmagic.com   \n",
       "68     https://caﬀe.berkeleyvision.org/model_zoo.html   \n",
       "69                 www.elsevier.com/locate/neuroimage   \n",
       "\n",
       "                                            Sentences  \\\n",
       "0   [Corticospinal projections has been shown also...   \n",
       "1   [NeuroImage 248 (2022) 118839 Contents lists a...   \n",
       "2   [This is an open access article under the CC B...   \n",
       "3   [2011), an accredited software freely availabl...   \n",
       "4   [This is an open access article under the CC B...   \n",
       "..                                                ...   \n",
       "65  [Codes for the fMRI data analysis at https://g...   \n",
       "66  [Codes for the fMRI data analysis at https://g...   \n",
       "67  [NeuroImage 264 (2022) 119676 All image transf...   \n",
       "68  [Another version of pre-trained AlexNet was im...   \n",
       "69  [NeuroImage 264 (2022) 119676 Contents lists a...   \n",
       "\n",
       "                                 DOI  \n",
       "0   10.1016/j.neuroimage.2021.118839  \n",
       "1   10.1016/j.neuroimage.2021.118839  \n",
       "2   10.1016/j.neuroimage.2021.118839  \n",
       "3   10.1016/j.neuroimage.2021.118839  \n",
       "4   10.1016/j.neuroimage.2021.118854  \n",
       "..                               ...  \n",
       "65  10.1016/j.neuroimage.2022.119676  \n",
       "66  10.1016/j.neuroimage.2022.119676  \n",
       "67  10.1016/j.neuroimage.2022.119676  \n",
       "68  10.1016/j.neuroimage.2022.119676  \n",
       "69  10.1016/j.neuroimage.2022.119676  \n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automatic_groundtruth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279facf2",
   "metadata": {},
   "source": [
    "Brief exploration of the URLs extracted from the groundtruth articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008da580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL          70\n",
       "Sentences    70\n",
       "DOI          70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automatic_groundtruth_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aab89dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(automatic_groundtruth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c493c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1016/j.neuroimage.2022.119526    15\n",
      "10.1016/j.neuroimage.2022.119676    10\n",
      "10.1016/j.neuroimage.2022.119030     9\n",
      "10.1016/j.neuroimage.2022.119443     8\n",
      "10.1016/j.neuroimage.2022.119240     7\n",
      "10.1016/j.neuroimage.2021.118854     6\n",
      "10.1016/j.neuroimage.2022.119050     5\n",
      "10.1016/j.neuroimage.2021.118839     4\n",
      "10.1016/j.neuroimage.2022.119549     3\n",
      "10.1016/j.neuroimage.2022.119646     3\n",
      "Name: DOI, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# How many URLs are saved per DOI\n",
    "doi_counts = automatic_groundtruth_df['DOI'].value_counts()\n",
    "\n",
    "# Print the number of rows for each unique DOI\n",
    "print(doi_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af1fdfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with links to Creative Commons license: 10\n"
     ]
    }
   ],
   "source": [
    "# Count the rows with URLs containing 'creativecommons.org'\n",
    "cc_license_count = len(automatic_groundtruth_df[automatic_groundtruth_df['URL'].str.contains('creativecommons.org')])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of rows with links to Creative Commons license: {cc_license_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd513f44",
   "metadata": {},
   "source": [
    "## 2.1. Process URLs\n",
    "<a name = 'processURLs'></a>\n",
    "\n",
    "Before this point, I already performed a few preprocessing steps of the URLs: \n",
    "- in *get_urls*, I returned only unique URLs. \n",
    "- in *process_url*, I stripped URLs of the characters '.' and ')', if they were at the end of the link. \n",
    "\n",
    "At this point, I have only unique URLs for each DOI. But I want to remove some URLs that I know do not point to datasets. As such, this processing step is: \n",
    "* Remove URLs: \n",
    "    * 'www.elsevier.com/locate/neuroimage' - this link is placed outside of the article's text. \n",
    "    * URLs containing the DOI of the article - this link is placed outside of the article's text. \n",
    "    * Creative Commons licenses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064b80f",
   "metadata": {},
   "source": [
    "I want to check any links that are common between the articles to see if there are some NeuroImage or Elsevier specific links that can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fd7a063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 248 (2022) 118839 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 249 (2022) 118854 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 252 (2022) 119030 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 253 (2022) 119050 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 256 (2022) 119240 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 259 (2022) 119443 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 261 (2022) 119526 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 262 (2022) 119549 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 263 (2022) 119646 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 264 (2022) 119676 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "1                  www.elsevier.com/locate/neuroimage   \n",
       "2   http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "4         http://creativecommons.org/licenses/by/4.0/   \n",
       "7                  www.elsevier.com/locate/neuroimage   \n",
       "15  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "18                 www.elsevier.com/locate/neuroimage   \n",
       "21  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "23                 www.elsevier.com/locate/neuroimage   \n",
       "26  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "28                 www.elsevier.com/locate/neuroimage   \n",
       "34        http://creativecommons.org/licenses/by/4.0/   \n",
       "38                 www.elsevier.com/locate/neuroimage   \n",
       "41  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "52                 www.elsevier.com/locate/neuroimage   \n",
       "54  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "55                 www.elsevier.com/locate/neuroimage   \n",
       "57                 www.elsevier.com/locate/neuroimage   \n",
       "58        http://creativecommons.org/licenses/by/4.0/   \n",
       "63  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "69                 www.elsevier.com/locate/neuroimage   \n",
       "\n",
       "                                            Sentences  \\\n",
       "1   [NeuroImage 248 (2022) 118839 Contents lists a...   \n",
       "2   [This is an open access article under the CC B...   \n",
       "4   [This is an open access article under the CC B...   \n",
       "7   [NeuroImage 249 (2022) 118854 Contents lists a...   \n",
       "15  [This is an open access article under the CC B...   \n",
       "18  [NeuroImage 252 (2022) 119030 Contents lists a...   \n",
       "21  [This is an open access article under the CC B...   \n",
       "23  [NeuroImage 253 (2022) 119050 Contents lists a...   \n",
       "26  [This is an open access article under the CC B...   \n",
       "28  [NeuroImage 256 (2022) 119240 Contents lists a...   \n",
       "34  [This is an open access article under the CC B...   \n",
       "38  [NeuroImage 259 (2022) 119443 Contents lists a...   \n",
       "41  [This is an open access article under the CC B...   \n",
       "52  [NeuroImage 261 (2022) 119526 Contents lists a...   \n",
       "54  [This is an open access article under the CC B...   \n",
       "55  [NeuroImage 262 (2022) 119549 Contents lists a...   \n",
       "57  [NeuroImage 263 (2022) 119646 Contents lists a...   \n",
       "58  [This is an open access article under the CC B...   \n",
       "63  [This is an open access article under the CC B...   \n",
       "69  [NeuroImage 264 (2022) 119676 Contents lists a...   \n",
       "\n",
       "                                 DOI  \n",
       "1   10.1016/j.neuroimage.2021.118839  \n",
       "2   10.1016/j.neuroimage.2021.118839  \n",
       "4   10.1016/j.neuroimage.2021.118854  \n",
       "7   10.1016/j.neuroimage.2021.118854  \n",
       "15  10.1016/j.neuroimage.2022.119030  \n",
       "18  10.1016/j.neuroimage.2022.119030  \n",
       "21  10.1016/j.neuroimage.2022.119050  \n",
       "23  10.1016/j.neuroimage.2022.119050  \n",
       "26  10.1016/j.neuroimage.2022.119240  \n",
       "28  10.1016/j.neuroimage.2022.119240  \n",
       "34  10.1016/j.neuroimage.2022.119443  \n",
       "38  10.1016/j.neuroimage.2022.119443  \n",
       "41  10.1016/j.neuroimage.2022.119526  \n",
       "52  10.1016/j.neuroimage.2022.119526  \n",
       "54  10.1016/j.neuroimage.2022.119549  \n",
       "55  10.1016/j.neuroimage.2022.119549  \n",
       "57  10.1016/j.neuroimage.2022.119646  \n",
       "58  10.1016/j.neuroimage.2022.119646  \n",
       "63  10.1016/j.neuroimage.2022.119676  \n",
       "69  10.1016/j.neuroimage.2022.119676  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the rows with duplicate URLs, indicating which DOIs share the same URL. \n",
    "# Check the column 'url' for duplicates using subset='url' and keep=False keeps all occurrences of the duplicates.\n",
    "duplicate_urls = automatic_groundtruth_df[automatic_groundtruth_df.duplicated(subset='URL', keep=False)]\n",
    "\n",
    "# Print the rows with duplicate URLs\n",
    "duplicate_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f126047",
   "metadata": {},
   "source": [
    "Remove links: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "904a8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls(df, urls_to_remove):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    # Collect unique DOIs \n",
    "    unique_dois = filtered_df['DOI'].unique()\n",
    "     # Ensure 'URL' and 'DOI' columns are of string type\n",
    "    filtered_df['URL'] = filtered_df['URL'].astype(str)\n",
    "    filtered_df['DOI'] = filtered_df['DOI'].astype(str)\n",
    "    \n",
    "    # Remove URLs from list of URLs to remove \n",
    "    filtered_df = filtered_df[~filtered_df['URL'].isin(urls_to_remove)]\n",
    "\n",
    "    # Remove URLs referring to the Creative Commons license \n",
    "    filtered_df = filtered_df[~filtered_df['URL'].str.contains('creativecommons.org')]\n",
    "    \n",
    "    # Remove URLs containing the DOI \n",
    "    for doi in filtered_df['DOI'].unique():\n",
    "        filtered_df = filtered_df[~filtered_df['URL'].str.contains('doi')]\n",
    "    \n",
    "    # Check if all unique DOIs are still present, if not, add them with NaN values\n",
    "    missing_dois = set(unique_dois) - set(filtered_df['DOI'].unique())\n",
    "    for missing_doi in missing_dois:\n",
    "        filtered_df = filtered_df.append({'DOI': missing_doi, 'URL': np.nan, 'Sentences': np.nan}, ignore_index=True)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03310893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hg/yk6m9jqn41l9s2x1tfjqc9900000gn/T/ipykernel_64037/3397798183.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  filtered_df = filtered_df.append({'DOI': missing_doi, 'URL': np.nan, 'Sentences': np.nan}, ignore_index=True)\n",
      "/var/folders/hg/yk6m9jqn41l9s2x1tfjqc9900000gn/T/ipykernel_64037/3397798183.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  filtered_df = filtered_df.append({'DOI': missing_doi, 'URL': np.nan, 'Sentences': np.nan}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# URLs to remove\n",
    "urls_to_remove = [\n",
    "    'www.elsevier.com/locate/neuroimage',  # URL to remove\n",
    "]\n",
    "\n",
    "automatic_groundtruth_urls = filter_urls(automatic_groundtruth_df, urls_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35da4704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL          40\n",
       "Sentences    40\n",
       "DOI          42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automatic_groundtruth_urls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82856d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(automatic_groundtruth_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbdccd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOI\n",
       "10.1016/j.neuroimage.2022.119526    12\n",
       "10.1016/j.neuroimage.2022.119676     7\n",
       "10.1016/j.neuroimage.2022.119030     6\n",
       "10.1016/j.neuroimage.2022.119443     5\n",
       "10.1016/j.neuroimage.2022.119240     4\n",
       "10.1016/j.neuroimage.2021.118854     3\n",
       "10.1016/j.neuroimage.2022.119050     2\n",
       "10.1016/j.neuroimage.2021.118839     1\n",
       "10.1016/j.neuroimage.2022.119549     0\n",
       "10.1016/j.neuroimage.2022.119646     0\n",
       "Name: URL, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many URLs are saved per DOI\n",
    "doi_counts = automatic_groundtruth_urls.groupby('DOI')['URL'].count().fillna(0) \n",
    "\n",
    "doi_counts = doi_counts.sort_values(ascending=False)\n",
    "\n",
    "doi_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bb05c",
   "metadata": {},
   "source": [
    "The automatic URL extraction found a total of 40 links across the ten articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c90af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average URLs per DOI: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average number of URLs per DOI\n",
    "average_urls_per_doi = doi_counts.mean()\n",
    "print(\"Average URLs per DOI:\", average_urls_per_doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e215e",
   "metadata": {},
   "source": [
    "The counts don't fully match up with my manual exploration. \n",
    "- 10.1016/j.neuroimage.2022.119240 has one less than my manual count, specifically the link: 'https://github.jyeatman/AFQ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b196663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "      <td>www.cni.stanford.edu</td>\n",
       "      <td>[MRI data were acquired on a 3T Discovery MR75...</td>\n",
       "      <td>Resource</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "      <td>http://github.com/vistalab/vistasoft/mrDiffusion</td>\n",
       "      <td>[Diffusion weighted images were pre-processed ...</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "      <td>http://www.fil.ion.ucl.ac.uk/spm/</td>\n",
       "      <td>[Each diffusion weighted image was registered ...</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "      <td>https://github.com/mezera/mrQ</td>\n",
       "      <td>[Quantitative T1 (relaxation time, seconds) ma...</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "      <td>https://github.jyeatman/AFQ</td>\n",
       "      <td>[Automated Fiber Quantification (AFQ; https://...</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 DOI  \\\n",
       "12  10.1016/j.neuroimage.2022.119240   \n",
       "13  10.1016/j.neuroimage.2022.119240   \n",
       "14  10.1016/j.neuroimage.2022.119240   \n",
       "15  10.1016/j.neuroimage.2022.119240   \n",
       "16  10.1016/j.neuroimage.2022.119240   \n",
       "\n",
       "                                                 URL  \\\n",
       "12                              www.cni.stanford.edu   \n",
       "13  http://github.com/vistalab/vistasoft/mrDiffusion   \n",
       "14                 http://www.fil.ion.ucl.ac.uk/spm/   \n",
       "15                     https://github.com/mezera/mrQ   \n",
       "16                       https://github.jyeatman/AFQ   \n",
       "\n",
       "                                             Sentence     Label  \n",
       "12  [MRI data were acquired on a 3T Discovery MR75...  Resource  \n",
       "13  [Diffusion weighted images were pre-processed ...  Software  \n",
       "14  [Each diffusion weighted image was registered ...  Software  \n",
       "15  [Quantitative T1 (relaxation time, seconds) ma...  Software  \n",
       "16  [Automated Fiber Quantification (AFQ; https://...  Software  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_groundtruth_urls[manual_groundtruth_urls['DOI'] == '10.1016/j.neuroimage.2022.119240']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "836f011a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://github.com/vistalab/vistasoft/mrDiﬀusion</td>\n",
       "      <td>[Diﬀusion weighted images were pre-processed w...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://github.com/mezera/mrQ</td>\n",
       "      <td>[Quantitative T1 (relaxation time, seconds) ma...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://www.ﬁl.ion.ucl.ac.uk/spm/</td>\n",
       "      <td>[Each diﬀusion weighted image was registered t...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>www.cni.stanford.edu</td>\n",
       "      <td>[MRI data acquisition and processing MRI data ...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URL  \\\n",
       "12  http://github.com/vistalab/vistasoft/mrDiﬀusion   \n",
       "13                    https://github.com/mezera/mrQ   \n",
       "14                 http://www.ﬁl.ion.ucl.ac.uk/spm/   \n",
       "15                             www.cni.stanford.edu   \n",
       "\n",
       "                                            Sentences  \\\n",
       "12  [Diﬀusion weighted images were pre-processed w...   \n",
       "13  [Quantitative T1 (relaxation time, seconds) ma...   \n",
       "14  [Each diﬀusion weighted image was registered t...   \n",
       "15  [MRI data acquisition and processing MRI data ...   \n",
       "\n",
       "                                 DOI  \n",
       "12  10.1016/j.neuroimage.2022.119240  \n",
       "13  10.1016/j.neuroimage.2022.119240  \n",
       "14  10.1016/j.neuroimage.2022.119240  \n",
       "15  10.1016/j.neuroimage.2022.119240  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automatic_groundtruth_urls[automatic_groundtruth_urls['DOI'] == '10.1016/j.neuroimage.2022.119240']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8cd93",
   "metadata": {},
   "source": [
    "Looking at the links before filtering them, the link was not caught: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3676ccf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>http://github.com/vistalab/vistasoft/mrDiﬀusion</td>\n",
       "      <td>[Diﬀusion weighted images were pre-processed w...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://github.com/mezera/mrQ</td>\n",
       "      <td>[Quantitative T1 (relaxation time, seconds) ma...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/4.0/</td>\n",
       "      <td>[This is an open access article under the CC B...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>http://www.ﬁl.ion.ucl.ac.uk/spm/</td>\n",
       "      <td>[Each diﬀusion weighted image was registered t...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>www.elsevier.com/locate/neuroimage</td>\n",
       "      <td>[NeuroImage 256 (2022) 119240 Contents lists a...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>www.cni.stanford.edu</td>\n",
       "      <td>[MRI data acquisition and processing MRI data ...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://doi.org/10.1016/j.neuroimage.2022.119240</td>\n",
       "      <td>[The sample included FT and PT children across...</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "24    http://github.com/vistalab/vistasoft/mrDiﬀusion   \n",
       "25                      https://github.com/mezera/mrQ   \n",
       "26  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
       "27                   http://www.ﬁl.ion.ucl.ac.uk/spm/   \n",
       "28                 www.elsevier.com/locate/neuroimage   \n",
       "29                               www.cni.stanford.edu   \n",
       "30   https://doi.org/10.1016/j.neuroimage.2022.119240   \n",
       "\n",
       "                                            Sentences  \\\n",
       "24  [Diﬀusion weighted images were pre-processed w...   \n",
       "25  [Quantitative T1 (relaxation time, seconds) ma...   \n",
       "26  [This is an open access article under the CC B...   \n",
       "27  [Each diﬀusion weighted image was registered t...   \n",
       "28  [NeuroImage 256 (2022) 119240 Contents lists a...   \n",
       "29  [MRI data acquisition and processing MRI data ...   \n",
       "30  [The sample included FT and PT children across...   \n",
       "\n",
       "                                 DOI  \n",
       "24  10.1016/j.neuroimage.2022.119240  \n",
       "25  10.1016/j.neuroimage.2022.119240  \n",
       "26  10.1016/j.neuroimage.2022.119240  \n",
       "27  10.1016/j.neuroimage.2022.119240  \n",
       "28  10.1016/j.neuroimage.2022.119240  \n",
       "29  10.1016/j.neuroimage.2022.119240  \n",
       "30  10.1016/j.neuroimage.2022.119240  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automatic_groundtruth_df[automatic_groundtruth_df['DOI'] == '10.1016/j.neuroimage.2022.119240']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f1c61",
   "metadata": {},
   "source": [
    "Compared to the manual extraction, 39 of the 40 links were picked up on by the code. I will move on with the current code and extract all URLs from the entire corpus of NeuroImage 2022 articles. \n",
    "\n",
    "<a name='URLsinNeuroImage2022articles'></a>\n",
    "## 2.2. URLs in NeuroImage 2022 articles \n",
    "\n",
    "Some expectations based on the ground truth sample: \n",
    "- 4 URLs per article on average (ranges from 1 to 12)\n",
    "- Two out of ten articles will not contain any URLs (after filtering). \n",
    "    - 20 % of the articles seems quite high. \n",
    "    - 20% of 834 is 166.34. \n",
    "- With 834 articles total, based on the average: \n",
    "    - Before filtering: seven URLs per article on average (four average plus at least three URLs I filter out), i.e., 5.838 URLs \n",
    "    - After filtering: 3.336 URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "478fc2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing PDFs\n",
    "articles_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_articles_doi/'\n",
    "editorialboard_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_editorialboard_doi/'\n",
    "\n",
    "# Path to the JSON file containing DOI values\n",
    "json_file_path = '../Data/ElsevierAPI/downloadedPDFs_info.json'\n",
    "\n",
    "# URLs to remove\n",
    "urls_to_remove = [\n",
    "    'www.elsevier.com/locate/neuroimage',  # URL to remove\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ed9ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last run on October 29th\n",
    "# Extract all URLs in NeuroImage 2022 articles \n",
    "#df = process_DOIs(articles_directory, editorialboard_directory, json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed6663fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the extracted URLs\n",
    "#filtered_df = filter_urls(df, urls_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b8ef38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the URLs (filtered and unfiltered) to csv \n",
    "# The file path\n",
    "#path_all_urls = os.path.join(os.pardir, 'Data/articles_all_urls.csv')\n",
    "#path_filtered_urls = os.path.join(os.pardir, 'Data/articles_filtered_urls.csv')\n",
    "\n",
    "# Save the DataFrame to CSV, overwriting the file if it exists\n",
    "#df.to_csv(path_all_urls, index=False, mode='w')\n",
    "#filtered_df.to_csv(path_filtered_urls, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53700c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all_urls = os.path.join(os.pardir, 'Data/articles_all_urls.csv')\n",
    "path_filtered_urls = os.path.join(os.pardir, 'Data/articles_filtered_urls.csv')\n",
    "all_urls = pd.read_csv(path_all_urls)\n",
    "filtered_urls = pd.read_csv(path_filtered_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7ff3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the Editorial Board DOIs to exclude them \n",
    "path = os.path.join(os.pardir,'Data/ElsevierAPI/downloaded_pdfs/fulltext_editorialboard_doi')\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Extract the DOIs from the file names\n",
    "editorial_dois = [file.split('_')[0] for file in files]\n",
    "editorial_dois = list(editorial_dois)\n",
    "editorial_dois = [doi.replace('.pdf', '').replace('.S', '/S') for doi in editorial_dois]\n",
    "\n",
    "# Exclude the Editorial Board articles \n",
    "all_urls = all_urls[~all_urls['DOI'].isin(editorial_dois)]\n",
    "filtered_urls = filtered_urls[~filtered_urls['DOI'].isin(editorial_dois)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6194f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique DOIs:  815\n",
      "Extracted URLs (excl. NaN values):  5382\n",
      "Count of DOIs with NaN values in the 'URL' column: 0\n",
      "'www.elsevier.com/locate/neuroimage':  815\n",
      "creativecommons.org:  815\n",
      "doi:  891\n",
      "URLs to be filtered out:  2521\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique DOIs: \", len(all_urls['DOI'].unique()))\n",
    "print(\"Extracted URLs (excl. NaN values): \", len(all_urls[~all_urls['URL'].isna()]))\n",
    "count_of_nan_dois = all_urls['URL'].isna().sum()\n",
    "print(\"Count of DOIs with NaN values in the 'URL' column:\", count_of_nan_dois)\n",
    "\n",
    "urls_elsevier = len(all_urls[all_urls['URL'].str.contains('www.elsevier.com/locate/neuroimage')])\n",
    "urls_creativecommons = len(all_urls[all_urls['URL'].str.contains('creativecommons.org')])\n",
    "urls_doi = len(all_urls[all_urls['URL'].str.contains('doi')])\n",
    "print(\"'www.elsevier.com/locate/neuroimage': \", urls_elsevier)\n",
    "print('creativecommons.org: ', urls_creativecommons)\n",
    "print('doi: ', urls_doi)\n",
    "print(\"URLs to be filtered out: \", urls_elsevier+urls_creativecommons+urls_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09f9c8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [URL, Sentences, DOI]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls[all_urls['URL'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b74d3080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique DOIs:  815\n",
      "Extracted URLs (excl. NaN values):  2861\n",
      "Count of DOIs with NaN values in the 'URL' column: 122\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique DOIs: \", len(filtered_urls['DOI'].unique()))\n",
    "print(\"Extracted URLs (excl. NaN values): \", len(filtered_urls[~filtered_urls['URL'].isna()]))\n",
    "count_of_nan_dois = filtered_urls['URL'].isna().sum()\n",
    "print(\"Count of DOIs with NaN values in the 'URL' column:\", count_of_nan_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18abbfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.118982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2022.119678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL Sentences                               DOI\n",
       "2880  NaN       NaN  10.1016/j.neuroimage.2022.119207\n",
       "2881  NaN       NaN  10.1016/j.neuroimage.2022.119328\n",
       "2882  NaN       NaN  10.1016/j.neuroimage.2022.119643\n",
       "2883  NaN       NaN  10.1016/j.neuroimage.2021.118840\n",
       "2884  NaN       NaN  10.1016/j.neuroimage.2022.118982\n",
       "...   ...       ...                               ...\n",
       "2997  NaN       NaN  10.1016/j.neuroimage.2022.119406\n",
       "2998  NaN       NaN  10.1016/j.neuroimage.2022.119058\n",
       "2999  NaN       NaN  10.1016/j.neuroimage.2022.119633\n",
       "3000  NaN       NaN  10.1016/j.neuroimage.2022.119137\n",
       "3001  NaN       NaN  10.1016/j.neuroimage.2022.119678\n",
       "\n",
       "[122 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_urls[filtered_urls['URL'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15753f3c",
   "metadata": {},
   "source": [
    "There are a 122 articles that do not contain any URLs (excluding the 'Editorial Board' articles). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b8c96",
   "metadata": {},
   "source": [
    "<a name='textclassificationusingwordembeddings'></a>\n",
    "# 3. Text classification using word embeddings\n",
    "\n",
    "As my overarching goal is to identify and extract the datasets, I can classify the sentences that contain a URL based on the assumption that the sentences will somehow reflect what the URL contains. \n",
    "\n",
    "Inspired by the work of Halford (2020), I perform text classification using word embeddings (Birunda & Devi (2021), Kosar et al. (2022), Haj-Yahia et al. (2019)), but I use the pre-trained SciBERT model (Beltagy et al. 2019) and finetune it using my own manually labelled data. \n",
    "\n",
    "<a name='scibert'></a>\n",
    "## 3.1 Fine-tuning SciBERT \n",
    "\n",
    "SciBERT (Beltagy et al. 2019) is a BERT model (Devlin et al. 2019) that is trained on scientific papers from semanticscholar.org. They use the full text of the papers in training. I use Huggingface's Transformers (Wolf et al. 2020) and datasets (Lhoest et al. 2021) to download and finetune SciBert and classify my sentences. \n",
    "\n",
    "Together with three annotators, we manually labeled 129 sentences which will be used to fine-tune SciBERT. I use a function from scikit-learn (Pedregosa et al. 2011) to split my data into training and test sets. Upon fine-tuning the model, I can then classify the rest of the sentences to get a list of all sentences - and thereby URLs - that link to datasets. \n",
    "\n",
    "I follow one guide in particular: \n",
    "- Fine-tuning BERT (and friends) for multi-label text classification. (n.d.). Retrieved November 9, 2023, from https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=hiloh9eMK91o\n",
    "\n",
    "I also read and found the following useful: \n",
    "- Choudhary, R. (2021, December 29). Fine-Tuning Bert for Tweets Classification ft. Hugging Face. MLearning.Ai. https://medium.com/mlearning-ai/fine-tuning-bert-for-tweets-classification-ft-hugging-face-8afebadd5dbf\n",
    "- Text classification. (n.d.). 🤗 HuggingFace - Transformers. Retrieved October 29, 2023, from https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1364acbf-8536-430c-a183-e9593d422e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 19:24:27.345789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, EvalPrediction\n",
    "from datasets import load_metric, Dataset, DatasetDict \n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4a4db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '../Data/QA_manually_labeled_data'\n",
    "file_path = os.path.join(directory_path, 'labeled_data.csv')\n",
    "labeled_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7449f72-702c-4915-abdb-fc98adafbfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = labeled_data.rename(columns={\"Original_sentence\": \"sentence\", \"True_label\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f05a3268-4808-4ceb-98da-d47c97a7b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an ID column (assuming 'labeled_data' is a DataFrame)\n",
    "labeled_data['ID'] = labeled_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dc588e6-6cfe-4444-8e0f-d3b4e230c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels based on the 'label_mapping'\n",
    "label_mapping = {\n",
    "    \"Analysis\": 'ana',\n",
    "    \"Atlas/map\": 'at_map',\n",
    "    \"Dataset\": 'data',\n",
    "    \"Model\": 'mod',\n",
    "    \"Not a URL\": 'no_url',\n",
    "    \"Not enough information\": 'no_info',\n",
    "    \"Person or institution\": 'pers_inst',\n",
    "    \"Processed dataset\": 'pro_data',\n",
    "    \"Resource\": 'res',\n",
    "    \"Software, incl. plugins, toolbox, packages, and functions\": 'soft'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b119470-784e-4911-92f8-a3df3ff432e7",
   "metadata": {},
   "source": [
    "I need to one-hot encode the labels to ensure the model can read the multilabels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "161f5d59-9b73-4fe8-a678-795776e29103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels based on the 'label_mapping'\n",
    "for label_name, label_code in label_mapping.items():\n",
    "    labeled_data[label_code] = labeled_data['label'].apply(lambda x: int(label_code in x))\n",
    "\n",
    "# Drop the original 'label' column if needed\n",
    "labeled_data = labeled_data.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "432640cd-23d7-4170-a3b9-c672d32119b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ID</th>\n",
       "      <th>ana</th>\n",
       "      <th>at_map</th>\n",
       "      <th>data</th>\n",
       "      <th>mod</th>\n",
       "      <th>no_url</th>\n",
       "      <th>no_info</th>\n",
       "      <th>pers_inst</th>\n",
       "      <th>pro_data</th>\n",
       "      <th>res</th>\n",
       "      <th>soft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In addition, as introduced in Section 3.1.3.1 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vries, I.E.J.de, Driel, J.van, Olivers, C.N.L....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All electrode coordinates and labels were save...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wen, J., Thibeau-Sutre, E., Diaz-Melo, M., Sam...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>∗ Data used in preparation of this article wer...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Funding JD was funded by the Rennes Clinical N...</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>We used MRIcroGL (www.mccauslandcenter.sc.edu/...</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>The 3D ﬁgure was realized using BrainNet viewe...</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Seed-based d mapping The SDM-PSI (www.sdmproje...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>fMRI analysis and ROI selection The fMRI data ...</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence   ID  ana  at_map  \\\n",
       "0    In addition, as introduced in Section 3.1.3.1 ...    0    0       0   \n",
       "1    Vries, I.E.J.de, Driel, J.van, Olivers, C.N.L....    1    0       0   \n",
       "2    All electrode coordinates and labels were save...    2    0       0   \n",
       "3    Wen, J., Thibeau-Sutre, E., Diaz-Melo, M., Sam...    3    0       0   \n",
       "4    ∗ Data used in preparation of this article wer...    4    0       0   \n",
       "..                                                 ...  ...  ...     ...   \n",
       "117  Funding JD was funded by the Rennes Clinical N...  117    0       0   \n",
       "118  We used MRIcroGL (www.mccauslandcenter.sc.edu/...  118    0       0   \n",
       "119  The 3D ﬁgure was realized using BrainNet viewe...  119    0       0   \n",
       "120  Seed-based d mapping The SDM-PSI (www.sdmproje...  120    0       0   \n",
       "121  fMRI analysis and ROI selection The fMRI data ...  121    0       0   \n",
       "\n",
       "     data  mod  no_url  no_info  pers_inst  pro_data  res  soft  \n",
       "0       0    0       1        0          0         0    0     0  \n",
       "1       0    0       1        0          0         0    0     0  \n",
       "2       0    0       0        0          0         0    1     0  \n",
       "3       0    0       1        0          0         0    0     0  \n",
       "4       1    0       0        0          0         0    0     0  \n",
       "..    ...  ...     ...      ...        ...       ...  ...   ...  \n",
       "117     0    0       0        0          1         0    0     0  \n",
       "118     0    0       0        0          0         0    0     1  \n",
       "119     0    0       0        0          0         0    0     1  \n",
       "120     0    0       0        0          0         0    0     1  \n",
       "121     0    0       0        0          0         0    0     1  \n",
       "\n",
       "[122 rows x 12 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "379d8636-5581-4fe1-9757-bfaed933ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming labeled_data contains your dataset\n",
    "X = labeled_data[['ID', 'sentence']]\n",
    "y = labeled_data[['ana', 'at_map', 'data', 'mod', 'no_url', 'no_info', 'pers_inst', 'pro_data', 'res', 'soft']]\n",
    "\n",
    "# Split your data into training, test, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reset the index before concatenation\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_val.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_val.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = Dataset.from_pandas(pd.concat([X_train, y_train], axis=1))\n",
    "val_dataset = Dataset.from_pandas(pd.concat([X_val, y_val], axis=1))\n",
    "test_dataset = Dataset.from_pandas(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "# Combine the datasets into a DatasetDict\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'validation': val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd015916-6d8c-4dfd-8490-90c469185372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'sentence', 'ana', 'at_map', 'data', 'mod', 'no_url', 'no_info', 'pers_inst', 'pro_data', 'res', 'soft'],\n",
       "        num_rows: 97\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'sentence', 'ana', 'at_map', 'data', 'mod', 'no_url', 'no_info', 'pers_inst', 'pro_data', 'res', 'soft'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'sentence', 'ana', 'at_map', 'data', 'mod', 'no_url', 'no_info', 'pers_inst', 'pro_data', 'res', 'soft'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ddf2ca6-81b1-4f0e-be06-1dd49160a8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ana',\n",
       " 'at_map',\n",
       " 'data',\n",
       " 'mod',\n",
       " 'no_url',\n",
       " 'no_info',\n",
       " 'pers_inst',\n",
       " 'pro_data',\n",
       " 'res',\n",
       " 'soft']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'sentence', 'label']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e65b2de7-ccae-41cf-895e-c9dd75abdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SciBERT with uncased scivocab \n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ddec4c8-c5ac-45f1-a8d2-4133e1160c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"sentence\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16357bb1-68b0-41b9-ac2b-ffdb409d8590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14259640911a42abba454e3bfad511da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fad100978c47568d666e48b15882d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c8e28c781d487882a79654770a873a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86184129-bd0b-4cc9-964d-811168a994dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b8a1e63-b752-45bb-afa5-83df8e6d27de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] they were based on several image processing pipelines, us - ing brainvisa ( riviere et al., 2011, https : / / brainvisa. info / web / ) and freesurfer ( http : / / surfer. nmr. mgh. harvard. edu / ), that were built to : 1 ) compute anatomical models from the structural mri preoperative se - quence, 2 ) normalize this sequence on mni template, 3 ) coregister pre - and post - operative sequences in the patient native space with the struc - tural preoperative mri as reference, using a block matching algorithm, 4 [SEP]'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "317ce80a-3dba-4267-a409-ea2d7ccc8bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebff5805-723b-4e38-9360-fc0ea7f7ffe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soft']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ad7dc5e-ef6f-46a8-89f4-d5bbdc0211a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77fc6dbc-819f-4565-9076-779311bda0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           num_labels=len(labels), \n",
    "                                                           id2label=id2label, \n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d34f161a-6226-4c58-be2f-7ab095df95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4700eb3-5471-424d-b99a-9fc7e4acd10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (pyOpenSSL 23.2.0 (/Users/carolinevanglarsen/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('pyopenssl<23.0.0')).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"SciBERT_finetuned\")\n",
    "\n",
    "#args = TrainingArguments(\n",
    "#    f\"bert-finetuned-sem_eval-english\",\n",
    "#    evaluation_strategy = \"epoch\",\n",
    "#    save_strategy = \"epoch\",\n",
    "#    learning_rate=2e-5,\n",
    "#    per_device_train_batch_size=batch_size,\n",
    "#    per_device_eval_batch_size=batch_size\n",
    "#    num_train_epochs=5,\n",
    "#    weight_decay=0.01,\n",
    "#    load_best_model_at_end=True,\n",
    "#    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42387fc9-d2be-4b90-a550-8cfc8368351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a58cea84-b29a-420a-993f-91ccf90c0377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3788d011-c2ff-41ba-af79-b010ff95c2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  102,   698,   267,   791,   191,  1323,  1572,  2307, 12714, 30113,\n",
       "          422,   227,   579,  5520,  2216,  5332, 30110,   145, 21176, 29187,\n",
       "          365,   186,   205,   422,  5228,   422,  6558,   862,  1352,  1352,\n",
       "         2216,  5332, 30110,   205, 25321,  1352,  2987,  1352,   546,   137,\n",
       "         2159,  9383,   815,   145,  2081,   862,  1352,  1352, 21614,   114,\n",
       "          205,  5744,   205,  1529, 30117,   205, 16048,   205, 27457,  1352,\n",
       "          546,   422,   198,   267,  5896,   147,   862,   158,   546,  4677,\n",
       "        10951,  1262,   263,   111,  3276,  6410, 10014,   262,   579, 22360,\n",
       "        30107,   422,   170,   546, 22585,   238,  1733,   191,  5060, 30109,\n",
       "         7475,   422,   239,   546,  3077, 25445,   192,   382,   579,   137,\n",
       "         1422,   579, 12741,  2789,   121,   111,  1454,  6227,  1630,   190,\n",
       "          111,   785,   579,  4011,   120, 10014,  6410,   188,  2470,   422,\n",
       "          487,   106,  1984,  4740,  1172,   422,   286,   103])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abbb4614-8694-4386-8da1-e30f1642c2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6893, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.2792, -0.5600, -0.1132, -0.2784, -0.0494,  0.4248, -0.0650,  0.8157,\n",
       "         -0.2457,  0.0703]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b44aabec-1889-4038-8cf6-460023a41f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model, \n",
    "    args = training_args, \n",
    "    train_dataset=encoded_dataset[\"train\"], \n",
    "    eval_dataset=encoded_dataset[\"validation\"], \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "498869bc-60bd-4cf6-8fd8-ff738e58005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 09:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=0.3313069954896585, metrics={'train_runtime': 578.3507, 'train_samples_per_second': 0.503, 'train_steps_per_second': 0.067, 'total_flos': 19142704175616.0, 'train_loss': 0.3313069954896585, 'epoch': 3.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cea128-1f59-4594-b8ae-f37caaec7706",
   "metadata": {},
   "source": [
    "### \n",
    "After training, we evaluate our model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3389c58-7231-4373-baab-9d1955cc1df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.26600751280784607,\n",
       " 'eval_f1': 0.14285714285714288,\n",
       " 'eval_roc_auc': 0.5384615384615384,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 5.8823,\n",
       " 'eval_samples_per_second': 2.04,\n",
       " 'eval_steps_per_second': 0.34,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2001e81-2784-40f5-a4cf-fe49e2d7d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./SciBERT_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83b3b14a-ad68-4606-a0fe-895f2c4061bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the fine-tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30813032-dc15-4ecb-96f0-e5c1b686f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"SciBERT_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228116f-2329-4bbd-8532-858f95987221",
   "metadata": {},
   "source": [
    "These maps are openly available in the multi-modal atlas of the Human Brain Project at the EBRAINS platform (https://ebrains.eu/service/human-brain-atlas/), together with a surface map in the FreeSurfer reference space (https://ebrains.eu/news/new-maps-features-ebrains-multilevel-human-brain-atlas/).\n",
    "\n",
    "Labels: \n",
    "- at_map\n",
    "- soft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "128607dd-d405-47a5-9cca-da95098b5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'These maps are openly available in the multi-modal atlas of the Human Brain Project at the EBRAINS platform (https://ebrains.eu/service/human-brain-atlas/), together with a surface map in the FreeSurfer reference space (https://ebrains.eu/news/new-maps-features-ebrains-multilevel-human-brain-atlas/).'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b4aa8e5-200d-4c6b-97e6-6ad3981f76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c352a3f2-9570-4653-be31-865553338dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"SciBERT_finetuned\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00032773-1ed4-43c8-a706-641658b46750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soft'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "model_finetuned.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eeba0-5ecf-4970-b95b-1e07af39dc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57850a1d-6284-4da9-8ecd-a67ed446550f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494fe586",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.pardir, 'Data')\n",
    "file_path = os.path.join(data_dir, 'articles_groundtruth_urls_and_sentences.csv')\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my labeled data \n",
    "# Remove rows with NaN values\n",
    "df = df.dropna()\n",
    "# Remove square brackets and single quotes from the entire 'Sentences' column\n",
    "df['Sentence'] = df['Sentence'].str.replace(r\"[\\[\\]']\", '', regex=True)\n",
    "# Change the type so that it works with the model \n",
    "df['Dataa'] = df['Data'].astype(str).replace(\"True\", 1).replace(\"False\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Sentence', 'Data']]\n",
    "# Rename columns\n",
    "df = df.rename(columns={'Sentence': 'text', 'Data': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9130e56-615e-46f4-b064-aa7ae93ebcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "\n",
    "#test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "# Alternative, if the data are in CSV files \n",
    "#dataset = load_dataset('csv', data_files={'train': 'Corona_NLP_train.csv', 'test': 'Corona_NLP_test.csv'}, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SciBERT with uncased scivocab \n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "# Define a preprocessing function\n",
    "def tokenize_data(data):\n",
    "    \"\"\"Function from: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer(data[\"text\"], truncation=True)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd08a7-9c1b-4f88-b8c3-87e63c04ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the expected ids to their labels with id2label and label2id:\n",
    "id2label = {0: \"FALSE\", 1: \"TRUE\"}\n",
    "label2id = {\"FALSE\": 0, \"TRUE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SciBERT with uncased scivocab \n",
    "#model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3687f68-3829-46e4-8a53-fdae6324f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0cdfa-9004-44c3-a25c-12ee323a489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"From: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fbd4d-ab01-4c5c-b97a-3c123ff670ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edb0db-5a4c-4ce5-aae7-d21028a1b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(os.pardir, \"SciBERT-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79502373-44e2-4d30-b175-ff1bc0a40488",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"SciBERT_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd2b23-42eb-43e6-ad6c-986378e2bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train'].shuffle(seed=10).select(range(26))\n",
    "eval_dataset = dataset['train'].shuffle(seed=10).select(range(26, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773ab33-da11-4d5b-b000-4b4d32da77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec089d-8b8b-440b-9f39-655f999608c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff3a2c-a318-4cec-b2f7-d80f8cfc07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./SciBERT_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40c930-5e0b-4cea-8c19-72a7c9cc8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'These maps are openly available in the multi-modal atlas of the Human Brain Project at the EBRAINS platform (https://ebrains.eu/service/human-brain-atlas/), together with a surface map in the FreeSurfer reference space (https://ebrains.eu/news/new-maps-features-ebrains-multilevel-human-brain-atlas/).'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99346157-15c8-44fa-808e-2d3d8e2d8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"SciBERT_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ea208",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a05cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"SciBERT_finetuned\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "model_finetuned.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ea6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeca190-3021-4002-851e-64ea8d0de652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55b10b45-3751-43a3-a5f9-c2f484955e7a",
   "metadata": {},
   "source": [
    "<a name='classify'></a>\n",
    "## 3.2 Classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23b67b-47dc-475b-86cc-e2229f55ed0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaadbca-735b-4629-bdc6-6899bda10e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4158431-dc50-46ee-bec9-0fb0572d989d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82665262-4af6-4190-9cef-d15a6526cf1d",
   "metadata": {},
   "source": [
    "<a name='getdatasets'></a>\n",
    "# 4. Get datasets \n",
    "Now that the sentences have been classified, I will continue working with those that were labelled as 'Dataset'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fa30c-02de-42b9-afd1-694ca0384e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae31df5-7e4b-4c88-8c45-eaced5509b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1506698-ff60-4b00-9e13-d0b72d602c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9f71a25",
   "metadata": {},
   "source": [
    "<a name='processsentencesforclassification'></a>\n",
    "## 3.2. Process sentences for classification \n",
    "I combine the the cleaning steps mention in Gasparetto et al. (2022), Haj-Yahia et al. (2019), and Halford (2020) using functions from both gensim (Řehůřek and Sojka 2010) and NLTK (Bird et al. 2009): \n",
    "- Uninformative tokens are removed, including: \n",
    "    - Numeric characters (Gensim)\n",
    "    - Tags (Gensim)\n",
    "    - Punctuation (Gensim)\n",
    "    - Extra whitespace (Gensim)\n",
    "    - Stopwords based on stopword list from NLTK.\n",
    "    - Isolated characters (len = 1)\n",
    "- The URLs are removed \n",
    "- Stem text (Gensim)\n",
    "- The text is converted to lowercase.\n",
    "- The text is tokenized using SciBERT's tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the 'Data' directory\n",
    "data_dir = os.path.join(os.pardir, 'Data')\n",
    "\n",
    "# Define the file path for 'articles_filtered_urls.csv'\n",
    "path = os.path.join(data_dir, 'articles_filtered_urls.csv')\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Now, the 'df' variable contains the data from 'articles_filtered_urls.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb846fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values\n",
    "df = df.dropna()\n",
    "# Remove square brackets and single quotes from the entire 'Sentences' column\n",
    "df['Sentences'] = df['Sentences'].str.replace(r\"[\\[\\]']\", '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentences'].loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f920b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f70e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"This function removes URLs from a text using the URLExtract library (Lipovský 2022) to\n",
    "    identify the URLs. \n",
    "    \n",
    "    Paramters: \n",
    "    :param text(str): text. \n",
    "    \n",
    "    Returns: \n",
    "    :return: text (str) without URLs. \n",
    "    \"\"\"\n",
    "    extractor = urlextract.URLExtract()\n",
    "    urls = list(extractor.find_urls(text))\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"This function cleans the text using multiple of gensim's preprocessing functions (Řehůřek and Sojka 2010), \n",
    "    NLTK's stopwords and tokenize modules (Bird et al. 2009), and it removes the URLs using \n",
    "    the URLExtract library (Lipovský 2022). \n",
    "    \n",
    "    Parameters: \n",
    "    :param text (str): \n",
    "    \n",
    "    Returns: \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Remove URLs from the text\n",
    "    text = remove_urls(text)\n",
    "    \n",
    "    # Combine multiple spaces, strip tags, punctuation, numerics, stem the text, and convert to lowercase\n",
    "    # POSSIBLE: remove_stopwords\n",
    "    text = ' '.join(preprocess_string(text, filters=[strip_tags, strip_punctuation, strip_numeric, strip_multiple_whitespaces, stem_text]))\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization (split text into words)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove uninformative tokens\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and len(token) > 1:  # Check if token is a word and not a single character\n",
    "            if token.lower() not in stop_words:  # Remove stopwords\n",
    "                cleaned_tokens.append(token)\n",
    "\n",
    "    tokenized_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = df['Sentences'].loc[1]\n",
    "#test_clean = clean_text(test)\n",
    "#test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653352e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb58d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to the 'Sentences' column\n",
    "df_cleaned['Tokens'] = df_cleaned['Sentences'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eee46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9cc4fa-0a39-4161-b1ca-10103d9f0e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa1d11cd",
   "metadata": {},
   "source": [
    "<a name='references'></a>\n",
    "# References\n",
    "\n",
    "- Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A Pretrained Language Model for Scientific Text (arXiv:1903.10676). arXiv. http://arxiv.org/abs/1903.10676\n",
    "- Bird, S., Loper, E., & Klein, E. (2009). Natural Language Processing with Python. O’Reilly Media, INC.\n",
    "- Choudhary, R. (2021, December 29). Fine-Tuning Bert for Tweets Classification ft. Hugging Face. MLearning.Ai. https://medium.com/mlearning-ai/fine-tuning-bert-for-tweets-classification-ft-hugging-face-8afebadd5dbf\n",
    "- Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n",
    "- Fine-tuning BERT (and friends) for multi-label text classification. (n.d.). Retrieved November 9, 2023, from https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=hiloh9eMK91o\n",
    "- Gasparetto, A., Marcuzzo, M., Zangari, A., & Albarelli, A. (2022). A Survey on Text Classification Algorithms: From Text to Predictions. _Information_, _13_(2), Article 2. https://doi.org/10.3390/info13020083\n",
    "- Haj-Yahia, Z., Sieg, A., & Deleris, L. A. (2019). Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 371–379. https://doi.org/10.18653/v1/P19-1036\n",
    "- Halford, M. (2020, October 3). Unsupervised text classification with word embeddings. https://maxhalford.github.io/blog/unsupervised-text-classification/\n",
    "- Kosar, A., Pauw, G. D., & Daelemans, W. (2022). Unsupervised Text Classification with Neural Word Embeddings. _Computational Linguistics in the Netherlands Journal_, _12_, 165–181.\n",
    "- Lipovský, J. (2022). urlextract: Collects and extracts URLs from given text. (1.8.0) [Python]. https://github.com/lipoja/URLExtract\n",
    "- Lhoest, Q., Villanova del Moral, A., von Platen, P., Wolf, T., Šaško, M., Jernite, Y., Thakur, A., Tunstall, L., Patil, S., Drame, M., Chaumond, J., Plu, J., Davison, J., Brandeis, S., Sanh, V., Le Scao, T., Canwen Xu, K., Patry, N., Liu, S., … Delangue, C. (2021). Datasets: A Community Library for Natural Language Processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 175–184) [Python]. Association for Computational Linguistics. https://aclanthology.org/2021.emnlp-demo.21 (Original work published 2020)\n",
    "- Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, É. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12(85), 2825–2830.\n",
    "- Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget\n",
    "- Text classification. (n.d.). 🤗 HuggingFace - Transformers. Retrieved October 29, 2023, from https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "- Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., … Rush, A. M. (2020). HuggingFace’s Transformers: State-of-the-art Natural Language Processing (arXiv:1910.03771). arXiv. https://doi.org/10.48550/arXiv.1910.03771\n",
    "\n",
    "\n",
    "\n",
    "Do I still use these? \n",
    "- Grave, E., Bojanowski, P., Gupta, P., Joulin, A., & Mikolov, T. (2018). Learning Word Vectors for 157 Languages (arXiv:1802.06893). arXiv. https://doi.org/10.48550/arXiv.1802.06893\n",
    "- Kalai, A. T. & Brown University (Directors). (2019, April 18). An ICERM Public Lecture - Bias in bios: Fairness in a high-stakes machine-learning setting. https://www.youtube.com/watch?v=IDNXZitcQng\n",
    "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013a). Efficient Estimation of Word Representations in Vector Space (arXiv:1301.3781). arXiv. https://doi.org/10.48550/arXiv.1301.3781\n",
    "- Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. https://doi.org/10.3115/v1/D14-1162\n",
    "- Řehůřek, R., & Sojka, P. (2010). Software Framework for Topic Modelling with Large Corpora. 45–50. https://doi.org/10.13140/2.1.2393.1847\n",
    "- Řehůřek, R. (2023). What is Gensim-data for? [Python]. https://github.com/piskvorky/gensim-data (Original work published 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a9b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d81f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns_v2 = [\n",
    "    ([\"Data and Code Availability\", \"Data Availability\", \"Data/code availability\"], [\"3. \", \"CRediT authorship contribution statement\", \"Acknowledgements\", \"References\", \"Declaration of Competing Interests\", \"Credit authorship contribution statement\", \"\\n\\n\"]),\n",
    "    ([\"2.1.\"], [\"2.2.\"]),\n",
    "    ([\"Resource\", \"3.1.\"], [\"3.2.\"]),\n",
    "    ([\"Fig.\\d+\", \"Fig.\\d+\\.?\", \"Figure \\d+\"], [\"https?://[^\\s]+\"]),\n",
    "    ([\"Tab.\\d+\", \"Table \\d+\\.?\"], [\"https?://[^\\s]+\", \"[\\w\\s-]+\\d{4}\"]),\n",
    "    ([\"Introduction\", \"1. \"], [\"2. \"]),\n",
    "    ([\"Abstract\"], [\"1. \", \"Introduction\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store individual results\n",
    "results_list_v2 = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    # Get the first 10 DOIs\n",
    "    first_10_dois = doi_data['DOIs'][11:21]\n",
    "\n",
    "    for doi in first_10_dois:\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        section_content = get_content(pdf_path, alternative_pdf_directory, section_patterns_v2)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results_list_v2.append({\"DOI\": doi, \"Section\": section_content})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df2 = pd.DataFrame(results_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8340b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e19617",
   "metadata": {},
   "source": [
    "# Old notes \n",
    "\n",
    "## 1.1. Get text sections \n",
    "\n",
    "I use the work of Akkoç (2023) and Sourget (2023) to search the PDFs for their datasets. I am using the code from two separate git repositories as inspiration for the two functions presented in this section. \n",
    "- *get_section* is losely interpreted from Akkoç (2023) using the following breadcrumb in the github repository: PublicDatasets/ArticleAnalyser.ipynb, section '2.1 Get section function'\n",
    "- *get_content* is losely interpreted from Soruget (2023) using the following breadcrumb in the github repository: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "\n",
    "<br>\n",
    "\n",
    "References: \n",
    "- Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022)\n",
    "- Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget\n",
    "\n",
    "## Stuff \n",
    "    URLs do not necessarily link to the data.\n",
    "    A git repository can contain both data and code - but not always.\n",
    "    The dataset might only be mentioned by name and not linked (so far, I've only seen the names in camelcase).\n",
    "    QUESTION: How do we treat reviews that summarizes data but does not contain new data? Is the reuse of a dataset not also the same as not containing new data?\n",
    "\n",
    "## More stuff\n",
    "\n",
    "- Worries \n",
    "    - How to get the name of the dataset itself and the url\n",
    "        - The URL can be broken up by spaces (due to line changes in the pdf) - can I find a way to find out which is the entire URL? \n",
    "            - Is there any slashes in the text ahead? A parenthesis, dot, comma, or another symbol might end it URL. \n",
    "    - If someone uses e.g., HCP, do they use all of the data? Do I need to catch more text-sections to learn this (in relation to the discussion of significance testing - if they use different parts of the dataset, they are not testing on the same). \n",
    "        - \"Due to HCP and dHCP privacy policies, the preprocessed resting-state images of human adults and neonates (with their IDs) can only be shared upon request with qualified investigators who agree to the Restricted Data Use Terms of these two datasets.\" (from 10.1016/j.neuroimage.2022.119339)\n",
    "    - What if the article does not analyse any data? (e.g., 10.1016/j.neuroimage.2022.119295 presents a software package for the execution of RT-fMRI experiments. \n",
    "- FUNCTION GET_CONTENT: Make a comment about trying the \"Editorial board\" texts in the other file - just so I don't get en \"Error reading PDF:\" \n",
    "    - Make an addition to 'get_section' where the says 'Editorial board' instead of None for the section text. \n",
    "\n",
    "\n",
    "### Clean text \n",
    "I will do a very simple initial cleaning of the extracted text sections: \n",
    "- Replace multiple spaces with a single space\n",
    "    - [.replace('   ', ' ').replace('  ', ' ')]\n",
    "- Remove all \\n characters \n",
    "    - [.replace('\\n', '')]\n",
    "- Remove leading and trailing spaces after the following characters: -, (, ), /, ., _ , and between : / \n",
    "    - [.replace('- ', '-').replace('( ', '(').replace(' )', ')').replace('/ ', '/').replace(' /', '/').replace(' .', '.').replace(': /', ':/').replace(' _ ', '_').replace(' _', '_').replace('_ ', '_')] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb704386-772e-4117-88b4-f5e4ee1d8c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
