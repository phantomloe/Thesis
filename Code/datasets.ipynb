{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7067776c",
   "metadata": {},
   "source": [
    "# Table of contents \n",
    "- [Setup](#setup) \n",
    "    - [Target](#target)\n",
    "    - [Libraries](#libraries)\n",
    "- [Gather datasets](#gatherdatasets)\n",
    "    - [Get content](#getcontent)\n",
    "        - [Section patterns v1](#sectionpatternsv1)\n",
    "        - [Section patterns v2](#sectionpatternsv2)\n",
    "        - \n",
    "    - [Get datasets](#getdatasets)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af309d",
   "metadata": {},
   "source": [
    "<a name='setup'></a>\n",
    "# 0. Setup \n",
    "\n",
    "This notebook contains the code to extract the datasets used in the articles published in NeuroImage in 2022. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='target'></a> \n",
    "## 0.1. Target\n",
    "The goal is the use pypdf to locate and extract the datasets used for analysis in the research articles. Based on an initial review of nine random \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='libraries'></a>\n",
    "## 0.2. Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json \n",
    "import os \n",
    "import re \n",
    "\n",
    "import pypdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6525d",
   "metadata": {},
   "source": [
    "<a name='gatherdatasets'></a>\n",
    "# 1. Gather datasets \n",
    "\n",
    "PLAN OF ATTACK TO EXPLORE: \n",
    "* IF - Locate 'Data availability' (or similar) section and look for links - if multiple, save all of them and look at surrounding words for context \n",
    "* ELSE If there is no 'Data availability' (or similar) section \n",
    "\t* Look at wording in section 2.1 \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='getcontent'></a>\n",
    "## 1.1. Get content \n",
    "\n",
    "I use the work of Akkoç (2023) and Sourget (2023) to search the PDFs for their datasets. I am using the code from two separate git repositories as inspiration for the two functions presented in this section. \n",
    "- *get_section* is losely interpreted from Akkoç (2023) using the following breadcrumb in the github repository: PublicDatasets/ArticleAnalyser.ipynb, section '2.1 Get section function'\n",
    "- *get_content* is losely interpreted from Soruget (2023) using the following breadcrumb in the github repository: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "\n",
    "<br>\n",
    "\n",
    "References: \n",
    "- Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022)\n",
    "- Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(pdf_path, alt_pdf_path, section_patterns):\n",
    "    \"\"\"Get a PDF. \n",
    "    This function is loosely interpreted from Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget\n",
    "    specifically: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "    \n",
    "    Parameters: \n",
    "    :param pdf_path (str): Path to the PDF file.\n",
    "    :param json_file_path (str): Path to the JSON file containing the DOIs of the relevant research articles. \n",
    "    \n",
    "    Returns: \n",
    "    :return: Extracted content or 'Editorial board' if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_file = open(pdf_path, 'rb')\n",
    "        pdf_reader = pypdf.PdfReader(pdf_file)\n",
    "        # Read the entire PDF content\n",
    "        pdf_text = \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "        \n",
    "        # Extract sections using the provided section patterns\n",
    "        content = get_section(pdf_text, section_patterns)\n",
    "        if content: \n",
    "            return content \n",
    "        pdf_file.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Try to open the PDF from the alternative directory\n",
    "            alternative_pdf_path = os.path.join(alt_pdf_path, os.path.basename(pdf_path))\n",
    "            print(alternative_pdf_path)\n",
    "            pdf_file = open(alternative_pdf_path, 'rb')\n",
    "            return 'Editorial board'\n",
    "        except FileNotFoundError:\n",
    "            # If PDF is not found in the original or alternative directory, return 'Editorial board'\n",
    "            return 'Editorial board'\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "\n",
    "\n",
    "def get_section(article, section_patterns):\n",
    "    \"\"\"Get sections from a research paper based on patterns.\n",
    "    This function is losely interpreted from Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022) with some alterations.\n",
    "    specifically PublicDatasets/ArticleAnalyser.ipynb, section '2.1 Get section function'\n",
    "    \n",
    "    Parameters: \n",
    "    :param article (str): Text contents of the research paper.\n",
    "    :param section_patterns (list of lists): A list of lists where each inner list represents the start and end patterns.\n",
    "    \n",
    "    Returns: \n",
    "    :return: The extracted section text.\n",
    "    \"\"\"\n",
    "    article_lower = article.lower()  # Convert contents to lowercase\n",
    "\n",
    "    # Attempt to find the section based on the current patterns (case-insensitive)\n",
    "    for start_patterns, end_patterns in section_patterns:\n",
    "        for start_pattern in start_patterns:\n",
    "            start_pattern = re.compile(re.escape(start_pattern), re.IGNORECASE)\n",
    "            match_start = start_pattern.search(article_lower)\n",
    "            if match_start:\n",
    "                idx0 = match_start.start()\n",
    "                for end_pattern in end_patterns:\n",
    "                    end_pattern = re.compile(re.escape(end_pattern), re.IGNORECASE)\n",
    "                    match_end = end_pattern.search(article_lower[idx0:])\n",
    "                    if match_end:\n",
    "                        end_idx = idx0 + match_end.end()\n",
    "                        section = article[idx0:end_idx]  # Extract the matched section\n",
    "                        return section\n",
    "\n",
    "    # If no match is found, return an empty string\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing PDFs\n",
    "pdf_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_articles_doi/'\n",
    "alternative_pdf_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_editorialboard_doi/'\n",
    "\n",
    "# Path to the JSON file containing DOI values\n",
    "json_file_path = '../Data/ElsevierAPI/downloadedPDFs_info.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4c760",
   "metadata": {},
   "source": [
    "<a name='sectionpatternsv1'></a>\n",
    "### 1.1.1. Section patterns v1 \n",
    "Before I continue working on extracting the dataset names and potential links from the sections, I am curious to see how the section pattern performs. \n",
    "\n",
    "I investigate the first ten DOIs in downloadedPDFs_info.json() to see exactly what text sections were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns = [\n",
    "    ([\"Data and Code Availability\", \"Data Availability\"], [\"3\", \"CRediT authorship contribution statement\", \"Acknowledgements\", \"References\"]),\n",
    "    ([\"2.1\"], [\"2.2\"]),\n",
    "    ([\"Resource\", \"3.1 'Resource'\"], [\"3.2\"]),\n",
    "    ([\"Fig.\\d+\", \"Fig.\\d+\\.?\", \"Figure \\d+\"], [\"https?://[^\\s]+\"]),\n",
    "    ([\"Tab.\\d+\", \"Table \\d+\\.?\"], [\"https?://[^\\s]+\", \"[\\w\\s-]+\\d{4}\"]),\n",
    "    ([\"Introduction\", \"1\"], [\"2\"]),\n",
    "    ([\"Abstract\"], [\"1\", \"Introduction\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc263ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store individual results\n",
    "results_list = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    # Get the first 10 DOIs\n",
    "    first_10_dois = doi_data['DOIs'][:10]\n",
    "\n",
    "    for doi in first_10_dois:\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        section_content = get_content(pdf_path, alternative_pdf_directory, section_patterns)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results_list.append({\"DOI\": doi, \"Section\": section_content})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a4712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2943c03",
   "metadata": {},
   "source": [
    "*In the following description, I refer to the index of the articles in results_df.*\n",
    "\n",
    "Observations from the text sections extracted with section_patterns: \n",
    "* In 1, 2, 4, 7, and 9, **the text is cut short because there's a mention of a number 3** within the section (in a link, in a release number, etc.). \n",
    "* In 2, they call it: 'Data/code availability statement'\n",
    "* In 2 and 9, the **end of the section can be 'Acknowledgements'**.\n",
    "* In 3 and 6, the **end of the section can be 'Declaration of Competing Interest'**.\n",
    "* In 4, 5, and 8, the **section ends with 'Credit authorship contribution statement'**.\n",
    "* In 5, we see that the use of a **URL does not necessarily mean that it's pointing to data (in this case, it's code and software)**. \n",
    "* In 6, we see that **the formulation of the text is important** (as the github link both contains data and code, but that is tricky to see). \n",
    "* In 7 and 8, they **mention which dataset they used, but do not link it**. \n",
    "* In 9, it says: \"The review summarizes data but does not contain new data.\" (this is important if I want to look into and further filter the documents for significance testing). \n",
    "\n",
    "<br>\n",
    "From this investigation I can see that I need to edit the section patterns. Ideas: \n",
    "\n",
    "- Maybe the end of a section can be \\n\\n? \n",
    "- Section end '3' should be called '3. ' - maybe this will fix some \n",
    "- Add variations: \n",
    "    - Section starts: \n",
    "        - Data/code availability statement\n",
    "    - Section ends: \n",
    "        - [data and code] Declaration of Competing Interest\n",
    "        - [data and code] Acknowledgements\n",
    "        - [data and code] Credit authorship contribution statement\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "FOR FUTURE STEPS: \n",
    "- URLs do not necessarily link to the data. \n",
    "- A git repository can contain both data and code - but not always. \n",
    "- The dataset might only be mentioned by name and not linked (so far, I've only seen the names in camelcase). \n",
    "- QUESTION: How do we treat reviews that summarizes data but does not contain new data? Is the reuse of a dataset not also the same as not containing new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92ce80",
   "metadata": {},
   "source": [
    "<a name='sectionpatternsv2'></a>\n",
    "### 1.1.2. Section patterns v2 \n",
    "Based on my exploration on the performance of the first section patterns, I can see that they need to be rewritten. For version 2, I made a few edits: \n",
    "* Add variations\n",
    "    * Section starts: \n",
    "        * Data/code availability statement \n",
    "    * Section ends: \n",
    "        * '\\n\\n' (this could be a general way to end the section) \n",
    "        * [data and code] Declaration of Competing Interest\n",
    "        * [data and code] Acknowledgements\n",
    "        * [data and code] Credit authorship contribution statement\n",
    "* Change pattern containing numbers (e.g., '3' is now '3. ')\n",
    "<br>\n",
    "I investigate the next ten DOIs in downloadedPDFs_info.json() to see exactly what text sections were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns_v2 = [\n",
    "    ([\"Data and Code Availability\", \"Data Availability\", \"Data/code availability\"], [\"3. \", \"CRediT authorship contribution statement\", \"Acknowledgements\", \"References\", \"Declaration of Competing Interests\", \"Credit authorship contribution statement\", \"\\n\\n\"]),\n",
    "    ([\"2.1.\"], [\"2.2.\"]),\n",
    "    ([\"Resource\", \"3.1.\"], [\"3.2.\"]),\n",
    "    ([\"Fig.\\d+\", \"Fig.\\d+\\.?\", \"Figure \\d+\"], [\"https?://[^\\s]+\"]),\n",
    "    ([\"Tab.\\d+\", \"Table \\d+\\.?\"], [\"https?://[^\\s]+\", \"[\\w\\s-]+\\d{4}\"]),\n",
    "    ([\"Introduction\", \"1. \"], [\"2. \"]),\n",
    "    ([\"Abstract\"], [\"1. \", \"Introduction\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store individual results\n",
    "results_list_v2 = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    # Get the first 10 DOIs\n",
    "    first_10_dois = doi_data['DOIs'][11:21]\n",
    "\n",
    "    for doi in first_10_dois:\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        section_content = get_content(pdf_path, alternative_pdf_directory, section_patterns_v2)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results_list_v2.append({\"DOI\": doi, \"Section\": section_content})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df2 = pd.DataFrame(results_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8340b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc69422",
   "metadata": {},
   "source": [
    "*In the following description, I refer to the index of the articles in results_df2.*\n",
    "\n",
    "Observations from the text sections extracted with section_patterns_v2: \n",
    "- In 0, there are links, but these are not to the dataset - they write \"The used data can be shared with other researchers upon reasonable request.\" \n",
    "- In 0 and 7, the next section is called 'Supplementary materials' - which means that my attempt at \\n\\n did not work.  \n",
    "- In 2, the only mention of data was picked up in section 2.1.\n",
    "- In 2, the 'Declaration  of Competing  Interest' was not picked up - it looks like it's because there are double spaces between the words. \n",
    "- In 3, the 'Credit authorship  contribution  statement' is not picked - double spaces?\n",
    "- In 6, the data section is called 'Code and data availability' - but it was picked up by 'data availability'. \n",
    "- In 6, there are multiple links mentioned - one for data (an atlas), one for the code, and one for the data. \n",
    "    - NB! When copying the URL for the data, it is broken up by the formatting: https://www.humanconnectome.org/study/hcp-young-adult/ document/1200-subjects-data-release - this is also the case for the atlas. \n",
    "- In 7 and 8, there are spaces in the URL. \n",
    "- In 8, the following section 'Declaration of Competing Interest' was not picked up. \n",
    "- In 9, the introduction was picked up: but it does not look like any data is analysed in this article. \n",
    "\n",
    "<br>\n",
    "From this investigation I can see that I need to edit the section patterns further. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "TO DO: \n",
    "\n",
    "- Section_patterns that do not work: \n",
    "    - Section_end: \\n\\n\n",
    "    - Section_end: 'Declaration  of Competing  Interest' + Section_end: 'Credit authorship  contribution  statement' + Section_end: 'Supplementary materials'\n",
    "        - double-spaces between words mess these up\n",
    "- Section_patterns I'm worried about: \n",
    "    - Section_start: 2.1. - what if it's '2.1'?\n",
    "    - Section_start: 'Code and data availability'\n",
    "- Undiscovered section_patterns: \n",
    "    - Section_end: 'Ethics statement'\n",
    "- Worries \n",
    "    - How to get the name of the dataset itself and the url\n",
    "        - The URL can be broken up by spaces (due to line changes in the pdf) - can I find a way to find out which is the entire URL? \n",
    "            - Is there any slashes in the text ahead? A parenthesis, dot, comma, or another symbol might end it URL. \n",
    "    - If someone uses e.g., HCP, do they use all of the data? Do I need to catch more text-sections to learn this (in relation to the discussion of significance testing - if they use different parts of the dataset, they are not testing on the same). \n",
    "        - \"Due to HCP and dHCP privacy policies, the preprocessed resting-state images of human adults and neonates (with their IDs) can only be shared upon request with qualified investigators who agree to the Restricted Data Use Terms of these two datasets.\" (from 10.1016/j.neuroimage.2022.119339)\n",
    "    - What if the article does not analyse any data? (e.g., 10.1016/j.neuroimage.2022.119295 presents a software package for the execution of RT-fMRI experiments. \n",
    "- FUNCTION GET_CONTENT: Make a comment about trying the \"Editorial board\" texts in the other file - just so I don't get en \"Error reading PDF:\" \n",
    "    - Make an addition to 'get_section' where the says 'Editorial board' instead of None for the section text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be6ece",
   "metadata": {},
   "source": [
    "### 1.1.3. Section patterns v3 \n",
    "\n",
    "I want to make a regex_pattern work, as it seems like a double space after \n",
    "\n",
    "\n",
    "TO DO: \n",
    "- Section_patterns that do not work: \n",
    "    - Section_end: \\n\\n\n",
    "    - Section_end: 'Declaration  of Competing  Interest' + Section_end: 'Credit authorship  contribution  statement' + Section_end: 'Supplementary materials'\n",
    "        - double-spaces between words messed these up. \n",
    "- Section_patterns I'm worried about: \n",
    "    - Section_start: 2.1. - what if it's '2.1'?\n",
    "    - Section_start: 'Code and data availability'\n",
    "- Undiscovered section_patterns: \n",
    "    - Section_end: 'Ethics statement'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_regex(pdf_path, alt_pdf_path, section_patterns):\n",
    "    try:\n",
    "        pdf_file = open(pdf_path, 'rb')\n",
    "        pdf_reader = pypdf.PdfReader(pdf_file)\n",
    "        # Read the entire PDF content\n",
    "        pdf_text = \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "        \n",
    "        # Extract sections using the provided section patterns\n",
    "        content, matched_start_pattern, matched_end_pattern = get_section_regex(pdf_text, section_patterns)\n",
    "        \n",
    "        if content:\n",
    "            return content, matched_start_pattern, matched_end_pattern\n",
    "        else:\n",
    "            # Handle the case where no content is found\n",
    "            return content, matched_start_pattern, matched_end_pattern\n",
    "        pdf_file.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Try to open the PDF from the alternative directory\n",
    "            alternative_pdf_path = os.path.join(alt_pdf_path, os.path.basename(pdf_path))\n",
    "            pdf_file = open(alternative_pdf_path, 'rb')\n",
    "            return 'Editorial board', '', ''\n",
    "        except FileNotFoundError:\n",
    "            # If PDF is not found in the original or alternative directory, return 'Editorial board'\n",
    "            return 'Editorial board', '', ''\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "\n",
    "\n",
    "def get_section_regex(article, section_patterns):\n",
    "    matched_start_pattern = None  # Variable to store the matched start pattern\n",
    "    matched_end_pattern = None    # Variable to store the matched end pattern\n",
    "    \n",
    "    # Iterate through each pattern pair\n",
    "    for start_pattern, end_pattern in section_patterns:\n",
    "        # Find all matches of the start pattern in the article\n",
    "        start_matches = re.finditer(start_pattern, article, re.IGNORECASE)\n",
    "\n",
    "        # Iterate through each start match\n",
    "        for match in start_matches:\n",
    "            start_idx = match.start()  # Get the start position of the start match\n",
    "\n",
    "            # Search for the end pattern starting from the end position of the start match\n",
    "            end_match = re.search(end_pattern, article[start_idx:], re.IGNORECASE)\n",
    "            \n",
    "            if end_match:\n",
    "                end_idx = start_idx + end_match.start()  # Calculate the end position of the section\n",
    "                section_text = article[start_idx:end_idx].strip()  # Extract the section text\n",
    "\n",
    "                # Store the matched start and end patterns\n",
    "                matched_start_pattern = match\n",
    "                matched_end_pattern = end_pattern\n",
    "\n",
    "                # Return the section text and matched patterns\n",
    "                return section_text, matched_start_pattern, matched_end_pattern\n",
    "\n",
    "    # If no match is found, return an empty string and the last matched patterns\n",
    "    return '', '', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075aca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns_regex = [\n",
    "    (r'(?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availability |(?<![\\'\"]) \\s*?\\n?Data\\s+availability |(?<![\\'\"]) \\s*?\\n?Data/code\\s+availability', r'\\s*?\\n\\n |\\s*?\\n?3\\. | \\s*?\\n?CRediT\\s+authorship\\s+contribution\\s+statement(?:s)? | \\s*?\\n?Acknowledgement(?:s)? | \\s*?\\n?Reference(?:s)? | \\s*?\\n?Declaration\\s+of\\s+Competing\\s+Interest(?:s)? | \\s*?\\n?Credit\\s+authorship\\s+contribution\\s+statement(?:s)? | \\s*?\\n?Funding | \\s*?\\n?Supplementary\\s+materials | \\s*?\\n?Ethic(?:s)? statement(?:s)?'),\n",
    "    (r'\\n?2\\.1\\.', r'\\n?2\\.2. | \\n\\n '),\n",
    "    (r'\\n?Resource | \\n?3\\.1\\.\\s*?\\n?', r'\\n?3\\.2.\\s*?| \\s*?\\n\\n '),\n",
    "    (r'\\n?Introduction\\s*?\\n? | \\s*?\\n?1\\.\\s*?\\n? ', r'\\s*?\\n?2\\.\\s*?\\n? | \\s*?\\n\\n '),\n",
    "    (r'\\n?Fig\\.\\d+ | \\n?Fig\\.\\d+\\.? | \\n?Figure \\d+', r'https?://[^\\s]+ | \\s*?\\n\\n '),\n",
    "    (r'\\n?Tab\\.\\d+ | \\n?Table \\d+\\.?', r'https?://[^\\s]+ | [\\w\\s-]+\\d{4} | \\s*?\\n\\n '),\n",
    "    (r'\\n?Abstract\\s*?\\n? | \\s*?\\n?1\\.\\s*?\\n? ', r'\\n?Introduction\\s*?\\n? | \\s*?\\n\\n ')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store individual results\n",
    "results_list_regex = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    # Get the first X DOIs\n",
    "    first_dois = doi_data['DOIs'][20:21]\n",
    "    \n",
    "    print(len(first_dois))\n",
    "\n",
    "    for doi in first_dois:\n",
    "        print(doi)\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        section_content_regex, matched_start_pattern, matched_end_pattern = get_content_regex(pdf_path, alternative_pdf_directory, section_patterns_regex)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results_list_regex.append({\"DOI\": doi, \"Section\": section_content_regex, \"Start_pattern\": matched_start_pattern, \"End_pattern\": matched_end_pattern})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df_regex = pd.DataFrame(results_list_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ac63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-21 in the json\n",
    "results_df2.loc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88753dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df2['Section'].loc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182516bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_regex['Section'].loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05023e",
   "metadata": {},
   "source": [
    "Persisting issues: \n",
    "- Can I change the regex patterns to camelcase? \n",
    "    - It looks like the titles mainly have the first letter capitalized - this will fix the next problem. \n",
    "- When searching using lowercase, results_df2['Section'].loc[2], this is cut short\n",
    "    - From 'Data and code availability  statement  \\nData used in the study are available  upon direct request.  Conditions  \\nfor its sharing  involve  the formalisation  of a research  agreement.  The \\ndata and code sharing  adopted  by the authors  comply  with the require-  \\nments of the funding  body or institute,  and with the institutional  ethics \\napproval.  Parts of the data are conﬁdential  and additional  ethical ap- \\nproval may be needed  for re-use. \\n'\n",
    "    - To: 'Data and code availability  statement  \\nData used in the study are available  upon direct request.  Conditions  \\nfor its sharing  involve  the formalisation  of a research  agreement.  The \\ndata and code sharing  adopted  by the authors  comply  with the require-  \\nments of the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f030f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bac9ed8",
   "metadata": {},
   "source": [
    "<a name='getdatasets'></a>\n",
    "## 1.2. Get datasets\n",
    "I need to extract the datasets from the text sections we extracted above. \n",
    "\n",
    "Based on my previous observations, I will start the extraction with the following notions in mind: \n",
    "- Private datasets (meaning either fully private or available upon request) \n",
    "    - Markers: \n",
    "        - Text \n",
    "- Public datasets (meaning it's available to everyone with a link or title of the dataset)\n",
    "    - Markers: \n",
    "        - Hyperlink \n",
    "        - Camelcase \n",
    "- Issues (**code**)\n",
    "    - The URL can be broken up by spaces due to line changes in the PDF. Do we stop at the parenthesis, comma or another symbol that might end the URL? \n",
    "        - EXAMPLES \n",
    "    - Identify the dataset by name; 'Github' can also be identified if we search for letters in camelcase. \n",
    "- Issues (**analysis**)\n",
    "    - If someone uses e.g., HCP, do they use all of the data? Do I need to catch more text-sections to learn this (in relation to the discussion of significance testing - if they use different parts of the dataset, they are not testing on the same). \n",
    "        - \"Due to HCP and dHCP privacy policies, the preprocessed resting-state images of human adults and neonates (with their IDs) can only be shared upon request with qualified investigators who agree to the Restricted Data Use Terms of these two datasets.\" (from 10.1016/j.neuroimage.2022.119339)\n",
    "    - What if the article does not analyse any data? (e.g., 10.1016/j.neuroimage.2022.119295 presents a software package for the execution of RT-fMRI experiments. \n",
    "    - What if there are multiple sections and the text is slightly different (e.g., 10.1016/j.neuroimage.2022.118986)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Columns: \n",
    "- Section text \n",
    "- Section pattern (multiple reasons: 1) I can get a sense of whether the data statement is common in NeuroImage, 2) I can go back and handle potential more difficult cases) \n",
    "- Extracted dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cef4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fcfc09",
   "metadata": {},
   "source": [
    "# Save datasets \n",
    "\n",
    "- Store the extracted datasets for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac298936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be94c596",
   "metadata": {},
   "source": [
    "# X. References\n",
    "\n",
    "- Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022)\n",
    "- Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_v1(article, section_patterns):\n",
    "    \"\"\"Get a section from a research paper. \n",
    "    \n",
    "    Parameters: \n",
    "    :param contents (): Text contents of the resaerch paper.\n",
    "    :param section_patterns (list): A list of strings to indicate the start and ends of the dataset section.\n",
    "    :return: returns substring of text region between section_header and a potential section_end. returns \"\" if it fails to find it.\n",
    "    \n",
    "    This function is adapted from Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022) with some alterations.\n",
    "    \"\"\"\n",
    "    contents_lower = article.lower()  # Convert contents to lowercase\n",
    "    \n",
    "    \"\"\"THE CODE BELOW DOES WHAT I WANT IT TO DO\"\"\"\n",
    "    #test_start = r'data and code'\n",
    "    #test_end = r'availability'\n",
    "    #idx0 = contents_lower.find(test_start)\n",
    "    #if idx0 != -1:\n",
    "        #idxend = contents_lower.find(test_end, idx0)  # Start searching for test_end from idx0\n",
    "        #if idxend != -1:\n",
    "            #section = article[idx0:idxend]  # \"+ len(test_end)\" to include the end pattern in the extracted section\n",
    "            #print(section)\n",
    "\n",
    "    # If no match is found, return an empty string\n",
    "    return \"\" \n",
    "\n",
    "\n",
    "def get_content_v1(pdf_path, section_patterns):\n",
    "    \"\"\"Get a PDF. \n",
    "    This function is loosely interpreted from Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget\n",
    "    specifically: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "    \n",
    "    Parameters: \n",
    "    :param pdf_path (str): Path to the PDF file.\n",
    "    :param json_file_path (str): Path to the JSON file containing the DOIs of the relevant research articles. \n",
    "    \n",
    "    Returns: \n",
    "    :return: Extracted content or 'Editorial board' if not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        pdf_file = open(pdf_path, 'rb')\n",
    "        pdf_reader = pypdf.PdfReader(pdf_file)\n",
    "        # ORIGINAL \n",
    "        # Read the entire PDF content        \n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            print(page_text)\n",
    "            \n",
    "            # Search for the regex pattern in the page text\n",
    "            #if re.search(target_text_pattern, page_text, re.IGNORECASE):\n",
    "            #    print(f\"Found '{target_text_pattern}' on page {page_num + 1} of {pdf_path}\")\n",
    "\n",
    "            # Extract sections using the provided section patterns\n",
    "            content = get_section(page_text, section_patterns)\n",
    "            if content:\n",
    "                return content\n",
    "        pdf_file.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Try to open the PDF from the alternative directory\n",
    "            alternative_pdf_path = os.path.join(alternative_pdf_directory, os.path.basename(pdf_path))\n",
    "            pdf_file = open(alternative_pdf_path, 'rb')\n",
    "            return 'Editorial board'\n",
    "        except FileNotFoundError:\n",
    "            # If PDF is not found in the original or alternative directory, return 'Editorial board'\n",
    "            return 'Editorial board'\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
