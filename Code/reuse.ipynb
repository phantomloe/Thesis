{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af22f8f9-ff36-4e38-80e7-2bbd7f8e81ad",
   "metadata": {},
   "source": [
    "# Table of contents \n",
    "- [Setup](#setup)\n",
    "    - [Purpose](#purpose)\n",
    "    - [Libraries](#libraries) \n",
    "- [](#)\n",
    "- [](#)\n",
    "- [References](#references)\n",
    "\n",
    "\n",
    "<a name='setup'></a>\n",
    "# 0. Setup \n",
    "\n",
    "This notebook contains the code to investigate the extent of dataset reuse; specifically the reuse of the datasets used in the articles published in NeuroImage in 2022. \n",
    "\n",
    "<a name='purpose'></a>\n",
    "## 0.1. Purpose \n",
    "\n",
    "The purpose of this notebook is to investigate how many other articles across articles available in OpenAlex use the same datasets as those used by the researchers who published their work in NeuroImage in 2022. \n",
    "This notebook draws on the work by The√≥ Sourget (2023a, 2023b), who similarly investigated the usage of a select couple of datasets. \n",
    "\n",
    "<a name='libraries'></a>\n",
    "## 0.2. Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8ec017-fa65-43a7-8999-5aaa8d1eee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import requests\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5565f-d1e9-4269-ac93-ccb0b79398a7",
   "metadata": {},
   "source": [
    "# 1. NeuroImage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6eba3-a098-49f4-b951-b2d7fe223fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse of the links: how many use each of the links in urls = pd.read_csv('../Data/material_URLs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5908b80-1bf7-4e39-92a2-bfca3638f37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66da3e-37cf-4009-be67-f514f5a18c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case exploration: how many places are HCP datasets hosted? Or maybe another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91f4ba-61c6-4c4a-8cc0-90555885c7f7",
   "metadata": {},
   "source": [
    "Some of the URLs take you to the same dataset, e.g., *adni.loni.usc.edu* and *adni.loni.usc.edu/* so I will merge the URLs that this applies to. However, some of the URLs share a root, but shows you different datasets, e.g., all *github.com* and *openneuro.org* links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50346e9d-a19f-438d-9330-41cc16921642",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv('../Data/material_URLs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21946ec5-334f-4e6e-b27b-45a1c1879c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb1262-1f95-4ebd-becb-e77212f9e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277ac84-880b-4937-830a-6b8dd4c4124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_and_derivatives(urls):\n",
    "    \"\"\" \n",
    "    This function performs some initial cleaning of the URLs, removing the beginning (https:// or wwww) and puts all text into lowercase. \n",
    "    Parameters: \n",
    "    :param urls(list): Sorted list of unique URLs from a dataframe\n",
    "    \"\"\"    \n",
    "    # Remove \"https://\", \"http://\", and \"www.\" from the URLs\n",
    "    clean_urls = [url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"www.\", \"\").lower() for url in urls]\n",
    "    \n",
    "    url_dict = {}\n",
    "    for url in clean_urls:\n",
    "        root = url.split('/')[0]\n",
    "        # root = '/'.join(url.split('/'))\n",
    "        if root in url_dict:\n",
    "            url_dict[root].append(url)\n",
    "        else:\n",
    "            url_dict[root] = [url]\n",
    "\n",
    "    grouped_df = pd.DataFrame(url_dict.items(), columns=['Root', 'Derivatives']).sort_values('Root', ascending=True)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba5c77-dbe7-4539-840f-ff8dcbbb4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = find_root_and_derivatives(urls['Material_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fe3ba-e827-4606-8c37-33e2d9196df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e23c9d-5f22-4396-8d49-124ec0140830",
   "metadata": {},
   "source": [
    "Investigating the 315 unique and valid URLs, there are 137 root websites, some leading to the same dataset, and others to different datasets. \n",
    "- IDA (Image & Data Analysis) features 25 studies, including ADNI, AIBL, and a HCP project as well as others (at another place on their website, they say they feature 151 studies)\n",
    "- db.hummanconnectome.org (ConnectomeDB) gives people access to the Human Connectome Project (HCP), both lifespan and hcp-aging\n",
    "- fcon_1000.projects.nitrc.org links to 1000 Functional Connectomes Project, INDI-prospective, INDI-retrospective, and Preprocessed Connectome Project\n",
    "- HCP is actually Human Connectome Projects, and there are 20 connectome studies supported on humanconnectome.org (they call themselves Connectome Coordination Facility (CCF)), leading to e.gg., HCP young adult, aging, development, Amish, epilepsy, and other connectomes.\n",
    "- nda.nih.gov features different datasets, including a couple of HCPs \n",
    "- datalad, github, openneuro, openfmri, osf, and zenodo link to numerous different datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44763367-77db-4c9d-8e07-d5f225831477",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeed4be-975c-4c11-89c6-1b1513b63f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa64ba-8709-4378-8830-00aed2b35f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0edef77c-6033-440b-b4ea-eb30cf0a6536",
   "metadata": {},
   "source": [
    "# 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219417e-3416-4a9d-92b8-ed3f3609400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_info():\n",
    "    \"\"\"Load dataset information from csv at ../Data/data/datasets.csv and convert the doi to openalex_id\n",
    "    @return:\n",
    "        -dictionnary with dataset name as key and dictionary of information as value\n",
    "    \"\"\"\n",
    "    datasets_info = {}\n",
    "    with open('./Resources/data/datasets.csv') as ds_csv:\n",
    "        ds_reader = csv.DictReader(ds_csv)\n",
    "        for ds in ds_reader:\n",
    "            datasets_info[ds[\"name\"]] = {\n",
    "                                            \"doi\":ds[\"doi\"],\n",
    "                                            \"title\":ds[\"paper_title\"],\n",
    "                                            \"name\":ds[\"name\"],\n",
    "                                            \"aliases\":ds[\"aliases\"].split(\",\"),\n",
    "                                            \"url\":ds[\"url\"]\n",
    "                                         }\n",
    "    return datasets_info\n",
    "\n",
    "\n",
    "def search_dataset_in_section(paper_path,section_name,dataset_infos,field=\"name\"):\n",
    "    res = {ds_name:False for ds_name in dataset_infos}\n",
    "    try:\n",
    "        text = pdf_util.extract_section(paper_path,section_name)\n",
    "        if text:\n",
    "            text = \" \".join(text)\n",
    "            for ds_name in dataset_infos:\n",
    "                searched_str = dataset_infos[ds_name][field]\n",
    "                if re.search(f\"(?<![^_\\\\W]){searched_str}(?![^_\\\\s\\\\d\\\\.\\\\),'])\",text):\n",
    "                    res[ds_name] = True\n",
    "                else:\n",
    "                    for alias in dataset_infos[ds_name][\"aliases\"]:\n",
    "                        if re.search(f\"(?<![^_\\\\W]){alias}(?![^_\\\\s\\\\d\\\\.\\\\),'])\",text):\n",
    "                            res[ds_name] = True\n",
    "                            break\n",
    "    except:\n",
    "        print(f\"Unreadable paper:{paper_path}\")\n",
    "    \n",
    "    return res\n",
    "def main():\n",
    "    #Load dataset_info\n",
    "    ds_info = load_datasets_info()\n",
    "\n",
    "    #Load list of downloaded pdf \n",
    "    papers_pdf_path = glob.glob(\"./Results/extraction/fulltext/*.pdf\")\n",
    "    data_csv = []\n",
    "    \n",
    "    print(\"Search in abstract started\")\n",
    "    for paper in tqdm(papers_pdf_path):\n",
    "        paper_name = paper.split(\"/\")[-1].removesuffix(\"pdf\")\n",
    "        #Get the abstract part\n",
    "        search_res = search_dataset_in_section(paper,\"abstract\",ds_info)\n",
    "        search_res[\"name\"] = paper_name\n",
    "        data_csv.append(search_res)\n",
    "    \n",
    "    with open(\"./Results/extraction/fulltext_datasets_abstract.csv\",\"w\",newline=\"\") as ft_ds_ref:\n",
    "        fields = [\"name\"]\n",
    "        fields += [ds_name for ds_name in ds_info]\n",
    "        \n",
    "        writer = csv.DictWriter(ft_ds_ref, fieldnames=fields)\n",
    "        # Write the header row (column names)\n",
    "        writer.writeheader()\n",
    "        for paper in data_csv:\n",
    "            writer.writerow(paper)\n",
    "    \n",
    "    data_csv = []\n",
    "    print(\"Search in references started\")\n",
    "    for paper in tqdm(papers_pdf_path):\n",
    "        paper_name = paper.split(\"/\")[-1].removesuffix(\"PDF\")\n",
    "        #Get the abstract part\n",
    "        search_res = search_dataset_in_section(paper,\"references\",ds_info,\"title\")\n",
    "        search_res[\"name\"] = paper_name\n",
    "        data_csv.append(search_res)\n",
    "            \n",
    "    with open(\"./Results/extraction/fulltext_datasets_references.csv\",\"w\",newline=\"\") as ft_ds_ref:\n",
    "        fields = [\"name\"]\n",
    "        fields += [ds_name for ds_name in ds_info]\n",
    "        \n",
    "        writer = csv.DictWriter(ft_ds_ref, fieldnames=fields)\n",
    "        # Write the header row (column names)\n",
    "        writer.writeheader()\n",
    "        for paper in data_csv:\n",
    "            writer.writerow(paper)\n",
    "    \n",
    "    # data_csv = []\n",
    "    # print(\"Search in results started\")\n",
    "    # for paper in tqdm(papers_pdf_path):\n",
    "    #     paper_name = paper.split(\"/\")[-1].removesuffix(\"PDF\")\n",
    "    #     #Get the abstract part\n",
    "    #     search_res = search_dataset_in_section(paper,\"results\",ds_info)\n",
    "    #     search_res[\"name\"] = paper_name\n",
    "    #     data_csv.append(search_res)\n",
    "            \n",
    "    # with open(\"./Results/extraction/fulltext_datasets_results.csv\",\"w\",newline=\"\") as ft_ds_ref:\n",
    "    #     fields = [\"name\"]\n",
    "    #     fields += [ds_name for ds_name in ds_info]\n",
    "        \n",
    "    #     writer = csv.DictWriter(ft_ds_ref, fieldnames=fields)\n",
    "    #     # Write the header row (column names)\n",
    "    #     writer.writeheader()\n",
    "    #     for paper in data_csv:\n",
    "    #         writer.writerow(paper)\n",
    "\n",
    "    data_csv = []\n",
    "    print(\"Search in method started\")\n",
    "    for paper in tqdm(papers_pdf_path):\n",
    "        paper_name = paper.split(\"/\")[-1].removesuffix(\"PDF\")\n",
    "        #Get the abstract part\n",
    "        search_res = search_dataset_in_section(paper,\"method\",ds_info)\n",
    "        search_res[\"name\"] = paper_name\n",
    "        data_csv.append(search_res)\n",
    "            \n",
    "    with open(\"./Results/extraction/fulltext_datasets_method.csv\",\"w\",newline=\"\") as ft_ds_ref:\n",
    "        fields = [\"name\"]\n",
    "        fields += [ds_name for ds_name in ds_info]\n",
    "        \n",
    "        writer = csv.DictWriter(ft_ds_ref, fieldnames=fields)\n",
    "        # Write the header row (column names)\n",
    "        writer.writeheader()\n",
    "        for paper in data_csv:\n",
    "            writer.writerow(paper)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d70302-bcd0-4c8b-a86c-b9de08dfa906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469046d-efd3-4c53-acf5-0780359ff3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38359d5b-f442-40fa-80b5-52cb47bcb1ce",
   "metadata": {},
   "source": [
    "<a name='references'></a>\n",
    "# References\n",
    "- Sourget, T. (2023a). Public_Medical_Datasets_References [Jupyter Notebook]. https://github.com/TheoSourget/Public_Medical_Datasets_References (Original work published 2023)\n",
    "- Sourget, T. (2023b). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
