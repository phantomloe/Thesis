DOI,URL,sentence,Method,Material,Supplement
10.1016/j.neuroimage.2021.118798,osf.io/8nbd4,Data and code availability statement The data and analysis scripts are available on OSF: https://osf.io/8nbd4/.,1,1,0
10.1016/j.neuroimage.2021.118811,csie.ntu.edu.tw/∼cjlin/libsvm,"We carried out MVPA using the PyMVPA software package (Hanke et al., 2009) and the LibSVM’s implementation of the linear support vector ma-chine classiﬁer (LSVM www.csie.ntu.edu.tw/∼cjlin/libsvm).",1,0,0
10.1016/j.neuroimage.2022.118920,orcid.org/0000-0002-9117-4449,"Tavor). 1 ORCID iD https://orcid.org/0000-0002-9117-4449 used as a ﬁngerprint to detect identity (Finn et al., 2015), to correlate with behavioral and demographic measurements (Smith et al., 2015) as well as personality traits (Cai et al., 2020; Dubois et al., 2018a), and was also linked to individuals’ genetic proﬁle (Colclough et al., 2017).",0,0,0
10.1016/j.neuroimage.2022.118980,github.com/rdelahoy/dmn_eﬀective_connectivity,"Data and code for the eﬀective connectivity analysis are available at https://github.com/RDelahoy/DMN_eﬀective_connectivity., Data code/availability statement Data and code for the eﬀective connectivity analysis are available at https://github.com/RDelahoy/DMN_eﬀective_connectivity.",1,0,0
10.1016/j.neuroimage.2022.118980,paradigmexperiments.com,"The task was presented using Paradigm soft-ware (http://paradigmexperiments.com) on a Dell computer via MRI-compatible high-resolution goggles (VisuaStim Digital System, Reso-nance Technology Inc., Northridge, CA).",1,0,0
10.1016/j.neuroimage.2022.118980,nitrc.org/projects/bnv,"Visualization of results provided by BrainNet Viewer (http://www.nitrc.org/projects/bnv/)., Visualization of results provided by BrainNet Viewer (http://www.nitrc.org/projects/bnv/).",1,0,0
10.1016/j.neuroimage.2022.118986,ieeg.org,"NeuroImage 254 (2022) 118986 tions) allowed their de-identiﬁed intracranial EEG (iEEG) data to be publicly available for research purposes on the International Epilepsy Electrophysiology Portal (https://www.ieeg.org) Kini et al., iEEG snippets used speciﬁ-cally in this manuscript are also available, while full iEEG recordings are publicly available at https://www.ieeg.org., iEEG snippets used speciﬁ-cally in this manuscript are also available, while full iEEG recordings are publicly available at https://www.ieeg.org.",0,1,0
10.1016/j.neuroimage.2022.118986,dsi-studio.labsolver.org,"Structural network generation DSI-Studio (http://dsi-studio.labsolver.org , version: December 2020) was used to reconstruct the orientation density functions within each voxel using generalized q-sample imaging with a diﬀusion sam-pling length ratio of 1.25 (Fang-Cheng et al., 2010).",1,0,0
10.1016/j.neuroimage.2022.118986,surfer.nmr.mgh.harvard.edu,"The De-strieux and DKT atlases are also structural atlases, and already in-corporated into one of the most commonly used neuroimaging soft-ware, FreeSurfer (https://surfer.nmr.mgh.harvard.edu).",1,0,0
10.1016/j.neuroimage.2022.118986,fsl.fmrib.ox.ac.uk/fsl/fslwiki,(2017) (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki).,0,0,0
10.1016/j.neuroimage.2022.118986,ieeg.org,All electrode coordinates and labels were saved and matched with the electrode names on IEEG.org.,0,0,1
10.1016/j.neuroimage.2022.119016,miniscope.org/index.php/guides_and_tutorials,"2.4.4. 1 ‐photon calcium imaging We used the UCLA Miniscope V3 and Data Acquisition Sys-tem (http://miniscope.org/index.php/Guides_and_Tutorials ; Cai et al., 2016).",0,0,1
10.1016/j.neuroimage.2022.119016,francismanno.github.io/fmanno,"Data and code availability statement Supplementary Information and raw data can be downloaded from the website: https://francismanno.github.io/fmanno/, https://osf.io/m3p7t/, and https://drive.google.com/drive/folders/1WICx70 WiHDCI_MZrWipQ_q17mdJ8KDDC?usp = sharing.",0,1,0
10.1016/j.neuroimage.2022.119016,osf.io/m3p7t,"Data and code availability statement Supplementary Information and raw data can be downloaded from the website: https://francismanno.github.io/fmanno/, https://osf.io/m3p7t/, and https://drive.google.com/drive/folders/1WICx70 WiHDCI_MZrWipQ_q17mdJ8KDDC?usp = sharing.",0,1,0
10.1016/j.neuroimage.2022.119016,drive.google.com/drive/folders/1wicx70,"Data and code availability statement Supplementary Information and raw data can be downloaded from the website: https://francismanno.github.io/fmanno/, https://osf.io/m3p7t/, and https://drive.google.com/drive/folders/1WICx70 WiHDCI_MZrWipQ_q17mdJ8KDDC?usp = sharing.",0,1,0
10.1016/j.neuroimage.2022.119016,ﬁl.ion.ucl.ac.uk/spm/.,"rsfMRI connectivity analysis All data were processed using custom scripts in MAT-LAB (Math Works, Natick, MA), SPM 12 (Welcome Depart-ment of Imaging Neuroscience, University College London; http://www.ﬁl.ion.ucl.ac.uk/spm/.) and the network-based statistics (NBS) toolbox (Zalesky et al., 2010).",1,0,0
10.1016/j.neuroimage.2022.119016,github.com/jinghaolu/min1pipe,"2.4.5. 1 ‐photon calcium imaging analysis pipeline The analysis steps were modiﬁed for dCA1 hippocam-pus based on the previous protocol (Lu et al., 2018 ; https://github.com/JinghaoLu/MIN1PIPE).",1,0,0
10.1016/j.neuroimage.2022.119016,itksnap.org,"Network based statistics (NBS) : 142 region of interest (ROI) masks were drawn manually (Zerbi et al., 2019 ; Chelini and Zerbi et al., 2019) using ITK-SNAP (http://www.itksnap.org/) including the follow-ing modules: hippocampal formation (HPF), isocortex, cortical subplate, pallidum, striatum, midbrain, thalamus, hypothalamus (HPA) and hind-brain (Fig.",1,0,0
10.1016/j.neuroimage.2022.119016,imaging.org.au/ambmc/model,"Functional images were co-registered to the skull-stripped anatomical images and normalized to the Australian Mouse Brain Mapping Consortium template (Janke et al., 2015 ; www.imaging.org.au/AMBMC/Model/) using SPM12.",1,0,0
10.1016/j.neuroimage.2022.119016,github.com/elifesciences-publications/imaging_analysis_pipeline,"The data analysis used custom-written scripts and routines in MATLAB based on previous code (LeMessurier et al., 2019 ; https://github.com/elifesciences-publications/imaging_analysis_pipeline).",1,0,0
10.1016/j.neuroimage.2022.119021,surfer.nmr.mgh.harvard.edu/optseq,"The events were ordered in an optimal rapid event-related design speciﬁed by optseq2 (Dale, 1999 ; https://surfer.nmr.mgh.harvard.edu/optseq).",1,0,1
10.1016/j.neuroimage.2022.119021,ﬁl.ion.ucl.ac.uk/spm,"Lesion maps were created semi-automatically from patients’ clinical scans using the Clusterize SPM toolbox (SPM12; Clas et al., 2012 ; de Haan et al., 2015) and were then normalized to an age-related template with the algorithm provided by the Clinical toolbox (Rorden et al., 2012) based on SPM8 (http://ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2022.119021,marsbar.sourceforge.net,"We used MarsBar (http://marsbar.sourceforge.net) to extract the mean percent signal change from individual ventral objects, faces and places ROIs for all four experimental conditions.",1,0,0
10.1016/j.neuroimage.2022.119021,ﬁl.ion.ucl.ac.uk/spm,fMRI data analysis Data pre-processing and model estimation were performed using SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119021,3dcadbrowser.com,"In the ﬁrst fMRI experiment, participants were presented with computer-generated object stimuli in diﬀerent viewing conditions that were taken from the Object Databank (http://wiki.cnbc.cmu.edu/Objects) and from 3dcadbrowser (https://www.3dcadbrowser.com).",0,0,1
10.1016/j.neuroimage.2022.119021,wiki.cnbc.cmu.edu/objects,"In the ﬁrst fMRI experiment, participants were presented with computer-generated object stimuli in diﬀerent viewing conditions that were taken from the Object Databank (http://wiki.cnbc.cmu.edu/Objects) and from 3dcadbrowser (https://www.3dcadbrowser.com).",0,0,1
10.1016/j.neuroimage.2022.119055,osf.io/j9vka/supplementary,"Data and code availability The data used in the study as well as the software and scripts used to compute the analysis have been made publicly available via the Open Science Framework and can be accessed at: https://osf.io/j9vka/Supplementary materials Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.neuroimage.2022.119055.",1,1,0
10.1016/j.neuroimage.2022.119094,coins.trendscenter.org,"Data/code availability statement The data used in this article are openly available through the COINS framework (https://coins.trendscenter.org/)., Data availability The data used in this article are openly available through the COINS framework (https://coins.trendscenter.org/).",0,1,0
10.1016/j.neuroimage.2022.119097,ibeat.cloud,"First, all the images were prepro-cessed and segmented using an infant brain dedicated toolbox iBEAT V2.0 Cloud (Wang et al., 2018) (http://www.ibeat.cloud/).",1,0,0
10.1016/j.neuroimage.2022.119097,nitrc.org/projects/uncbcp_4d_atlas,"Therefore, our 4D atlas is a good choice for studying early brain growth patterns and other clinical applications, which will be publicly available on NITRC (https://www.nitrc.org/projects/uncbcp_4d_atlas/) website to remark-ably facilitate the studies on early human brain development.",1,0,0
10.1016/j.neuroimage.2022.119116,r-project.org,"URL ""https://www.R-project.org"".)) and lme4 package (v1.1-27.1, Bates et al., 2014 , p.",0,0,0
10.1016/j.neuroimage.2022.119116,surfer.nmr.mgh.harvard.edu,"They were based on several image processing pipelines, us-ing Brainvisa (Rivière et al., 2011 , https://brainvisa.info/web/) and Freesurfer (http://surfer.nmr.mgh.harvard.edu/), that were built to: 1) compute anatomical models from the structural MRI preoperative se-quence, 2) normalize this sequence on MNI template, 3) coregister pre-and post-operative sequences in the patient native space with the struc-tural preoperative MRI as reference, using a block matching algorithm, 4) automatically localize depth sEEG electrodes on CT postoperative se-quences, by segmentation of electrode artifacts present on the postop-erative TDM and their classiﬁcation using their distance to the theoret-ical trajectories planned on the stereotactic guidance device (ROSA or Leksell), 5) label all the contacts using the MNI atlases and the patient speciﬁc anatomical models.",1,0,0
10.1016/j.neuroimage.2022.119116,brainvisa.info/web,"They were based on several image processing pipelines, us-ing Brainvisa (Rivière et al., 2011 , https://brainvisa.info/web/) and Freesurfer (http://surfer.nmr.mgh.harvard.edu/), that were built to: 1) compute anatomical models from the structural MRI preoperative se-quence, 2) normalize this sequence on MNI template, 3) coregister pre-and post-operative sequences in the patient native space with the struc-tural preoperative MRI as reference, using a block matching algorithm, 4) automatically localize depth sEEG electrodes on CT postoperative se-quences, by segmentation of electrode artifacts present on the postop-erative TDM and their classiﬁcation using their distance to the theoret-ical trajectories planned on the stereotactic guidance device (ROSA or Leksell), 5) label all the contacts using the MNI atlases and the patient speciﬁc anatomical models.",1,0,0
10.1016/j.neuroimage.2022.119116,nitrc.org,The 3D ﬁgure was realized using BrainNet viewer (www.nitrc.org).,1,0,0
10.1016/j.neuroimage.2022.119116,danielsoleil.com,"EMC-ZM (Electromagnetic Compatibility -Zero Method, Soleil et al., 1992 , http://danielsoleil.com) measurements were performed and uncovered that the EEG recording room was full of noisy sources from electrical power distribution.",1,0,0
10.1016/j.neuroimage.2022.119116,github.com/stephenwhitmarsh/epicode,"The analyses pipeline was implemented using custom MATLAB scripts (https://github.com/stephenwhitmarsh/EpiCode), and used FieldTrip (Oostenveld et al., 2011), a MATLAB (The Mathworks Inc., Natick, Massachusetts) toolbox for MEEG and spike analyses.",1,0,0
10.1016/j.neuroimage.2022.119122,journals.elsevier.com/neuroimage,journals.elsevier.com/neuroimage  and Cerebral Cortex  https://academic.oup.com/cercor  webpages.,0,0,0
10.1016/j.neuroimage.2022.119122,academic.oup.com/cercor,journals.elsevier.com/neuroimage  and Cerebral Cortex  https://academic.oup.com/cercor  webpages.,0,0,0
10.1016/j.neuroimage.2022.119135,openneuro.org/datasets/ds003959/versions/1.0.1,Data availability The data sets generated and analysed during the current study are available on OpenNeuro: https://openneuro.org/datasets/ds003959/versions/1.0.1.,0,1,0
10.1016/j.neuroimage.2022.119135,scalablebrainatlas.incf.org/mouse/aba_v3,"Registration To create the template, the P56 Mouse Brain Atlas in NIFTY format (https://scalablebrainatlas.incf.org/mouse/ABA_v3) was downsampled to match the resolution of the dMRI data, namely 0.12 ×0.12 ×0.4 mm.",1,0,0
10.1016/j.neuroimage.2022.119135,github.com/palombom,Code availability SANDI code is available on GitHub at https://github.com/palombom.,1,0,0
10.1016/j.neuroimage.2022.119140,osf.io/jkcys,Data availability statement Data and code are available online (https://osf.io/jkcys/).,1,1,0
10.1016/j.neuroimage.2022.119140,ﬁl.ion.ucl.ac.uk,"Statis-tical Parametric Mapping software SPM12 (Welcome to Department of Imaging Neuroscience, London, http://www.ﬁl.ion.ucl.ac.uk) was used to process the fMRI data.",1,0,0
10.1016/j.neuroimage.2022.119164,github.com/cobralab/documentation/wiki/motion-quality-control-(qc)-manual,"Quality control of structural MRIs, according to the CoBrA lab-oratory protocol (https://github.com/CoBrALab/documentation/wiki/Motion-Quality-Control-(QC)-Manual) resulted in the exclusion of data from 37 participants (19 young, 4 middle age and 14 older adults; see Bedford, 2017 ; Bedford et al., 2019 ; Snytte et al., 2020 for more infor-mation).",0,0,1
10.1016/j.neuroimage.2022.119203,surfer.nmr.mgh.harvard.edu,"Surface parcellation and Source reconstruction of MEEG data Anatomical preprocessing included an automatic volumetric seg-mentation of the individual MRIs, surface reconstruction and sur-face parcellations, using FreeSurfer image analysis suite (Fischl, 2012) (http://surfer.nmr.mgh.harvard.edu/).",1,0,0
10.1016/j.neuroimage.2022.119203,martinos.org/mne/stable/index,MNE software (http://martinos.org/mne/stable/index.,1,0,0
10.1016/j.neuroimage.2022.119212,preprocessed-connectomes-project.org/and,"The data analyzed in this study were all down-loaded from ABIDE repositories: http://preprocessed-connectomes-project.org/and http://fcon_1000.projects.nitrc.org/indi/abide/., The data analyzed in this study were all down-loaded from ABIDE repositories: http://preprocessed-connectomes-project.org/and http://fcon_1000.projects.nitrc.org/indi/abide/.",0,1,0
10.1016/j.neuroimage.2022.119212,balsa.wustl.edu,"The cortex-wide FC ma-trix was constructed by Pearson’s correlation of time series between every pair of the two brain areas, for which boundaries were de-ﬁned by a multimodal parcellation atlas of Human Connectome Project (https://balsa.wustl.edu/) (Glasser et al., 2016).",1,0,0
10.1016/j.neuroimage.2022.119212,readthedocs.io/en/latest,"readthedocs.io/en/latest/), and the FRF was derived from (https://github.com/DCAN-Labs/functional-random-forest)., readthedocs.io/en/latest/), and the FRF was derived from (https://github.com/DCAN-Labs/functional-random-forest).",1,0,0
10.1016/j.neuroimage.2022.119212,github.com/dcan-labs/functional-random-forest,"readthedocs.io/en/latest/), and the FRF was derived from (https://github.com/DCAN-Labs/functional-random-forest)., readthedocs.io/en/latest/), and the FRF was derived from (https://github.com/DCAN-Labs/functional-random-forest).",1,0,0
10.1016/j.neuroimage.2022.119212,preprocessed-connectomes-project.org,"Data preprocessing The ABIDE-I database provided preprocessed T1w and rs-fMRI data, which are openly shared through the Preprocessed Connectomes initia-tive (http://preprocessed-connectomes-project.org/) (Craddock et al., 2013)., The data analyzed in this study were all down-loaded from ABIDE repositories: http://preprocessed-connectomes-project.org/and http://fcon_1000.projects.nitrc.org/indi/abide/., The data analyzed in this study were all down-loaded from ABIDE repositories: http://preprocessed-connectomes-project.org/and http://fcon_1000.projects.nitrc.org/indi/abide/.",0,1,0
10.1016/j.neuroimage.2022.119212,fcon_1000.projects.nitrc.org/indi/abide,"The data analyzed in this study were all down-loaded from ABIDE repositories: http://preprocessed-connectomes-project.org/and http://fcon_1000.projects.nitrc.org/indi/abide/., The data analyzed in this study were all down-loaded from ABIDE repositories: http://preprocessed-connectomes-project.org/and http://fcon_1000.projects.nitrc.org/indi/abide/.",0,1,0
10.1016/j.neuroimage.2022.119212,github.com/mica-mni/brainspace,"We derived functional gradients using principal component analysis (PCA), via Brainspace (https://github.com/MICA-MNI/BrainSpace) (Vos de Wael et al., 2020), as this has been demonstrated as having generally a high reliability and prediction power in our recent biomarker study (S.J.",1,0,0
10.1016/j.neuroimage.2022.119212,github.com/gudtls17/asd,", statistical comparisons, con-trol analysis, main running codes) in this study will be uploaded upon the acceptance of this paper (https://github.com/gudtls17/ASD., , statistical comparisons, con-trol analysis, main running codes) in this study will be uploaded upon the acceptance of this paper (https://github.com/gudtls17/ASD.",1,0,0
10.1016/j.neuroimage.2022.119245,github.com/sorenwt/oscifrac2021,All code for the above analyses (including preprocessing of the CamCAN data) is available online at http://www.github.com/SorenWT/oscifrac2021.,1,0,0
10.1016/j.neuroimage.2022.119245,github.com/sorenwt/oscifrac2021,Code for this project is freely available at https://github.com/SorenWT/oscifrac2021.,1,0,0
10.1016/j.neuroimage.2022.119245,db.humanconnectome.org,Data and code availability Data from the Human Connectome Project can be accessed at https://db.humanconnectome.org/.,0,1,0
10.1016/j.neuroimage.2022.119245,camcan-archive.mrc-cbu.cam.ac.uk/dataaccess,Data from the Cambridge Center for Aging Neuroscience project can be accessed at https://camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/.,0,1,0
10.1016/j.neuroimage.2022.119278,afni.nimh.nih.gov/pub/dist/doc/program_help/3dtoutcount.html,Out-liers were deﬁned in relation to the median absolute deviation of the signal time course (see afni.nimh.nih.gov/pub/dist/doc/program_help/3dToutcount.html for outlier deﬁnition).,0,0,1
10.1016/j.neuroimage.2022.119290,cai2r.net,"cai2r.net), a NIBIB Biomedical Technology Resource Center (NIH P41-EB017183).",0,0,0
10.1016/j.neuroimage.2022.119290,github.com/nyu-diﬀusionmri/smi,"These are publicly available as part of the SMI (standard model imaging) toolbox at https://github.com/NYU-DiﬀusionMRI/SMI., All processing codes for the estimation of the Standard Model (SMI toolbox) are available at https://github.com/NYU-DiﬀusionMRI/SMI.",1,0,0
10.1016/j.neuroimage.2022.119331,mcgill.ca/bic/software/tools-data-analysis/anatomical-mri/atlases/icbm152lin,"Here, we used the Boundary Element Method (BEM) head model ﬁtted to the ICBM MRI template (Kötter et al., 2001), downloaded from https://www.mcgill.ca/bic/software/tools-data-analysis/anatomical-mri/atlases/icbm152lin using the OpenMEEG toolbox (Gramfort et al., 2010), and used the Electrical Geodesic Inc (EGI) conﬁguration for the EEG electrodes.",1,0,0
10.1016/j.neuroimage.2022.119331,incr.fr,"Funding JD was funded by the Rennes Clinical Neuroscience Institute (INCR: www.incr.fr)., We also would like to thank Bretagne Atlantique Ambition (BAA) as well as the Rennes Clinical Neu-roscience Institute (INCR: www.incr.fr), and the Ille-et-Vilaine Parkin-sonian Association (APIV) who funded this work.",0,0,1
10.1016/j.neuroimage.2022.119331,github.com/jduprez/eegcog-control_dynfc_pd,"Code availability All the Matlab and R codes used for source reconstruction, dFC, tICA, backﬁtting and microstate metrics, as well as all subsequent sta-tistical analyses are publicly available at https://github.com/jduprez/EEGcog-control_dynFC_PD., Data and code availability statement All the codes used to perform the analyses are available at https://github.com/jduprez/EEGcog-control_dynFC_PD.",1,0,0
10.1016/j.neuroimage.2022.119332,fsl.fmrib.ox.ac.uk/fsl/fslwiki/melodic,Preprocessed data from the ADHD and TD samples were concate-nated and entered into a group independent component analysis (ICA) to identify large-scale networks in the combined population (MELODIC; http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MELODIC).,1,0,0
10.1016/j.neuroimage.2022.119332,cran.rproject.org/web/packages/caret,The aforementioned classiﬁcation analysis was performed using the caret R package (https://cran.rproject.org/web/packages/caret/).,1,0,0
10.1016/j.neuroimage.2022.119332,ﬁl.ion.ucl.ac.uk/spm,"fMRI data pre ‐processing and ICA analysis A standard preprocessing procedure was implemented using SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm/), including slice-timing correction, realignment, normalization, spatial smoothing (6-mm smoothing ker-nel), regression of nuisance variables (24 motion parameters, white mat-ter, and cerebrospinal ﬂuid signals), and bandpass ﬁltering (0.008 Hz < f < 0.1Hz) (Cai et al., 2018 , Supekar et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119353,s.lifestyle,"Wen, J., Thibeau-Sutre, E., Diaz-Melo, M., Samper-Gonzalez, J., Routier, A., Bot-tani, S.Lifestyle ﬂagship study of, a, 2020.",0,0,0
10.1016/j.neuroimage.2022.119353,adni.loni.usc.edu,"∗ Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf., GE(N = 232) Philips (N = 172) Siemens (N = 537) Total (N = 941) p-value ADNI Memory 0.043 -Mean (SD) 0.38(0.95) 0.36(0.92) 0.52(0.90) 0.45(0.91) Diagnosis, n(%) 0.034 -CN 99(42.7%) 64(37.2%) 254(47.3%) 417(44.3%) -MCI 87(37.5%) 77(44.8%) 214(39.9%) 378(40.2%) -AD 46(19.8%) 31(18.0%) 69(12.8%) 146(15.5%) Age 0.803 -Mean (SD) 72.62(7.13) 72.50(6.86) 72.26(7.36) 72.40(7.21) Gender, n(%) 0.307 -Female 109(47.0%) 83(48.3%) 282(52.5%) 474(50.4%) -Male 123(53.0%) 89(51.7%) 255(47.5%) 467(49.6%) PT Education 0.665 -Mean (SD) 16.31(2.64) 16.38(2.58) 16.49(2.46) 16.42(2.53) NART IQ 0.365 -N-Miss 10 5 2 17 -Mean (SD) 115.53 (11.55) 116.05 (11.31) 116.77 (11.07) 116.35 (11.23) Detailed inclusion and exclusion criteria for the ADNI study can be found at adni.loni.usc.edu., More de-tails concerning the sMRI images is available on the ADNI homepage (http://adni.loni.usc.edu/methods/mri-tool/mri-analysis/).",0,1,0
10.1016/j.neuroimage.2022.119353,surfer.nmr.mgh.harvard.edu,"For all images that passed quality check, cross-sectional image processing was performed using FreeSurfer Ver-sion 7.1.1 (https://surfer.nmr.mgh.harvard.edu/).",1,0,0
10.1016/j.neuroimage.2022.119353,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.,0,0,1
10.1016/j.neuroimage.2022.119353,ncbi.nlm.nih.gov/pubmed/11939702,Retrieved from https://www.ncbi.nlm.nih.gov/pubmed/11939702.,0,0,0
10.1016/j.neuroimage.2022.119353,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119353,surfer.nmr.mgh.harvard.edu,Neuroimaging data processing Each subject’s structural T1 scan was reconstructed using FreeSurfer v7.1.1 (http://surfer.nmr.mgh.harvard.edu/).,1,0,0
10.1016/j.neuroimage.2022.119353,github.com/xizhu-cu/transfer-learning-submission,"The code of the transfer learn-ing is available at https://github.com/XiZhu-CU/Transfer-Learning-Submission., Data and code availability statement MATLAB scripts for cascade neural networks, transfer learning, statistical evaluations, and visualizations can be found here: https://github.com/XiZhu-CU/Transfer-Learning-Submission.",1,0,0
10.1016/j.neuroimage.2022.119353,ncbi.nlm.nih.gov/pubmed/8139057,Retrieved from https://www.ncbi.nlm.nih.gov/pubmed/8139057.,0,0,0
10.1016/j.neuroimage.2022.119353,reserveandresilience.com,(https://reserveandresilience.com/).,0,0,0
10.1016/j.neuroimage.2022.119354,mccauslandcenter.sc.edu/mricrogl,We used MRIcroGL (www.mccauslandcenter.sc.edu/mricrogl/) to visualize the results.,1,0,0
10.1016/j.neuroimage.2022.119354,sdmproject.com,"Seed-based d mapping The SDM-PSI (www.sdmproject.com) software imputes the brain maps of statistical eﬀects for each study and conducts a standard random-eﬀects meta-analysis to test whether the eﬀects are diﬀerent from zero (Albajes-Eizagirre et al., 2019a).",1,0,0
10.1016/j.neuroimage.2022.119418,humanconnectome.org/software/connectome-workbench,"To further in-crease the signal-to-noise ratio, an additional smoothing of 4mm FWHM was applied to the MSMAll-registered data (with subcor-tical structures smoothed within parcel boundaries, and cortical data smoothed in 2D on the surface) using the Connectome Work-bench (https://www.humanconnectome.org/software/connectome-workbench).",1,0,0
10.1016/j.neuroimage.2022.119418,github.com/yingqiuz/predict-task-individual-variability,Code availability Code for the model and analysis in this paper can be found in https://github.com/yingqiuz/predict-task-individual-variability.,1,0,0
10.1016/j.neuroimage.2022.119418,humanconnectome.org/study/hcp-young-adult,"Human connectome project data We used the MSMAll-registered data provided by the Human Connectome Project (HCP), S1200 Release (https://www.humanconnectome.org/study/hcp-young-adult).",0,1,0
10.1016/j.neuroimage.2022.119418,git.fmrib.ox.ac.uk/rezvanh/sprofumo_develop,It is currently available in https://git.fmrib.ox.ac.uk/rezvanh/sprofumo_develop.,0,0,0
10.1016/j.neuroimage.2022.119494,ibenfund.com/ressources,These templates are made available to the scientiﬁc community (https://www.ibenfund.com/ressources) and could be used in a regis-tration pipeline.,1,0,0
10.1016/j.neuroimage.2022.119494,github.com/iben-foundation/iben-atlas,NeuroImage 260 (2022) 119494 Data and code availability Statement All atlas ﬁles and python codes are currently available at this address: https://github.com/iBen-foundation/iBen-atlas.,1,0,0
10.1016/j.neuroimage.2022.119507,enigma.ini.usc.edu/protocols/imaging-protocols,"To check the quality of the FreeSurfer outputs, we followed the Enigma protocol (http://enigma.ini.usc.edu/protocols/imaging-protocols/).",0,0,1
10.1016/j.neuroimage.2022.119507,osf.io/cw8t2,"See https://osf.io/cw8t2/for our anal-ysis code., Data availability The code to reproduce the analyses shown in the paper in full is available free of charge from https://osf.io/cw8t2/.",1,0,0
10.1016/j.neuroimage.2022.119507,osf.io/cw8t2/for,See https://osf.io/cw8t2/for our anal-ysis code.,1,0,0
10.1016/j.neuroimage.2022.119513,github.com/joramvd/tfdecomp,Code and data availability The code we used for preprocessing EEG are available at https://github.com/joramvd/eegpreproc; The code we used for performing time-frequency analysis are available at https://github.com/joramvd/tfdecomp.,1,0,0
10.1016/j.neuroimage.2022.119513,github.com/joramvd/eegpreproc,Code and data availability The code we used for preprocessing EEG are available at https://github.com/joramvd/eegpreproc; The code we used for performing time-frequency analysis are available at https://github.com/joramvd/tfdecomp.,1,0,0
10.1016/j.neuroimage.2022.119513,i.e.j.de,"Vries, I.E.J.de, Driel, J.van, Olivers, C.N.L., 2017.",0,0,0
10.1016/j.neuroimage.2022.119513,github.com/joramvd/tfdecomp,"Time-frequency analysis We decomposed the epoched EEG time series into time-frequency representations with custom-written MATLAB scripts (github.com/joramvd/tfdecomp)., Code and data availability The code we used for preprocessing EEG are available at https://github.com/joramvd/eegpreproc; The code we used for performing time-frequency analysis are available at https://github.com/joramvd/tfdecomp.",1,0,0
10.1016/j.neuroimage.2022.119528,valdo.grand-challenge.org/description,"To this end, the ‘Where is VALDO –V a s c u l a r Lesions Detection Challenge’ (https://valdo.grand-challenge.org/Description/) is a good initiative.",0,1,0
10.1016/j.neuroimage.2022.119528,appsrv.cse.cuhk.edu.hk/∼qdou/cmb-3dcnn/cmb-3dcnn.html,(2021) https://appsrv.cse.cuhk.edu.hk/∼qdou/cmb-3dcnn/cmb-3dcnn.html code and labelled data for CMB segmentation Dou et al.,1,0,0
10.1016/j.neuroimage.2022.119528,github.com/yonsei-milab/cerebral-microbleeds-detection,https://github.com/Yonsei-MILab/Cerebral-Microbleeds-Detection code for CMB segmentation Al-Masni et al.,1,0,0
10.1016/j.neuroimage.2022.119528,3.1.3.1,"In addition, as introduced in Section 3.1.3.1 ‘Classical image pro-cessing’, RST outputs voxel-wise maps with intensities indicating local radial symmetry, and can therefore be used as shape features in clas-sical machine learning approaches (Roy et al., 2015).",0,0,0
10.1016/j.neuroimage.2022.119528,github.com/hjkuijf/mixlacune,"(2016) https://github.com/hjkuijf/MixLacune code for lacune segmentation –https://valdo.grand-challenge.org/""Where is VALDO"" challenge, including segmenting PVS, CMB and lacunes, with example training data –https://github.com/hjkuijf/MixMicrobleed ; https://hub.docker.com/r/hjkuijf/mixmicrobleed Code for a deep learning-based toolbox to detect and segment CMB, participating in 2021 ‘Where is VALDO’ challenge.",1,0,0
10.1016/j.neuroimage.2022.119528,hub.docker.com/r/hjkuijf/mixmicrobleed,"(2016) https://github.com/hjkuijf/MixLacune code for lacune segmentation –https://valdo.grand-challenge.org/""Where is VALDO"" challenge, including segmenting PVS, CMB and lacunes, with example training data –https://github.com/hjkuijf/MixMicrobleed ; https://hub.docker.com/r/hjkuijf/mixmicrobleed Code for a deep learning-based toolbox to detect and segment CMB, participating in 2021 ‘Where is VALDO’ challenge., (2021) https://github.com/hjkuijf/MixMicrobleedNet ; https://hub.docker.com/r/hjkuijf/mixmicrobleednet Code for a deep learning-based toolbox to detect and segment CMB, participating in 2021 ‘Where is VALDO’ challenge.",1,0,0
10.1016/j.neuroimage.2022.119528,hub.docker.com/r/hjkuijf/mixmicrobleednet,"(2021) https://github.com/hjkuijf/MixMicrobleedNet ; https://hub.docker.com/r/hjkuijf/mixmicrobleednet Code for a deep learning-based toolbox to detect and segment CMB, participating in 2021 ‘Where is VALDO’ challenge.",1,0,0
10.1016/j.neuroimage.2022.119528,github.com/hjkuijf/mixmicrobleednet,"(2021) https://github.com/hjkuijf/MixMicrobleedNet ; https://hub.docker.com/r/hjkuijf/mixmicrobleednet Code for a deep learning-based toolbox to detect and segment CMB, participating in 2021 ‘Where is VALDO’ challenge.",1,0,0
10.1016/j.neuroimage.2022.119528,github.com/yonsei-milab/lacunes-identiﬁcation,(2020) https://github.com/Yonsei-MILab/Lacunes-Identiﬁcation code for lacune segmentation Al-Masni et al.,1,0,0
10.1016/j.neuroimage.2022.119616,bic.mni.mcgill.ca/servicesatlases/icbm152nlin2009,"(2021) , where they also include some surrounding cortical areas, such as the entorhinal and parahippocampal cortex and Brodmann areas 35 and 36. 1 http://www.bic.mni.mcgill.ca/ServicesAtlases/ICBM152NLin2009.",1,0,0
10.1016/j.neuroimage.2022.119626,github.com/daniel-adan-lopez/abcd_mtbi/blob/main/rcode,"Code for the replication of study results can be obtained on Github: https://github.com/Daniel-Adan-Lopez/ABCD_mTBI/blob/main/Rcode., Code for replication of the analyses conducted in this manuscript can be re-trieved at https://github.com/Daniel-Adan-Lopez/ABCD_mTBI/blob/main/Rcode.",1,0,0
10.1016/j.neuroimage.2022.119626,abcdstudy.org/federal-partners.html,A full list of supporters is available at https://abcdstudy.org/federal-partners.html.,0,0,1
10.1016/j.neuroimage.2022.119626,abcdstudy.org/consortium_members,A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.,0,0,1
10.1016/j.neuroimage.2022.119626,abcdstudy.org,"Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development SM (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA)., A full list of supporters is available at https://abcdstudy.org/federal-partners.html., A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.",0,1,0
10.1016/j.neuroimage.2022.119626,nda.nih.gov/abcd,Access can be requested at https://nda.nih.gov/abcd/.,0,1,0
10.1016/j.neuroimage.2022.119642,osf.io/s4ydx,"However, as we demonstrate in an extreme example in Fig. 1 (along with the electrode recording ﬁltered between 300 –6000 Hz, converted to an audio ﬁle and uploaded to osf.io; link here: https://osf.io/s4ydx/), recording speech-related activity from an array of high impedance elec-trodes (tungsten microelectrodes, Alpha Omega Co., Inc., Alpharetta, GA) implanted in the subthalamic nucleus of Parkinson’s patients un-dergoing surgery for implantation of deep-brain stimulation (DBS) elec-trodes, it is possible for there to be a clear artifact present in the fre-quency ranges that would commonly be analyzed for unit activity when time-locked to the speech event., It is clear from this spectrogram and the full micro-electrode recording converted to audio (https://osf.io/s4ydx/) that this voice contamination is breaching into frequencies above 300 Hz.",0,1,0
10.1016/j.neuroimage.2022.119642,osf.io/s4ydx,"However, as we demonstrate in an extreme example in Fig. 1 (along with the electrode recording ﬁltered between 300 –6000 Hz, converted to an audio ﬁle and uploaded to osf.io; link here: https://osf.io/s4ydx/), recording speech-related activity from an array of high impedance elec-trodes (tungsten microelectrodes, Alpha Omega Co., Inc., Alpharetta, GA) implanted in the subthalamic nucleus of Parkinson’s patients un-dergoing surgery for implantation of deep-brain stimulation (DBS) elec-trodes, it is possible for there to be a clear artifact present in the fre-quency ranges that would commonly be analyzed for unit activity when time-locked to the speech event., It is clear from this spectrogram and the full micro-electrode recording converted to audio (https://osf.io/s4ydx/) that this voice contamination is breaching into frequencies above 300 Hz.",0,1,0
10.1016/j.neuroimage.2022.119668,osf.io,Data Availability Data and code supporting the reported analyses are available at osf.io (https://osf.io/wh4ua/).,1,1,0
10.1016/j.neuroimage.2022.119668,osf.io/wh4ua,Data Availability Data and code supporting the reported analyses are available at osf.io (https://osf.io/wh4ua/).,1,1,0
10.1016/j.neuroimage.2022.119708,openfmri.org/dataset/ds000117,"Dataset The multi-modal (MRI, EEG, MEG) human neuroimaging dataset is available on OpenfMRI (https://www.openfmri.org/dataset/ds000117/; Wakeman & Henson, 2015).",0,1,0
10.1016/j.neuroimage.2022.119708,github.com/smscottlee/face_dcm_fmri,"The Mat-lab scripts used for all analyses that follow are available here: https://github.com/SMScottLee/Face_DCM_fMRI., All codes used in the present study are available on Github repository (https://github.com/SMScottLee/Face_DCM_fMRI).",1,0,0
10.1016/j.neuroimage.2022.119708,ﬁl.ion.ucl.ac.uk/spm,fMRI analysis and ROI selection The fMRI data were pre-processed using the SPM12 software (www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119708,openneuro.org/datasets/ds000117,"It consists of 19 participants with an age range of 23–37 years (note that this is a superset of the participants available on OpenNeuro, https://openneuro.org/datasets/ds000117).",0,1,0
10.1016/j.neuroimage.2022.119721,github.com/rmarkello/pyls,"Partial least squares correlation To analyze the correspondence of cortical thickness measures with phenotypical data in individuals from the HCHS, we performed a partial least squares (PLS) correlation analysis using pyls (https://github.com/rmarkello/pyls).",1,0,0
10.1016/j.neuroimage.2022.119721,github.com/murraylab/brainsmash,"Furthermore, a variogram-based null model preserving spatial autocorrelation which is implemented in the brainSMASH toolbox was applied (https://github.com/murraylab/brainsmash) (Burt et al., 2020).",1,0,0
10.1016/j.neuroimage.2022.119721,github.com/m-wierzba/cat-container,"Estimation of age-related cortical thickness diﬀerences Structural preprocessing harnessed the CAT12 surface-based mor-phometry pipeline (CAT12.7 r1743; https://github.com/m-wierzba/cat-container) for surface reconstruction and cortical thickness mea-surement building upon a projection-based thickness estimation method (Dahnke et al., 2013 ; Gaser et al., 2022 ; Yotter et al., 2011 ; Yotter et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119742,db.humanconnectome.org,"Data and code availability statement The data used in this study for inference and benchmarking are open-source: HCP (https://db.humanconnectome.org), HBN (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing.html), and PNC (https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id = phs000607.v3.p2)., Data and Code Availability The data used in this study for inference and benchmarking are open-source: HCP (https://db.humanconnectome.org), HBN (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing.html), and PNC (https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id = phs000607.v3.p2).",0,1,0
10.1016/j.neuroimage.2022.119742,ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id,"Data and code availability statement The data used in this study for inference and benchmarking are open-source: HCP (https://db.humanconnectome.org), HBN (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing.html), and PNC (https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id = phs000607.v3.p2)., Data and Code Availability The data used in this study for inference and benchmarking are open-source: HCP (https://db.humanconnectome.org), HBN (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing.html), and PNC (https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id = phs000607.v3.p2).",0,1,0
10.1016/j.neuroimage.2022.119742,fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing.html,"Data and code availability statement The data used in this study for inference and benchmarking are open-source: HCP (https://db.humanconnectome.org), HBN (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing.html), and PNC (https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id = phs000607.v3.p2)., Data and Code Availability The data used in this study for inference and benchmarking are open-source: HCP (https://db.humanconnectome.org), HBN (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing.html), and PNC (https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id = phs000607.v3.p2).",0,1,0
10.1016/j.neuroimage.2022.119742,fcon_1000.projects.nitrc.org/indi/retro/yale_hires.html,"The Yale data used in this study to construct edge-centric networks are open-source and available here: http://fcon_1000.projects.nitrc.org/indi/retro/yale_hires.html., The Yale data used in this study to construct edge-centric networks are open-source and available here: http://fcon_1000.projects.nitrc.org/indi/retro/yale_hires.html.",0,1,0
10.1016/j.neuroimage.2022.119742,github.com/sneuroble/nbs_benchmarking,"Code used for bench-marking, inference, and summarization/reorganization by atlas is available here: https://github.com/SNeuroble/NBS_benchmarking., Code used for bench-marking, inference, and summarization/reorganization by atlas is available here: https://github.com/SNeuroble/NBS_benchmarking.",1,0,0
10.1016/j.neuroimage.2022.119742,nitrc.org/projects/bioimagesuite,"Code used for node-centric and edge-centric network construction can be found at https://www.nitrc.org/projects/bioimagesuite/., Code used for node-centric and edge-centric network construction can be found at https://www.nitrc.org/projects/bioimagesuite/.",1,0,0
10.1016/j.neuroimage.2022.119451,cran.r-project.org/web/packages/pcalg,"Data and code availability statements Speciﬁcally, GES, PC and LiNGAM were implemented using the widely used R package pcalg , which is available at https://cran.r-project.org/web/packages/pcalg/.",1,0,0
10.1016/j.neuroimage.2022.119451,coins.trendscenter.org,"The cohort data is accessible through the website (https://coins.trendscenter.org/) of COINS (COllaborative Infor-matics Neuroimaging Suite) database (Scott et al., 2011).",0,1,0
10.1016/j.neuroimage.2022.119451,github.com/xunzheng/notears,Notears method was implemented using Python available at https://github.com/xunzheng/notears.,1,0,0
10.1016/j.neuroimage.2022.119451,github.com/gmeng92/joint-notears,The proposed joint DAG method was implemented with Python and the code is available at https://github.com/gmeng92/joint-notears.,1,0,0
10.1016/j.neuroimage.2022.119451,ﬁl.ion.ucl.ac.uk/spm,"Then the data were pre-processed with SPM5 (http://www.ﬁl.ion.ucl.ac.uk/spm) and were realigned, spatially nor-malized and resliced to 3 ×3 ×3 mm , smoothed with a 10 ×10 ×10 mm 3 Gaussian kernel, and analyzed by multiple regression considering the stimulus and their temporal derivatives plus an intercept term as re-gressors.",1,0,0
10.1016/j.neuroimage.2022.119632,github.com/robloughnan/mostest_generalization,"Code for discovery and replication is available at (https://github.com/robloughnan/MOSTest_generalization)., Code for conducting discovery and replication is available at https://github.com/robloughnan/MOSTest_generalization.",1,0,0
10.1016/j.neuroimage.2022.119632,github.com/precimed/mostest/tree/master/simu,"Code for generating these simulations can be found at: https://github.com/precimed/mostest/tree/master/simu., Code for simu-lations is available at https://github.com/precimed/mostest/tree/master/simu.",1,0,0
10.1016/j.neuroimage.2022.119632,abcdstudy.org/federal-partners,"The ABCD Study is supported by the National Institutes of Health and additional federal partners under award numbers: U01DA041022, U01DA041028, U01DA041048, U01DA041089, U01DA041106, U01DA041117, U01DA041120, U01DA041134, U01DA041148, U01DA041156, U01DA041174, U24DA041123, and U24DA041147 A full list of supporters is available at https://abcdstudy.org/federal-partners/.",0,0,1
10.1016/j.neuroimage.2022.119632,abcdstudy.org/principal-investigators.html,A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/principal-investigators.html.,0,0,1
10.1016/j.neuroimage.2022.119632,abcdstudy.org,"Finally, we test the generalization of loci discovered in UK Biobank to a developmental cohort of 9–10 year old children from the Adolescent Brain Cognitive Development® (ABCD; https://abcdstudy.org) Study, where we see a higher yield of replicated loci for MOSTest-PVS versus min-P., ABCD Acknowledgement Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development 𝐒𝐌 Study (ABCD Study®) (https://abcdstudy.org), held in the NIMH Data Archive (NDA)., The ABCD Study is supported by the National Institutes of Health and additional federal partners under award numbers: U01DA041022, U01DA041028, U01DA041048, U01DA041089, U01DA041106, U01DA041117, U01DA041120, U01DA041134, U01DA041148, U01DA041156, U01DA041174, U24DA041123, and U24DA041147 A full list of supporters is available at https://abcdstudy.org/federal-partners/., A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/principal-investigators.html.",0,1,0
10.1016/j.neuroimage.2022.119632,ox.ac.uk/showcase/docs.cgi?id,ox.ac.uk/showcase/docs.cgi?id = 1).,0,0,1
10.1016/j.neuroimage.2022.119584,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"Intracranial volume (ICV) estimates were obtained from 3T whole-brain T1-weighted gradient echo volumes us-ing SPM12’s Segment and Tissue Volume Utility modules (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12) (Malone et al., 2015) (Fig.",1,0,0
10.1016/j.neuroimage.2022.119584,github.com/sujason/thomas,"Using version v0 of this tool (https://github.com/sujason/thomas), we segmented and extracted the volumes of 12 lateralized structures in each hemisphere of the brain: whole thalamus, ten thalamic nuclei (anteroventral AV, centrome-dian CM, lateral geniculate nucleus LGN, mediodorsal MD, medial geniculate nucleus MGN, pulvinar Pul, ventral anterior VA, ven-tral lateral anterior VLA, ventral lateral posterior VLP, and ventral posterolateral VPL), and one adjacent epithalamic structure, the habe-nula (Hb) (Fig.",1,0,0
10.1016/j.neuroimage.2022.119584,surfer.nmr.mgh.harvard.edu/fswiki/samseg,"Using the FreeSurfer tool SAMSEG (https://surfer.nmr.mgh.harvard.edu/fswiki/Samseg), we segmented and extracted the volumes of 11 structures on each side of the brain: nu-cleus accumbens, amygdala, caudate, cerebellar cortex, cerebellar white matter, cerebral cortex, cerebral white matter, hippocampus, lateral ventricle, pallidum, and putamen (Fig.",1,0,0
10.1016/j.neuroimage.2022.119550,humanconnectome.org/study/hcp-young-adult/data-use-terms,This data is publicly available to researchers who agree to the data use terms (www.humanconnectome.org/study/hcp-young-adult/data-use-terms).,0,1,0
10.1016/j.neuroimage.2022.119550,humanconnectome.org,"Data and code availability All data used in this project is from the Human Connec-tome Project (HCP) (www.humanconnectome.org)., This data is publicly available to researchers who agree to the data use terms (www.humanconnectome.org/study/hcp-young-adult/data-use-terms).",0,1,0
10.1016/j.neuroimage.2022.119550,mrtrix.org,"Diﬀusion-weighted images (DWI) from HCP database were processed using MRtrix3 (https://www.mrtrix.org) (Tournier et al., 2012).",1,0,0
10.1016/j.neuroimage.2022.119550,db.humanconnectome.org,All HCP data may be downloaded through the ConnectomeDB (db.humanconnectome.org).,0,1,0
10.1016/j.neuroimage.2022.119710,csie.ntu.edu.tw/~cjlin/libsvm,"In each of these clusters, we then used a linear support vector machine (SVM) implemented in LIBSVM (http://www.csie.ntu.edu.tw/~cjlin/libsvm/) with default parameters (a ﬁxed regularization parameter C = 1) to compute decoding accuracies.",1,0,0
10.1016/j.neuroimage.2022.119710,drive.google.com/ﬁle/d/1kl6tmf7b3gndbkgdfbp2vhkcbxk_qnio/view,"The TDT (the toolbox for MVPA) and codes used for the cross-validation approach, cross-classiﬁcation ap-proach, and RSA are freely available online (https://drive.google.com/ﬁle/d/1kl6TMf7b3gndbkGDfbP2VhKcBxK_qnio/view).",1,0,0
10.1016/j.neuroimage.2022.119710,ion.ucl.ac.uk/spm,ion.ucl.ac.uk/spm) and implemented in MATLAB.,1,0,0
10.1016/j.neuroimage.2022.119338,github.com/tierneytim/opm,"We provide an explicit form for 𝐀 and 𝐁 in Appendix A and code to create these harmonics is made publicly available at https://github.com/tierneytim/OPM/blob/master/spm_opm_vslm.m., Software Software required to generate the vector spherical harmonics de-scribed in this paper is made freely available on the ﬁrst author’s GitHub page (https://github.com/tierneytim/OPM)., Examples and tests can also be found on GitHub (https://github.com/tierneytim/OPM/blob/master/testScripts/testVSM.m)., The software required to gen-erate the vector spherical harmonics described in this paper is made freely available on the ﬁrst author’s GitHub page (https://github.com/tierneytim/OPM)., Examples and tests can also be found on GitHub (https://github.com/tierneytim/OPM/blob/master/testScripts/testVSM.m).",1,0,0
10.1016/j.neuroimage.2022.119338,quspin.com/products-qzfm,"The triaxial sensor was assumed to have a noise ﬂoor that is 2.5 times higher than a radial sensor (https://quspin.com/products-qzfm/)., For the results in Fig. 6 we assume the triaxial sensor has a noise ﬂoor that is 2.5 times higher than a radial sensor (https://quspin.com/products-qzfm/).",0,0,1
10.1016/j.neuroimage.2022.119338,github.com/tierneytim/opm/blob/master/testscripts/testvsm.m,"Examples and tests can also be found on GitHub (https://github.com/tierneytim/OPM/blob/master/testScripts/testVSM.m)., Examples and tests can also be found on GitHub (https://github.com/tierneytim/OPM/blob/master/testScripts/testVSM.m).",0,0,1
10.1016/j.neuroimage.2022.119338,github.com/tierneytim/opm/blob/master/spm_opm_vslm.m,We provide an explicit form for 𝐀 and 𝐁 in Appendix A and code to create these harmonics is made publicly available at https://github.com/tierneytim/OPM/blob/master/spm_opm_vslm.m.,1,0,0
10.1016/j.neuroimage.2022.119192,github.com/junjypark/clean,"•Software : CLEAN is currently available at https://github.com/junjypark/CLEAN as an R package., Our method is publicly available as a form of an R package at https://github.com/junjypark/CLEAN , and it supports parallel comput-ing to further reduce the computational cost., The codes for conduct-ing statistical analyses using CLEAN are currently available as a form of R package at https://github.com/junjypark/CLEAN.",1,0,0
10.1016/j.neuroimage.2022.119177,en.wikipedia.org/wiki/point_spread_function,"This is illustrated for the cases of a hypothetical micro-scope (https://en.wikipedia.org/wiki/Point_spread_function), as well as for EEG/MEG sensor and source space data in Fig.",0,1,0
10.1016/j.neuroimage.2022.119177,mne.tools/dev/overview/datasets_index.html,Simulation set ‐up In order to illustrate the use of PSFs and CTFs we computed individ-ual PSFs and CTFs in Fig. 3 for the EEG/MEG sample dataset 2 provided 2 https://mne.tools/dev/overview/datasets_index.html by the MNE-Python software package.,0,1,0
10.1016/j.neuroimage.2022.119177,github.com/olafhauk/eegmeg_resolutionatlas,"The software used for our resolution analysis is available online (https://github.com/olafhauk/EEGMEG_ResolutionAtlas)., The code used to analyze the EEG/MEG data is openly available on github: https://github.com/olafhauk/EEGMEG_ResolutionAtlas.",1,0,0
10.1016/j.neuroimage.2022.119177,mne.tools/stable/auto_examples/inverse/resolution_metrics.html,"Those results were then mor-phed based on a spherical representation of the cortex computed using the spherical registration of Freesurfer software (Greve et al., 2013) and 3 PSFs and CTFs for MNE-type methods: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices.html PSFs for LCMV beamformer: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices_lcmv.html Compute resolution metrics: https://mne.tools/stable/auto_examples/inverse/resolution_metrics.html Compute resolution metrics to compare EEG/MEG with MEG: https://mne.tools/stable/auto_examples/inverse/resolution_metrics_eegmeg.",1,0,0
10.1016/j.neuroimage.2022.119177,mne.tools/stable/auto_examples/inverse/psf_ctf_vertices_lcmv.html,"Those results were then mor-phed based on a spherical representation of the cortex computed using the spherical registration of Freesurfer software (Greve et al., 2013) and 3 PSFs and CTFs for MNE-type methods: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices.html PSFs for LCMV beamformer: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices_lcmv.html Compute resolution metrics: https://mne.tools/stable/auto_examples/inverse/resolution_metrics.html Compute resolution metrics to compare EEG/MEG with MEG: https://mne.tools/stable/auto_examples/inverse/resolution_metrics_eegmeg.",1,0,0
10.1016/j.neuroimage.2022.119177,mne.tools/stable/auto_examples/inverse/resolution_metrics_eegmeg,"Those results were then mor-phed based on a spherical representation of the cortex computed using the spherical registration of Freesurfer software (Greve et al., 2013) and 3 PSFs and CTFs for MNE-type methods: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices.html PSFs for LCMV beamformer: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices_lcmv.html Compute resolution metrics: https://mne.tools/stable/auto_examples/inverse/resolution_metrics.html Compute resolution metrics to compare EEG/MEG with MEG: https://mne.tools/stable/auto_examples/inverse/resolution_metrics_eegmeg.",1,0,0
10.1016/j.neuroimage.2022.119177,en.wikipedia.org/wiki/point-spread_function,From https://en.wikipedia.org/wiki/Point-spread_function.,0,0,1
10.1016/j.neuroimage.2022.119177,biorxiv.org/content/10.1101/672956v1,Researchers therefore need tools for a comprehensive evaluation of the spatial resolution of their experimental setup at hand. 1 This paper is a substantially revised and altered version of pre-print https://www.biorxiv.org/content/10.1101/672956v1.,0,0,1
10.1016/j.neuroimage.2022.119177,mne.tools/stable/auto_examples/inverse/psf_ctf_vertices.html,"Those results were then mor-phed based on a spherical representation of the cortex computed using the spherical registration of Freesurfer software (Greve et al., 2013) and 3 PSFs and CTFs for MNE-type methods: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices.html PSFs for LCMV beamformer: https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices_lcmv.html Compute resolution metrics: https://mne.tools/stable/auto_examples/inverse/resolution_metrics.html Compute resolution metrics to compare EEG/MEG with MEG: https://mne.tools/stable/auto_examples/inverse/resolution_metrics_eegmeg.",1,0,0
10.1016/j.neuroimage.2022.119177,mne.tools/stable/auto_examples/forward/forward_sensitivity_maps.html,html Compute correlations with source depth: https://mne.tools/stable/auto_examples/forward/forward_sensitivity_maps.html 7 O.,0,0,1
10.1016/j.neuroimage.2022.119110,dropbox.com/sh/xmxmzosxrudpx1l/aabm7l_jtkd2z6oi3pnqvnzoa?dl,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,exploredti.com,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,github.com/aswendtlab/aidaconnect,"In addition to exist-ing toolboxes, which include only a selection of network algorithms and/or an adaption to human MRI (e.g., MagnAn and CONN), we pro-vide with AIDAconnect the ﬁrst atlas-based toolkit dedicated to mouse brain MRI and a comprehensive collection of BCT-based ready-touse scripts (https://github.com/aswendtlab/AIDAconnect)., NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,nitrc.org/projects/gretna/9,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,github.com/aswendtlab/project_microbiome,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,sites.google.com/site/bctnet/list-of-measures,for local and global measures 1 https://sites.google.com/site/bctnet/list-of-measures over time for multiple groups.,0,0,1
10.1016/j.neuroimage.2022.119110,nitrc.org/projects/birn/7,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,boost.org/doc/libs/1_66_0/libs/graph/doc/index.html,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,biocom-online.de/products_ip.html,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,igraph.org/r/4,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,nwb.slis.indiana.edu,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,github.com/cobralab/documentation/wiki/dsurqe-atlas-hierarchical-downsample,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119110,edmond.mpdl.mpg.de/imeji/collection/ce5qnwu4sktzepn,"NeuroImage 253 (2022) 119110 Table 1 (continued) Study MRI Protocol Atlas-/voxel-based (Mechling, Areﬁn, et al., 2016) 7T: T2 (RARE): 0.051 ×0.051 ×0.3 mm, rs-fMRI (EPI): 0.15 ×0.15 ×0.75 mm, DTI (EPI): 0.094 ×0.094 ×0.5 mm Voxel-based: Identiﬁcation of clusters as nodes via ICA (100 components), followed by registration with the Allen Reference Atlas to select 87 functional clusters Graph-based measures Local: C,Diversity,s Global: L,Q,S Code/Data available Yes 9/No Tools Graph theory methods as described by (Newman, 2006) Statistics ICASSO to assess pattern stability, no statistical group comparison Disease model Mu opioide rececptor knockout mice (Oprm1-/-) vs control mice Study (Meningher et al., 2020) MRI Protocol 7T: DTI (EPI): 0.21 ×0.21 ×0.21 mm Atlas-/voxel-based Atlas-based (DSURQE atlas 10 , 68 ROIs) Voxel-based analysis for structural alteration analysis in gray matter brain regions Graph-based measures Local: b,Cl,d,E loc ,s Global: E glob Code/Data available No/No Tools Network Analysis Tool embedded in ExploreDTI 11 , Brain Connectivity Toolbox (Rubinov and Sporns, 2010) in combination with custom MATLAB scripts Statistics Mixed design ANOVA with group and time point eﬀect, FDR corrected Disease model Mild traumatic brain injury (mTBI) Study (Pallast, Wieters, et al., 2020) MRI Protocol 9.4T: T2w: 0.068 ×0.068 ×0.4 mm, DTI (EPI): 0.14 ×0.14 ×0.5 mm Atlas-/voxel-based Atlas-based (custom Allen Reference Atlas CCF v3 (Lein et al., 2007), 49 regions per hemisphere, 13 per hemisphere selected for graph analysis) Graph-based measures Local: d,l,s Global: None Code/Data available Yes 12/Yes 13 Tools Brain Connectivity Toolbox (Rubinov and Sporns, 2010) Statistics Mixed-eﬀects analysis (Greenhouse-Geisser correction) for time point and group comparison, post-hoc multiple comparisons FDR corrected Disease model Stroke (photothrombosis model) Study MRI Protocol Atlas-/voxel-based (Pradier et al., 2021) 9.4 T: rs-fMRI (EPI): 0.35 ×0.325 ×0.5 mm Atlas-based (custom mouse brain atlas with 188 ROIs derived from the Franklin and Paxinos mouse brain atlas (Franklin and Paxinos, 2008)) Graph-based measures Local: authority,b,C,d,hubscore,average l,s Global: C,L,S,customized Q (Blondel et al., 2008) Code/Data available No/No Tools MagnAn 14 Statistics Network based statistics NBS (Zalesky et al., 2010) as implemented in MagnAn Disease model No disease model (Inﬂuence of medetomidine/isoﬂurane anaesthesia) Study (van Meer et al., 2012) MRI Protocol 4.7T: T2w, DTI (EPI), rs-fMRI (EPI): 0.5 ×0.5 ×1.5 mm Atlas-/voxel-based Voxel-based (Probabilistic ICA to extract a group-averaged functional network, 7 components) Graph-based measures Local: C,l Global: S Code/Data available No/No Tools Custom (not speciﬁed), C ++ Boost Graph Library 15 Statistics Two-sample paired t-tests (time point comparison), Mann–Whitney-U-test (sensorimotor performance scores), Repeated-measures linear mixed model (network parameters over time and between groups) with post-hoc Tukey’s test Disease model Rat : Stroke (MCAO model) 1 https://edmond.mpdl.mpg.de/imeji/collection/Ce5QnWU4SktzEPN 2 https://github.com/aswendtlab/Project_Microbiome 3 https://igraph.org/r/4 https://www.dropbox.com/sh/xmxmzosxrudpx1l/AABM7L_jtkd2z6oi3pnqVNZoa?dl = 0 5 http://doi.org/10.5905/ethz-1007-59 6 https://www.nitrc.org/projects/birn/7 http://nwb.slis.indiana.edu 8 https://www.nitrc.org/projects/gretna/9 https://www.uniklinik-freiburg.de/mr-en/research-groups/diﬀperf/ﬁbertools.html 10 https://github.com/CoBrALab/documentation/wiki/DSURQE-atlas-hierarchical-downsample 11 https://www.exploredti.com 12 https://github.com/aswendtlab/AIDAconnect 13 https://doi.gin.g-node.org/10.12751/g-node.okz5nn 14 http://www.biocom-online.de/products_ip.html 15 https://www.boost.org/doc/libs/1_66_0/libs/graph/doc/index.html 10 L.",1,0,0
10.1016/j.neuroimage.2022.119747,birmingham.ac.uk/bear,See http://www.birmingham.ac.uk/bear for more details.,0,0,1
10.1016/j.neuroimage.2022.118931,surfer.nmr.mgh.harvard.edu,"Vascular territories To assess susceptibility diﬀerences of blood in diﬀerent vascular ter-ritories, Freesurfer’s (http://surfer.nmr.mgh.harvard.edu/, version 6.0) “recon-all ”(Fischl, 2012) combined with SPM12’s uniﬁed segmenta-tion algorithm (Ashburner and Friston, 2005) was used to generate segmentations of all white and gray matter regions from the cor-rected MP2RAGE data.",1,0,0
10.1016/j.neuroimage.2022.118931,github.com/nighres/nighres,"Second, a vein segmentation was computed from QSM data with a recursive vessel ﬁlter that ﬁrst extracts local shape information with a 2D ridge ﬁlter and then uses a diﬀusion tech-nique to obtain a more global vein segmentation (Bazin et al., 2016) in Python using the publicly available Nighres 1.2.0 (Huntenburg et al., 2018) Docker container (https://github.com/nighres/nighres) with de-fault parameters.",1,0,0
10.1016/j.neuroimage.2022.118931,github.com/washington-university/hcppipelines/blob/master/freesurfer/custom/recon-all.v6.hires,"To account for the high resolution of the data, conf2hires (https://github.com/Washington-University/HCPpipelines/blob/master/FreeSurfer/custom/recon-all.v6.hires) was employed.",1,0,0
10.1016/j.neuroimage.2022.118931,zenodo.org,Data availability The Matlab code for the proposed vein segmentation algorithm is available on github: https://github.com/SinaStraub/GRE_vessel_seg.git and example data on Zenodo.org: https://doi.org/10.,1,0,0
10.1016/j.neuroimage.2022.118931,github.com/haifafh/mfat,"The implementation of the fractional anisotropy tensor from (Alhasson et al., 2018) (https://github.com/Haifafh/MFAT) (Eq.",1,0,0
10.1016/j.neuroimage.2022.118931,github.com/josepmarques/mp2rage-related-scripts,MP2RAGE data processing The B 1 maps were used to correct for the inhomogeneities in the MP2RAGE data with github.com/JosePMarques/MP2RAGE-related-scripts.,1,0,0
10.1016/j.neuroimage.2022.118931,github.com/sinastraub/gre_vessel_seg.git,"The latter was not evaluated in this study and is not implemented in the provided GitHub reposi-tory (https://github.com/SinaStraub/GRE_vessel_seg.git)., Data availability The Matlab code for the proposed vein segmentation algorithm is available on github: https://github.com/SinaStraub/GRE_vessel_seg.git and example data on Zenodo.org: https://doi.org/10.",1,0,0
10.1016/j.neuroimage.2022.119447,biosemi.com/faq/cms&drl.htm,Recordings were grounded using common mode sense and driven right leg electrodes (http://www.biosemi.com/faq/cms&drl.htm).,0,0,1
10.1016/j.neuroimage.2022.119447,osf.io/gazx2/at,"Code used for stimulus presenta-tion will be available at https://osf.io/gazx2/at the time of publication., Analyses of accuracy, response times and conﬁdence ratings Code used for all behavioural and EEG data analyses will be available at https://osf.io/gazx2/at the time of publication., All data processing and analysis code and data will be available at https://osf.io/gazx2/at the time of publication.",1,0,0
10.1016/j.neuroimage.2021.118831,graphpad.com/scientiﬁc-software/prism,All statistical analyses were performed in GraphPad Prism 8 (https://www.graphpad.com/scientiﬁc-software/prism/).,1,0,0
10.1016/j.neuroimage.2022.119308,github.com/biomag/dbs_pd_beta_burst,"All analysis code is available under https://github.com/BioMag/dbs_pd_beta_burst., Scripts used to produce the results and ﬁg-ures presented in the study are published on the BioMag Gitlab page (https://github.com/BioMag/dbs_pd_beta_burst).",1,0,0
10.1016/j.neuroimage.2021.118792,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,Hu-man Connectome Project (HCP) data (S900) are publicly avail-able at https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release.,0,1,0
10.1016/j.neuroimage.2021.118792,nitrc.org/frs/?group_id,Code and data availability The Shen 268 atlas is available online on the BioImage Suite NI-TRC page (https://www.nitrc.org/frs/?group_id 1 4 51).,0,1,0
10.1016/j.neuroimage.2021.118792,pypi.org/project/biswebpython,The code for in-dividualized parcellation is available in the publicly available Python package biswebpython: https://pypi.org/project/biswebpython/.,1,0,0
10.1016/j.neuroimage.2022.118890,humanconnectome.org/study/hcp-lifespan-aging/data-releases,HCP-Aging brain imaging and behavioral data are available at: https://www.humanconnectome.org/study/hcp-lifespan-aging/data-releases.,0,1,0
10.1016/j.neuroimage.2022.118890,camcan-archive.mrc-cbu.cam.ac.uk/dataaccess,"Likewise, brain imaging and behavioral data from the Cam-CAN project are available at: https://camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/.",0,1,0
10.1016/j.neuroimage.2022.118890,pypi.org/project/chord,"Edges were visualized using BrainNet viewer toolbox (Xia et al., 2013) and Chord (https://pypi.org/project/chord/) in Python 3.",1,0,0
10.1016/j.neuroimage.2022.118890,fmriprep.readthedocs.io/en/latest/workﬂows.html,Additional details of the pipeline can be found here: https://fmriprep.readthedocs.io/en/latest/workﬂows.html.,0,0,1
10.1016/j.neuroimage.2022.118890,bids-standard.github.io/bids-validator,"Preprocessing of MRI data MRI NIfTI ﬁles were ﬁrst organized according to the Brain Imaging Data Structure (BIDS) format (Gorgolewski et al., 2016) and validated with the BIDS validator v.1.5.6 (https://bids-standard.github.io/bids-validator/).",1,0,0
10.1016/j.neuroimage.2022.118890,github.com/yalemrrc/cpm,Code and data availability The code used to run the CPM analysis is available at: https://github.com/YaleMRRC/CPM.,1,0,0
10.1016/j.neuroimage.2022.118890,pingouin-stats.org,"These sta-tistical tests were performed using Pingouin v0.3.8 (https://pingouin-stats.org/; Vallat, 2018) in Python 3.",1,0,0
10.1016/j.neuroimage.2022.118890,ants-brainextraction.sh,The T1w was then skull stripped using ants-BrainExtraction.sh v2.1.0.,1,0,0
10.1016/j.neuroimage.2022.119339,developingconnectome.org,"Imaging data of human neonates were ob-tained from the Developing Human Connectome Project (dHCP) conducted at the Newborn Imaging Center at Evelina Lon-don Children’s Hospital, London, UK (Makropoulos et al., 2018 , https://www.developingconnectome.org). 118 neonates (57 females, birth age = 39.7 ± 1.9 weeks; scan age = 40.9 ± 2.1 weeks, birth weight = 3.1 ± 0.66 kg) were selected from the two data releases available at the time of data analysis for the present study based on the following inclusion criteria: (1) images were acquired within the ﬁrst month (i.e., ≤ 4 weeks) after birth; (2) structural images showed no clinical concerns when evaluated by a perinatal neuroradiologist (i.e., radiology score ≤ 3); (3) ≤ 10% of scans contained excessive head movement, deﬁned as ≥ 0.3 mm FD; (4) there was good coverage (> 50% overlap) of the functional ROIs selected.",0,1,0
10.1016/j.neuroimage.2022.119339,projects.nitrc.org/indi/indiprime.html,"Macaque imaging data were obtained from the PRIMatE Data Ex-change (PRIME-DE) consortium (Milham et al., 2018 , http://fcon_1000.projects.nitrc.org/indi/indiPRIME.html)., projects.nitrc.org/indi/indiPRIME.html.",0,1,0
10.1016/j.neuroimage.2022.119339,github.com/tingsterx/alignment_macaque-human,"Moreover, the codes for the cross-species alignment are available at https://github.com/TingsterX/alignment_macaque-human.",1,0,0
10.1016/j.neuroimage.2022.119339,gin.g-node.org/biomedia/dhcp-volumetric-atlas-groupwise,"For neonates, these tool and face processing ROIs identiﬁed in human adults in MNI152 space were then transformed onto 40-week templates available on the dHCP-website (https://gin.g-node.org/BioMedIA/dhcp-volumetric-atlas-groupwise), using Advanced Normalization Tools (ANTs, Avants et al., 2009 , https://stnava.github.io/ANTs , Fig. 1 A).",0,0,1
10.1016/j.neuroimage.2022.119339,stnava.github.io/ants,"For neonates, these tool and face processing ROIs identiﬁed in human adults in MNI152 space were then transformed onto 40-week templates available on the dHCP-website (https://gin.g-node.org/BioMedIA/dhcp-volumetric-atlas-groupwise), using Advanced Normalization Tools (ANTs, Avants et al., 2009 , https://stnava.github.io/ANTs , Fig. 1 A).",0,0,1
10.1016/j.neuroimage.2022.119339,humanconnectome.org/study/hcp-young-adult,"Louis (Van Essen et al., 2013 , https://www.humanconnectome.org/study/hcp-young-adult).",0,0,1
10.1016/j.neuroimage.2022.119339,fcon_1000.projects.nitrc.org/indi/indiprime.html,"Macaque imaging data were obtained from the PRIMatE Data Ex-change (PRIME-DE) consortium (Milham et al., 2018 , http://fcon_1000.projects.nitrc.org/indi/indiPRIME.html).",0,1,0
10.1016/j.neuroimage.2022.119339,humanconnectome.org/and,humanconnectome.org/and https://www.developingconnectome.,0,0,1
10.1016/j.neuroimage.2022.119339,github.com/xiyu-bnu/neonate_tool_network,"For the macaque data, subject IDs included in the current study are listed in Supplementary Table S1, and the computed network results are available at https://github.com/xiyu-bnu/neonate_tool_network.",1,0,0
10.1016/j.neuroimage.2022.119339,neurosynth.org,"ROI selection and cross-population registration (Fig. 1 A) Nodes (ROIs) of the neural networks underlying tool (and face as a control domain) processing in human adults were objectively gener-ated from meta-analyses based on the Neurosynth database incorporat-ing 14,371 fMRI studies in total (https://neurosynth.org , version 0.7 released July, 2018, Yarkoni et al., 2011).",0,1,0
10.1016/j.neuroimage.2022.119339,nitrc.org/projects/mricron,"Slice views and projected brain images were prepared in Mricron (https://www.nitrc.org/projects/mricron) and BrainNet Viewer (Xia et al., 2013), respectively.",1,0,0
10.1016/j.neuroimage.2022.119295,healthit.gov,"HealthIT.gov recommends that hospitals have a minimum of 100 Mbps service, and that academic/large medical centers have 1000 Mbps.",0,0,1
10.1016/j.neuroimage.2022.119295,github.com/brainiak/rt-cloud,"To address these issues, we have developed RT-Cloud (https://github.com/brainiak/rt-cloud), a ﬂexible, cloud-based, open-source Python software package for the execution of RT-fMRI experiments., Here, we describe RT-Cloud, a newly-developed, open-source software framework written in Python 3 that leverages cloud computing and SaaS to address the challenges of RT-fMRI (https://github.com/brainiak/rt-cloud).",1,0,0
10.1016/j.neuroimage.2022.119295,speedtest.net,"In general, network in-frastructure bandwidths have been increasing to meet on-demand video streaming and as of 2020, Speedtest.net estimates the average U.S.",0,0,1
10.1016/j.neuroimage.2022.119295,intel.com/intellabs,"Funding was also provided by Intel Labs (https://www.intel.com/intellabs) and by a grant from the John Tem-pleton Foundation to J.D.C., K.A.N., and N.B.T-B.",0,0,1
10.1016/j.neuroimage.2022.119582,osf.io/2v9je/?view_only,"All the data and analysis scripts are available on Open Science Framework (https://osf.io/2v9je/?view_only = 48dﬀc0531ce435ba205bf64822c6f93)., Data and code availability statement The experimental materials, data, and processing scripts are avail-able at https://osf.io/2v9je/?view_only = 48dﬀc0531ce435ba205bf-64822c6f93 Funding This work was supported by the National Natural Science Foundation of China (31971030 , 31470979), the Youth Beijing Scholar Program of the Beijing Government , the Support Project for High-level Teachers in Beijing Municipal Universities in the Period of the 13th Five-year Plan, and Capacity Building for Sci-Tech Innovation-Fundamental Scientiﬁc Research Funds (131–20530290058). 13 Y.",1,0,0
10.1016/j.neuroimage.2022.119582,cran.r-project.org/package,"The lme4 package (http://CRAN.R-project.org/package = lme4) was used for modelling, the lmerTest package (http://CRAN.R-project.org/package = lmerTest) was used for testing parameters, and the bruceR package (https://CRAN.R-project.org/package = bruceR) was used for the correlations and moderation analyses., Post-hoc tests with a Bon-ferroni correction were performed using the emmeans pack-age (https://CRAN.R-project.org/package = emmeans).",1,0,0
10.1016/j.neuroimage.2022.119582,ru.nl/neuroimaging/ﬁeldtrip,Time-frequency analysis was performed to calculate the ERSPs of each condition using the FieldTrip toolbox (http://www.ru.nl/neuroimaging/ﬁeldtrip).,1,0,0
10.1016/j.neuroimage.2022.119582,sccn.ucsd.edu/eeglab,Oﬄine EEG data preprocessing was conducted with the EEGLAB tool-box (http://sccn.ucsd.edu/eeglab) in MATLAB.,1,0,0
10.1016/j.neuroimage.2022.119582,cran.r-project.org/package,"The lme4 package (http://CRAN.R-project.org/package = lme4) was used for modelling, the lmerTest package (http://CRAN.R-project.org/package = lmerTest) was used for testing parameters, and the bruceR package (https://CRAN.R-project.org/package = bruceR) was used for the correlations and moderation analyses.",1,0,0
10.1016/j.neuroimage.2022.119582,psychopy.org,psychopy.org).,0,0,1
10.1016/j.neuroimage.2022.119008,github.com/mouse-imaging-center/mice-lab,"Images were reconstructed oﬀ-line us-ing a purpose-written python script (available on the Mouse Imag-ing center GitHub page, https://github.com/Mouse-Imaging-center/MICe-lab).",1,0,0
10.1016/j.neuroimage.2022.119008,repo.mouseimaging.ca/repo/mousebedprintfiles,A custom mouse holder was designed and fab-ricated from 3D-printed parts as shown in Fig. 1 B-C (ﬁles available for download at http://repo.mouseimaging.ca/repo/MouseBedPrintFiles/).,0,0,1
10.1016/j.neuroimage.2022.119008,repo.mouseimaging.ca/repo/dsurqe_40micron_nifti,"An aggregate atlas combining 182 individually segmented structures (Dorr et al., 2008 ; Steadman et al., 2014 ; Ullmann et al., 2013 ; Richards et al., 2011) was registered to the unbiased consensus average (the atlas is available at http://repo.mouseimaging.ca/repo/DSURQE_40micron_nifti/).",0,0,1
10.1016/j.neuroimage.2022.119008,github.com/mouse-imaging-centre/mice-lab,Code for image reconstruction and data analysis is publicly available through the Mouse Imaging Centre GitHub page (https://github.com/Mouse-Imaging-Centre/MICe-lab and https://github.com/Mouse-Imaging-Centre/RMINC).,1,0,0
10.1016/j.neuroimage.2022.119008,github.com/mouse-imaging-centre/rminc,Code for image reconstruction and data analysis is publicly available through the Mouse Imaging Centre GitHub page (https://github.com/Mouse-Imaging-Centre/MICe-lab and https://github.com/Mouse-Imaging-Centre/RMINC).,1,0,0
10.1016/j.neuroimage.2022.119008,1.5.2.3,All statistical analyses were performed in R (3.6.1) using RMINC (1.5.2.3).,1,0,0
10.1016/j.neuroimage.2022.119698,osf.io/kuxqt,"NeuroImage 264 (2022) 119698 Data availability The data and codes that support the ﬁndings of this study are openly available in OSF at https://osf.io/kuxqt/, DOI: 10.17605/OSF.IO/KUXQT.",0,1,0
10.1016/j.neuroimage.2022.119118,biobank.ctsu.ox.ac.uk/crystal/ﬁeld.cgi?id,"All participants provided informed consent (“Re-sources tab ”at https://biobank.ctsu.ox.ac.uk/crystal/ﬁeld.cgi?id = 200)., Ethics approval statement All participants provided informed consent (“Resources tab ”at https://biobank.ctsu.ox.ac.uk/crystal/ﬁeld.cgi?id = 200).",0,0,1
10.1016/j.neuroimage.2022.119118,osf.io/wt6uf,The preregistration and code are on OSF (https://osf.io/wt6uf).,1,0,0
10.1016/j.neuroimage.2022.119118,ukbiobank.ac.uk,"Data/Code availability This research has been conducted using data from UK Biobank, a ma-jor biomedical database (http://www.ukbiobank.ac.uk/).",0,1,0
10.1016/j.neuroimage.2022.119118,osf.io/nqz4h,"Preregistration, supplemental tables and information, and code are available here: https://osf.io/nqz4h/.",0,0,1
10.1016/j.neuroimage.2022.119118,orcid.org/0000-0002-1471-6566,URL: https://orcid.org/0000-0002-1471-6566 (C.M.,0,0,1
10.1016/j.neuroimage.2022.119416,neurobs.com,"Stimuli Stimuli were delivered using Presentation®software (Version 18.0, Neurobehavioral Syste ms, Inc., Berkeley, CA, www.neurobs.com).",1,0,0
10.1016/j.neuroimage.2021.118765,github.com/bucanl/bids-lossless-eeg,"Automated pre-processing was done using the Loss-less Pipeline (https://github.com/BUCANL/BIDS-Lossless-EEG), imple-mented in EEGLAB and executed in Octave on Compute Canada’s national computing Cedar cluster.",1,0,0
10.1016/j.neuroimage.2021.118765,github.com/mensen/ept_tfce-matlab,These analy-ses were carried out using the ept_TFCE function from TFCE toolbox (https://github.com/Mensen/ept_TFCE-matlab).,1,0,0
10.1016/j.neuroimage.2022.118972,mkweb.bcgsc.ca/tableviewer/visualize/3,humanconnectome.org/The tool we used for analyzing the data can be downloaded from: rsHRF: https://www.nitrc.org/projects/rshrf PhysIO Toolbox: https://www.nitrc.org/projects/physio/Circos table viewer: http://mkweb.bcgsc.ca/tableviewer/visualize/3.,1,0,0
10.1016/j.neuroimage.2022.118972,nitrc.org/projects/rshrf,humanconnectome.org/The tool we used for analyzing the data can be downloaded from: rsHRF: https://www.nitrc.org/projects/rshrf PhysIO Toolbox: https://www.nitrc.org/projects/physio/Circos table viewer: http://mkweb.bcgsc.ca/tableviewer/visualize/3.,1,0,0
10.1016/j.neuroimage.2022.118972,humanconnectome.org/the,humanconnectome.org/The tool we used for analyzing the data can be downloaded from: rsHRF: https://www.nitrc.org/projects/rshrf PhysIO Toolbox: https://www.nitrc.org/projects/physio/Circos table viewer: http://mkweb.bcgsc.ca/tableviewer/visualize/3.,1,0,0
10.1016/j.neuroimage.2022.118972,mathworks.com/matlabcentral/ﬁleexchange/35094-knee-point,The elbow is judged to be at a bisection point which minimizes the sum of errors for the two ﬁts (Details can be found at https://www.mathworks.com/matlabcentral/ﬁleexchange/35094-knee-point).,0,0,1
10.1016/j.neuroimage.2022.118972,nitrc.org/projects/physio/circos,humanconnectome.org/The tool we used for analyzing the data can be downloaded from: rsHRF: https://www.nitrc.org/projects/rshrf PhysIO Toolbox: https://www.nitrc.org/projects/physio/Circos table viewer: http://mkweb.bcgsc.ca/tableviewer/visualize/3.,1,0,0
10.1016/j.neuroimage.2022.118977,med.upenn.edu/cmroi/imscribe.html),"Intrasubject registration between session was accom-plished using an in-house co-registration program, ImScribe (available at https://www.med.upenn.edu/cmroi/imscribe.html), as described in previous work (Nanga et al., 2018).",1,0,0
10.1016/j.neuroimage.2022.119649,github.com/pyvista/pyacvd,"and remeshed using PyACVD (https://github.com/pyvista/pyacvd) to obtain a high-quality, uniformly reﬁned surface mesh.",1,0,0
10.1016/j.neuroimage.2022.119649,crl.med.harvard.edu/software,"All images were coregistered to the T1 scan using a rigid transformation, estimated by optimizing the mutual information between the two images (Grau et al., 2004; Weisenfeld and Warﬁeld, 2009) as implemented in CRKIT (http://crl.med.harvard.edu/software), and resampled to have matching 1 mm isotropic resolution.",1,0,0
10.1016/j.neuroimage.2022.119649,llnl.gov/casc/hypre,The discretised equations were solved using the conjugate gradient (CG) method with an algebraic multigrid (AMG) preconditioner from the HYPRE library of linear solvers (http://www.llnl.gov/casc/hypre).,1,0,0
10.1016/j.neuroimage.2022.119649,slicer.org,"The forward displacement ﬁeld transform was inverted using 3D Slicer (https://www.slicer.org) (Fedorov et al., 2012), allowing it to be applied directly to scalar, vector or tensor images.",1,0,0
10.1016/j.neuroimage.2022.119649,surfer.nmr.mgh.harvard.edu,"We applied the skull stripping procedure available in FreeSurfer (http://surfer.nmr.mgh.harvard.edu) (Dale et al., 1999), an open-source software suite for processing and analyzing human brain MRIs, to the preoperative T1-weighted anatomical MRI to create a brain mask (i.e., a binary label map that is nonzero in the region of the brain only)., In this case, the conventional MRI (instead of DTI) may be transformed to the post-implantation conﬁguration using the same methods described herein, and the DTI-based method may be replaced with traditional segmen-tation methods based on MRI, such as those available in FreeSurfer (http://surfer.nmr.mgh.harvard.edu) (Dale et al., 1999).",1,0,0
10.1016/j.neuroimage.2022.119649,mfem.org,"The iEEG forward problem solution procedure was implemented using the open-source MFEM library (Anderson et al., 2020) (https://mfem.org).",1,0,0
10.1016/j.neuroimage.2022.119649,gmsh.info,"A tetrahedral grid was generated from the surface mesh of the brain using Gmsh (https://gmsh.info) (Geuzaine and Remacle, 2009).",1,0,0
10.1016/j.neuroimage.2022.119649,github.com/slicercbm/slicercbm,The SlicerCBM extension is made freely available under the BSD open-source license and can be down-loaded from our GitHub repository (https://github.com/SlicerCBM/SlicerCBM).,1,0,0
10.1016/j.neuroimage.2022.119128,nitrc.org/projects/conn,Data availability The Coon toolbox is available to download from http://www.nitrc.org/projects/conn.,1,0,0
10.1016/j.neuroimage.2022.119128,physionet.org/content/sampen/1.0.0,The sample entropy func-tion is freely available (https://physionet.org/content/sampen/1.0.0/).,1,0,0
10.1016/j.neuroimage.2022.119128,nitrc.org/projects/artefact_detect,"The ART quality-assurance/motion-artefact rejec-tion toolbox (https://www.nitrc.org/projects/artefact_detect), as imple-mented in CONN, was also used to further remove motion-related arti-facts in the timeseries data.",1,0,0
10.1016/j.neuroimage.2022.119128,web.conn-toolbox.org,Time series extraction Denoising steps were performed in the Matlab and SPM-based software CONN (17.f) (https://web.conn-toolbox.org/).,1,0,0
10.1016/j.neuroimage.2022.119128,complexsystemsupenn.com/codedata,The small world propensity measure code (PHI) can be obtained from https://complexsystemsupenn.com/codedata.,1,0,0
10.1016/j.neuroimage.2022.119128,openneuro.org/datasets/ds003171/versions/1.0.0,LON dataset: Freely available from https://openneuro.org/datasets/ds003171/versions/1.0.0 Declaration of Competing Interest The authors have no competing interests to declare.,0,1,0
10.1016/j.neuroimage.2022.119128,sites.google.com/site/bctnet,"The brain connectivity tool-box, which was used for the graph theory measures can be ob-tained gratuitously online (https://sites.google.com/site/bctnet/).",1,0,0
10.1016/j.neuroimage.2022.119128,ﬁl.ion.ucl.ac.uk/spm/software/spm12,Preprocessing All functional images were preprocessed in the same way using an in-house Matlab script that used SPM12 functions (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12).,1,0,0
10.1016/j.neuroimage.2022.119629,github.com/fnndsc/fetal-brain-segmentation,"For MRI processing, the algo-rithms in the Methods are publicly available:1) brain extraction (https://github.com/FNNDSC/fetal-brain-segmentation) and 2) cor-tical plate segmentation (https://github.com/jwhong1125/fetal_CP_segmentation).",1,0,0
10.1016/j.neuroimage.2022.119629,github.com/jwhong1125/fetal_cp_segmentation,"For MRI processing, the algo-rithms in the Methods are publicly available:1) brain extraction (https://github.com/FNNDSC/fetal-brain-segmentation) and 2) cor-tical plate segmentation (https://github.com/jwhong1125/fetal_CP_segmentation).",1,0,0
10.1016/j.neuroimage.2022.119629,surfer.nmr.mgh.harvard.edu,Geometrical smoothing was performed to reduce the noise and small geometric changes of cortical surface mod-els using Freesurfer (https://surfer.nmr.mgh.harvard.edu).,1,0,0
10.1016/j.neuroimage.2022.119559,github.com/nih-megcore/opmlab,"Python scripts using the FieldLine APIs (available for down-load at https://github.com/nih-megcore/OPMLab) were used for data acquisition and dynamic control of the currents on the transverse axes., Data and Code Availability Statement The Python scripts for implementing dynamic ﬁeld compensation on the FieldLine OPM electronics are available for download at https://github.com/nih-megcore/OPMLab.",1,0,0
10.1016/j.neuroimage.2022.119733,osf.io/n5bj7,"See ﬁgure supplements showing each individual T ∗ 2 , R ∗ 2 , and 𝑆 0 at https://osf.io/n5bj7 under Supplementary Figures folder., See ﬁgure supplements showing each individual T 1 , R 1 , and 𝑆 0 at https://osf.io/n5bj7 under Supplementary Figures folder.",0,0,1
10.1016/j.neuroimage.2022.119733,zenodo.org/record/7210802,"Data Analysis Data processing and analysis scripts referenced in this section are available at: https://github.com/ofgulban/meso-MRI (v1.0.2 saved at https://zenodo.org/record/7210802)., Processing and analysis scripts used in this study are available at: https://github.com/ofgulban/meso-MRI (v1.0.2 saved at https://zenodo.org/record/7210802)., Processing and analysis scripts used in this study are available at: https://github.com/ofgulban/meso-MRI (v1.0.2 saved at https://zenodo.org/record/7210802).",1,0,0
10.1016/j.neuroimage.2022.119733,github.com/pyushkevich/greedy,"All runs are co-registered to the ﬁrst MP2RAGE run using the second inversion time (INV2) contrast with the GREEDY (Yushkevich et al., 2016) registration algorithm (acquired from: https://github.com/pyushkevich/greedy).",1,0,0
10.1016/j.neuroimage.2022.119733,neurostars.org/t/multi-echo-anatomical-mri-bids-questions/17157,"Additionally, we thank Elizabeth DuPre, Gilles De Hollander, Taylor Salo, and Agah Karakuzu for their help with Brain Imaging Data Struc-ture (BIDS) organization of our dataset (https://neurostars.org/t/multi-echo-anatomical-mri-bids-questions/17157).",0,1,0
10.1016/j.neuroimage.2022.119733,thingsonthings.org/ln2_multilaterate,"How-ever, interested readers can refer to the interim description available at https://thingsonthings.org/ln2_multilaterate.",0,0,1
10.1016/j.neuroimage.2022.119733,github.com/ofgulban/meso-mri,"Data Analysis Data processing and analysis scripts referenced in this section are available at: https://github.com/ofgulban/meso-MRI (v1.0.2 saved at https://zenodo.org/record/7210802)., Processing and analysis scripts used in this study are available at: https://github.com/ofgulban/meso-MRI (v1.0.2 saved at https://zenodo.org/record/7210802)., Processing and analysis scripts used in this study are available at: https://github.com/ofgulban/meso-MRI (v1.0.2 saved at https://zenodo.org/record/7210802).",1,0,0
10.1016/j.neuroimage.2022.119520,2.2.3.1,Consistency in channel selection 2.2.3.1.,0,0,1
10.1016/j.neuroimage.2022.119520,2.2.2.1,2.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119520,3.2.2.1,3.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119520,2.1.5.5,2.1.5.5.,0,0,1
10.1016/j.neuroimage.2022.119520,osf.io/mx49d,"NeuroImage 261 (2022) 119520 Data availability statement Sample analytic code that forms the basis for all analyses reported here, as well as a simulated infant fNIRS dataset (i.e., study 3 dataset) are publicly available on Open Science Framework (https://osf.io/mx49d/).",0,1,0
10.1016/j.neuroimage.2022.119520,2.2.2.2,2.2.2.2.,0,0,1
10.1016/j.neuroimage.2022.119520,2.1.5.3,2.1.5.3.,0,0,1
10.1016/j.neuroimage.2022.119520,3.1.4.10,Data analysis 3.1.4.10.,0,0,1
10.1016/j.neuroimage.2022.119520,4.2.2.1,4.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119520,4.1.2.1,Data analysis 4.1.2.1.,0,0,1
10.1016/j.neuroimage.2022.119520,2.1.5.4,2.1.5.4.,0,0,1
10.1016/j.neuroimage.2022.119520,3.2.2.2,3.2.2.2.,0,0,1
10.1016/j.neuroimage.2022.119520,2.1.5.2,2.1.5.2.,0,0,1
10.1016/j.neuroimage.2022.119520,3.2.3.1,Consistency in channel selection 3.2.3.1.,0,0,1
10.1016/j.neuroimage.2022.119520,2.2.3.2,2.2.3.2.,0,0,1
10.1016/j.neuroimage.2022.119520,3.2.3.2,3.2.3.2.,0,0,1
10.1016/j.neuroimage.2022.119520,2.1.4.1,Data analysis 2.1.4.1.,0,0,1
10.1016/j.neuroimage.2022.119520,4.1.2.2,4.1.2.2.,0,0,1
10.1016/j.neuroimage.2022.119244,afni.nimh.nih.gov,"The codes for fMRI data analysis and graph-theory analysis can be found in the AFNI (https://afni.nimh.nih.gov), the FSL (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki), and the Brain Connectivity Toolbox (https://www.nitrc.org/projects/bct).",1,0,0
10.1016/j.neuroimage.2022.119244,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"The codes for fMRI data analysis and graph-theory analysis can be found in the AFNI (https://afni.nimh.nih.gov), the FSL (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki), and the Brain Connectivity Toolbox (https://www.nitrc.org/projects/bct).",1,0,0
10.1016/j.neuroimage.2022.119244,neurosciencelibrary.org/specimens/primates/squirrelmonk,"The anatomical location of each seed was determined by overlaying the multi-run heat-evoked activation map obtained in each imaging session on their corresponding three T2 weighted structural image planes (sagittal, axial, and coronal), and then on the squirrel monkey atlas, publications involving histology of squirrel monkeys (Gergen and MacLean, 1962 ; Jones and Burton, 1976) and an online reference (http://neurosciencelibrary.org/specimens/primates/squirrelmonk/).",0,0,1
10.1016/j.neuroimage.2022.119244,nitrc.org/projects/bct,"The codes for fMRI data analysis and graph-theory analysis can be found in the AFNI (https://afni.nimh.nih.gov), the FSL (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki), and the Brain Connectivity Toolbox (https://www.nitrc.org/projects/bct).",1,0,0
10.1016/j.neuroimage.2022.119244,mrvar.fdv.uni-lj.si/pajek,"Weighted undirected graphs (plotted in Pajek 5.14,Uni-versity of Ljubljana, http://mrvar.fdv.uni-lj.si/pajek/, Batagelj and Mr-var, 1998) by assigning links to all supra-threshold correlations, con-verted from the averaged region-region correlation matrix, are illus-trated in Fig. 5 A (with the threshold of 0.16), Fig. 5 B (with the thresh-old of 0.21), and Fig. 5 F (with the threshold of 0.27).",1,0,0
10.1016/j.neuroimage.2022.119737,bdr.birncommunity.org:8080/bdr,"Data for Schizophrenia used in this study were downloaded from the Function BIRN Data Repository (http://bdr.birncommunity.org:8080/BDR/), supported by grants to the Function BIRN (U24-RR021992) Testbed funded by the National Center for Research Resources at the National In-stitutes of Health, U.S.A.",0,1,0
10.1016/j.neuroimage.2022.119737,trendscenter.org/data/number,"However, the choice of components (and 1 We use FBIRN phase III. 2 http://fcon_1000.projects.nitrc.org/indi/abide/3 https://www.oasis-brains.org/4 https://trendscenter.org/data/number of components) can inﬂuence accuracy, but our study is not fo-cusing on determining the best number of ICs rather use the available components and let the model decide the task-dependant components.",1,0,0
10.1016/j.neuroimage.2022.119737,github.com/usmanmahmood27/dice,The code for the DICE model is available here https://github.com/UsmanMahmood27/DICE.,1,0,0
10.1016/j.neuroimage.2022.119737,fcon_1000.projects.nitrc.org/indi/abide/3,"However, the choice of components (and 1 We use FBIRN phase III. 2 http://fcon_1000.projects.nitrc.org/indi/abide/3 https://www.oasis-brains.org/4 https://trendscenter.org/data/number of components) can inﬂuence accuracy, but our study is not fo-cusing on determining the best number of ICs rather use the available components and let the model decide the task-dependant components.",1,0,0
10.1016/j.neuroimage.2022.119737,oasis-brains.org/4,"However, the choice of components (and 1 We use FBIRN phase III. 2 http://fcon_1000.projects.nitrc.org/indi/abide/3 https://www.oasis-brains.org/4 https://trendscenter.org/data/number of components) can inﬂuence accuracy, but our study is not fo-cusing on determining the best number of ICs rather use the available components and let the model decide the task-dependant components.",1,0,0
10.1016/j.neuroimage.2022.119737,ﬁl.ion.ucl.ac.uk/spm,"ICA parcellation: For all experiments conducted using ICA as brain parcellation technique the fMRI data was preprocessed using statistical parametric mapping (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) un-der the MATLAB 2021 environment.",1,0,0
10.1016/j.neuroimage.2022.119737,github.com/alvarouc/polyssiﬁer,"In the following sections we show a) classiﬁcation per-formance of our model, b) learned DC and DNC and c) the eﬀects of temporal attention module. 5 https://github.com/alvarouc/polyssiﬁer 6 U., Machine learning models’ results are computed using python package Polyssiﬁer, available at https://github.com/alvarouc/polyssiﬁer Ethics statement All the datasets used in this study are either publicly available or proper consent was taken before using the datasets.",1,0,0
10.1016/j.neuroimage.2022.119512,nitrc.org/projects/mricron,A publicly available free software MRICron (https://www.nitrc.org/projects/mricron) was utilized for the manual delineations.,1,0,0
10.1016/j.neuroimage.2022.119004,psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower,"The following software used in our analyses is publicly available: Brain Vision Analyzer 2.0 (Brain Products, www.brainproducts.com), LORETA-KEY software (http://www.uzh.ch/keyinst/loreta), G ∗ Power 3.1 (https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower).",1,0,0
10.1016/j.neuroimage.2022.119004,uzh.ch/keyinst/loreta,"The following software used in our analyses is publicly available: Brain Vision Analyzer 2.0 (Brain Products, www.brainproducts.com), LORETA-KEY software (http://www.uzh.ch/keyinst/loreta), G ∗ Power 3.1 (https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower).",1,0,0
10.1016/j.neuroimage.2022.119004,brainproducts.com,"The following software used in our analyses is publicly available: Brain Vision Analyzer 2.0 (Brain Products, www.brainproducts.com), LORETA-KEY software (http://www.uzh.ch/keyinst/loreta), G ∗ Power 3.1 (https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower).",1,0,0
10.1016/j.neuroimage.2022.119263,neu-rosynth.org,"The third column in Fig. 3 showed the co-activation maps of the identiﬁed clusters generated using neu-rosynth.org (https://neurosynth.org), based on a large number of task-based fMRI activation studies (see Materials and Methods).",1,0,0
10.1016/j.neuroimage.2022.119263,neurosynth.org,"Co-activation maps were generated using neurosynth.org (https://neurosynth.org), a platform for large-scale automated syn-thesis of fMRI data., The third column in Fig. 3 showed the co-activation maps of the identiﬁed clusters generated using neu-rosynth.org (https://neurosynth.org), based on a large number of task-based fMRI activation studies (see Materials and Methods)., The third column shows the co-activation maps of the obtained clusters generated with neurosynth.org (https://neurosynth.org).",1,0,0
10.1016/j.neuroimage.2022.119263,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"RSN atlases used in this study In this study, we used the probabilistic cerebellar atlas (Diedrichsen, 2006) distributed in the FSL soft-ware package (FSL; FMRIB Software Library; Oxford, UK; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/).",1,0,0
10.1016/j.neuroimage.2022.119263,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"Al-most all image preprocessing steps employed Statistical Parametric Map-ping 12 software (SPM12; Wellcome Trust Center for Neuroimaging, London, UK; http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) run-ning on MATLAB (R2019a, MathWorks, Natick, Mass, USA).",1,0,0
10.1016/j.neuroimage.2022.119263,alivelearn.net/xjview,"First, we identiﬁed the MNI coordinate of the center of the gravity of each cluster extracted using xjview software (https://www.alivelearn.net/xjview/).",1,0,0
10.1016/j.neuroimage.2022.119263,sites.wustl.edu/petersenschlaggarlab/resources,"There are 12 canoni-cal RSNs included: the DMN, cingulo-parietal network, fronto-parietal network, salience network, cingulo-opercular network, ventral attention (language) network, dorsal attention network, retrosplenial temporal network, visual network, auditory network, sensorimotor-hand network, and sensorimotor-mouth network (https://sites.wustl.edu/petersenschlaggarlab/resources/).",0,0,1
10.1016/j.neuroimage.2022.119263,neurosynth.org,"Co-activation maps were generated using neurosynth.org (https://neurosynth.org), a platform for large-scale automated syn-thesis of fMRI data., The third column in Fig. 3 showed the co-activation maps of the identiﬁed clusters generated using neu-rosynth.org (https://neurosynth.org), based on a large number of task-based fMRI activation studies (see Materials and Methods)., The third column shows the co-activation maps of the obtained clusters generated with neurosynth.org (https://neurosynth.org).",1,0,0
10.1016/j.neuroimage.2021.118841,github.com/delislab,Data and Code Availability The supplementary datasets (behavioural and neural) and custom-script MATLAB/Python codes used for analysis and modelling during the current study are available from the study’s online data repository (www.github.com/DelisLab).,0,1,0
10.1016/j.neuroimage.2022.118932,ﬁl.ion.ucl.ac.uk/spm,"For result visualization, the functional activation map was coreg-istered to the high-resolution T1-weighted image (AC-PC plane aligned) using SPM (https://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.118932,surfer.nmr.mgh.harvard.edu,"Cortical sur-face of DN’s brain was also reconstructed by segmenting the anatomical volumes into gray and white matter using Freesurfer (https://surfer.nmr.mgh.harvard.edu/, version 5.0).",1,0,0
10.1016/j.neuroimage.2021.118864,osf.io/u7sbf,Data and code availability The code and MRI dataset generated during this study is available on Open Science Framework: https://osf.io/u7sbf/.,0,1,0
10.1016/j.neuroimage.2021.118864,imaging.mrc-cbu.cam.ac.uk/downloads/mni2tal/tal2mni.m,"For each subject and each hemisphere, these Talairach coor-dinates were converted to Montreal Neurological Institute (MNI) co-ordinates using the tal2mni conversion utility (http://imaging.mrc-cbu.cam.ac.uk/downloads/MNI2tal/tal2mni.m).",1,0,0
10.1016/j.neuroimage.2022.118907,2.3.2.5,"2.3.2.5., Censoring thresholds were selected to ensure an equal number of volumes were removed across methods (thresholds for all methods were selected to match the percentage of volumes removed that was identiﬁed as optimal for LP-FD censoring using the methods described in Section 2.3.2.5 and Section 3.2.2.1) in the HCP500 dataset (HCP Dataset 1; without GSR: 46.68%; with GSR: 43.32%): A) FD > 0.1475 mm B) LPF-FD > 0.0318 mm C) FD > 0.155 mm D) LPF-FD > 0.0337 mm., Finally, ΔMSE-RSFC, calculated as the change in the ratio of the sum of squared bias and variance to the number of re-maining subjects (see Section 2.3.2.2), produces a U-shaped curve when plotted against percent frames removed (Fig. 5 D), and is thus suitable for optimization as described in Section 2.3.2.5.",0,1,0
10.1016/j.neuroimage.2022.118907,2.2.2.1,"Exploration of confounds in mFD-based dataset-QC metrics 2.2.2.1., In order to address the possibility that the Monte Carlo procedure described above in Section 2.2.2.1 and Supplementary Section S1.3 failed to match the distribution of motion within subjects, given that it focused exclusively on average motion, we also produced three datasets in which FH + and FH-participants were matched on a one-to-one basis based on the temporal dynamics and overall motion char-acteristics across all of their data., We divided each of three datasets (see Supplementary Section S1.4 for details on the construction of these datasets) into terciles based on mean FD to create high-and low-motion groups, similar to the procedure described in Section 2.2.2.1.",0,0,1
10.1016/j.neuroimage.2022.118907,3.2.2.1,"In HCP Dataset 1a (n = 475; shown in Section 3.1.1 and Section 3.2.2.1), we removed high-motion subjects who were dropped from analyses prior to reaching 50% of volumes censored using LPF-FD censoring, to control for subject removal when evaluating DQMs., Censoring thresholds were selected to ensure an equal number of volumes were removed across methods (thresholds for all methods were selected to match the percentage of volumes removed that was identiﬁed as optimal for LP-FD censoring using the methods described in Section 2.3.2.5 and Section 3.2.2.1) in the HCP500 dataset (HCP Dataset 1; without GSR: 46.68%; with GSR: 43.32%): A) FD > 0.1475 mm B) LPF-FD > 0.0318 mm C) FD > 0.155 mm D) LPF-FD > 0.0337 mm., Thresholds were selected such that an equivalent num-ber of volumes were removed by each evaluated method to allow an apples-to-apples comparison between censoring methods (the values chosen for display were the optimal number of volumes removed for LPF-FD for all FD measures and the optimal values for GEV-DV for all DV measures, determined separately for analyses with and without GSR, as presented in Section 3.2.2.1 , below). 10 J.C., Moreover, we would note that we did not fail to ob-serve distance-dependent eﬀects more broadly (see, e.g., Section 3.2.2.1 , below), only that standard FD and DV censoring did not produce these eﬀects., 3.2.2.1.",0,1,0
10.1016/j.neuroimage.2022.118907,2.3.2.6,"2.3.2.6., Thus, we developed a novel quantitative approach to ﬁnding optimal censoring thresholds, termed ΔMSE-RSFC, which is based on an estimated bias-variance de-composition of the mean squared error (MSE) of sample mean RSFC correlations (see Section 2.3.2.1 through Section 2.3.2.6)., Optimization of volume censoring parameters in arbitrary datasets Finally, we tested the ΔMSE-RSFC optimization procedure in a vali-dation set collected at the New York State Psychiatric Institute (NYSPI) as described in Section 2.3.2.6 , and present the results in Fig.",1,0,0
10.1016/j.neuroimage.2022.118907,2.3.2.4,"To this end, we developed a novel DQM, described in full in Section 2.3.2.2 through Section 2.3.2.4 , which we term ΔMSE-RSFC., Thus, Eq. 5 can also be written as: ΔMSE -RSF 𝐶 𝑘 = ̂𝜎2 (̂𝜃𝑐 𝑘) + Bias (̂𝜃𝑐 𝑘 , 𝜃𝑘)2 𝑁 𝑆 − ̂𝜎2 (̂𝜃𝑢 𝑘) + Bias (̂𝜃𝑢 𝑘 , 𝜃𝑘)2 𝑁 𝑆 𝑈 , (6) where 𝐵𝑖𝑎𝑠 (̂𝜃𝑐 𝑘 , 𝜃𝑘) 2 is the motion-induced bias in RSFC correlation k after censoring; 𝐵𝑖𝑎𝑠 (̂𝜃𝑢 𝑘 , 𝜃𝑘) 2 is the bias in RSFC correlation k before censoring, estimated as described in Section 2.3.2.4., 𝐵𝑖𝑎𝑠 (̂𝜃𝑐 𝑘 , 𝜃𝑘) 2 and 𝐵𝑖𝑎𝑠 (̂𝜃𝑢 𝑘 , 𝜃𝑘) 2 are respec-tively estimated as described in Section 2.3.2.3 and Section 2.3.2.4 , be-low., (12) Eq. 7 may be used to calculate Δ𝐵𝑖𝑎𝑠 (̂𝜃𝐶 𝑘 , 𝜃𝑘) , and 𝐵𝑖𝑎𝑠 (̂𝜃𝑢 𝑘 , 𝜃𝑘) 2 is calculated as described in Section 2.3.2.4 , below., 2.3.2.4.",1,0,0
10.1016/j.neuroimage.2022.118907,2.1.1.2,"2.1.1.2., We employed this technique to obtain optimal volume censoring parameters for combined LPF-FD and GEV-DV volume censoring in the NYSPI Dataset (n = 26; see Section 2.1.1.2).",0,1,0
10.1016/j.neuroimage.2022.118907,github.com/washington-university/hcppipelines,"Resting-state functional connectivity preprocessing Data from NYSPI were ﬁrst preprocessed using the Human Connec-tome Project (HCP) Minimal Preprocessing Pipelines (Glasser et al., 2013) version 3.20.0 (https://github.com/Washington-University/HCPpipelines).",1,0,0
10.1016/j.neuroimage.2022.118907,2.1.2.1,"Nuisance signals from white matter, cerebrospinal ﬂuid, and global signal were obtained follow-ing published methods (Power et al., 2014), described in detail in Section 2.1.2.1., 2.1.2.1., Finally, this approach also has the advantage of requiring substantially less disk space, as only the ROI timeseries (Section 2.1.2) and nuisance signals (see Section 2.1.2.1 and the following paragraph) need to be stored, rather than a fully processed whole-brain volume.",1,0,0
10.1016/j.neuroimage.2022.118907,2.2.2.2,2.2.2.2.,0,0,1
10.1016/j.neuroimage.2022.118907,2.1.3.2,NeuroImage 249 (2022) 118907 2.1.3.2.,0,0,1
10.1016/j.neuroimage.2022.118907,2.1.3.1,VOL censoring methods 2.1.3.1.,1,0,0
10.1016/j.neuroimage.2022.118907,db.humanconnectome.org,"All Human Connectome Project resting-state data were downloaded in minimally-preprocessed form from the “Resting State fMRI 1 Preprocessed ”a n d “Resting State fMRI 2 Preprocessed ”packages for each subject on db.humanconnectome.org (Hodge et al., 2016)., Data and code availability The data employed here are, in part, publicly available from the Human Connectome Project (HCP) at https://db.humanconnectome.org (Hodge et al., 2016), and include all resting-state fMRI data from the HCP 500 Subjects Release , a subset of the working memory task data from the HCP 500 Subjects Release, and a subset of the resting-state fMRI data from the HCP 1200 Subjects Release (Barch et al., 2013 ; Glasser et al., 2013 ; Smith et al., 2013 ; Ugurbil et al., 2013 ; Van Essen et al., 2013), as described in Section 2.1.1.1 and Supplementary Section S1.1.1.",0,1,0
10.1016/j.neuroimage.2022.118907,2.3.2.2,"Note further that, while ΔMSE-RSFC (see Section 2.3.2.2) decomposes the eﬀects of volume censoring on sample RSFC estimates into bias and variance components, MAC-RSFC measures both in aggregate., To this end, we developed a novel DQM, described in full in Section 2.3.2.2 through Section 2.3.2.4 , which we term ΔMSE-RSFC., 2.3.2.2., For each censoring threshold, ΔMSE-RSFC was calculated as described in Section 2.3.2.2., Finally, ΔMSE-RSFC, calculated as the change in the ratio of the sum of squared bias and variance to the number of re-maining subjects (see Section 2.3.2.2), produces a U-shaped curve when plotted against percent frames removed (Fig. 5 D), and is thus suitable for optimization as described in Section 2.3.2.5.",1,0,0
10.1016/j.neuroimage.2022.118907,3.2.2.2,"To validate ΔMSE-RSFC-based optimization of volume censoring parameters (see Section 3.2.2.2), we employed a sample of resting state data acquired from 26 healthy participants (8 female, 18 male, mean age = 33.58) using a General Electric (GE; Boston, MA) MR750 3T scanner (multiband factor 6, TR = 850 ms, voxel size = 2 mm isotropic) at the New York State Psychiatric Institute (NYSPI; New York, NY), over 4 runs of 15 minutes each., 3.2.2.2.",0,1,0
10.1016/j.neuroimage.2022.118907,2.1.3.4,2.1.3.4.,0,0,1
10.1016/j.neuroimage.2022.118907,2.1.2.2,"GS, white matter, and cerebro-spinal ﬂuid signals were all calculated prior to the removal of other nuisance signals (see Section 2.1.2.2)., 2.1.2.2.",0,0,1
10.1016/j.neuroimage.2022.118907,2.1.3.3,2.1.3.3.,0,0,1
10.1016/j.neuroimage.2022.118907,2.1.1.1,"Functional magnetic resonance imaging datasets 2.1.1.1., 1), we show in Fig. 6 the diﬀerence in RSFC correlations produced by optimal LPF-FD censoring using ΔMSE-RSFC over and above the optima suggested by minimizing median absolute QC-FC (Ciric et al., 2017) in HCP Dataset 1a (i.e., after removing 26 high-motion subjects from HCP Dataset 1 in order to control for the eﬀect of subject removal by excessive volume censoring; see Section 2.1.1.1)., Data and code availability The data employed here are, in part, publicly available from the Human Connectome Project (HCP) at https://db.humanconnectome.org (Hodge et al., 2016), and include all resting-state fMRI data from the HCP 500 Subjects Release , a subset of the working memory task data from the HCP 500 Subjects Release, and a subset of the resting-state fMRI data from the HCP 1200 Subjects Release (Barch et al., 2013 ; Glasser et al., 2013 ; Smith et al., 2013 ; Ugurbil et al., 2013 ; Van Essen et al., 2013), as described in Section 2.1.1.1 and Supplementary Section S1.1.1.",0,1,0
10.1016/j.neuroimage.2022.119239,surfer.nmr.mgh.harvard.edu/fswiki/hippocampalsubﬁeldsand,"The volume calculations for the anterior and posterior HC were done in Freesurfer 7 HC subﬁelds and nuclei of amygdala protocol (https://surfer.nmr.mgh.harvard.edu/fswiki/HippocampalSubﬁeldsAnd NucleiOfAmygdala).Included in the anterior Section was the hippocam-pal head, and in the posterior section was the body and tail.",1,0,0
10.1016/j.neuroimage.2022.119687,osf.io/fn7jc,"Code availability The custom-written code that was used for data processing and sta-tistical analyses is publicly available at: https://osf.io/fn7jc/., NeuroImage 264 (2022) 119687 Data availability The data of the alternating stimulation conditions used in this study are openly available at: https://osf.io/fn7jc/.",1,0,0
10.1016/j.neuroimage.2022.119687,uke.de/english/departments-institutes/institutes/neurophysiology-and-pathophysiology/research/research-groups/index.html,"The MATLAB implementation of the eLORETA algorithm was obtained from the MEG/EEG Toolbox of Ham-burg (METH; https://www.uke.de/english/departments-institutes/institutes/neurophysiology-and-pathophysiology/research/research-groups/index.html)., We used the PSI implementation of the MEG/EEG Toolbox of Hamburg (METH; https://www.uke.de/english/departments-institutes/institutes/neurophysiology-and-pathophysiology/research/research-groups/index.html).",1,0,0
10.1016/j.neuroimage.2021.118846,nitrc.org/projects/conn,"Functional connectivity analysis Functional connectivity was assessed with the CONN toolbox (www.nitrc.org/projects/conn) to examine coupling between the main brain regions of interest (ROI) identiﬁed in the group’s statistics (second-level fMRI analysis) for the comparison of previously HIGH and LOW re-warded distractors relative to Neutral distractors in the high-predictive condition (HR + LR > Neutral): right fusiform gyrus (FG), right/left Frontal Eye Field (R/L FEF), left Striatum, right Superior Parietal Lob-ule (R SPL), Superior Colliculus (SC).",1,0,0
10.1016/j.neuroimage.2021.118846,ion.ucl.ac.uk,ion.ucl.ac.uk).,0,0,1
10.1016/j.neuroimage.2021.118829,clinicaltrials.gov,The protocol was approved by the institutional review board (EK Nr.: 1739/2016) and the study was registered at clinicaltrials.gov (NCT02753738).,0,0,1
10.1016/j.neuroimage.2021.118829,brainwavelet.org,The BrainWavelet Toolbox version 2 (https://www.brainwavelet.org/) was additionally used for preprocessing.,1,0,0
10.1016/j.neuroimage.2021.118829,ﬁl.ion.ucl.ac.uk/spm/software/spm12,ﬁl.ion.ucl.ac.uk/spm/software/spm12/).,1,0,0
10.1016/j.neuroimage.2021.118763,fmrib.ox.ac.uk/fsl),"MRI preprocessing Processing of the MRI data was carried out using FSL (FMRIB’s Soft-ware Library, www.fmrib.ox.ac.uk/fsl), provided by the Functional MRI of the Brain (FMRIB) Analysis Group at University of Oxford, UK.",1,0,0
10.1016/j.neuroimage.2021.118763,fsl.fmrib.ox.ac.uk/eeglab/fmribplugin,"To remove them, the EEGs acquired inside the MR scanner were post-processed with the average artifact subtraction method of the FMRIB plug-in (https://fsl.fmrib.ox.ac.uk/eeglab/fmribplugin/) for EEGLAB (https://sccn.ucsd.edu/eeglab/index.php , Delorme and Makeig, 2004; Iannetti et al., 2005; Niazy et al., 2005) including ﬁtting and subtraction of optimal basis functions (OBS) derived from the ﬁrst three principal components of the artifact residuals.",1,0,0
10.1016/j.neuroimage.2021.118763,sccn.ucsd.edu/eeglab/index.php,"To remove them, the EEGs acquired inside the MR scanner were post-processed with the average artifact subtraction method of the FMRIB plug-in (https://fsl.fmrib.ox.ac.uk/eeglab/fmribplugin/) for EEGLAB (https://sccn.ucsd.edu/eeglab/index.php , Delorme and Makeig, 2004; Iannetti et al., 2005; Niazy et al., 2005) including ﬁtting and subtraction of optimal basis functions (OBS) derived from the ﬁrst three principal components of the artifact residuals.",1,0,0
10.1016/j.neuroimage.2022.119261,ﬁl.ion.ucl.ac.uk,"The subject-speciﬁc GM and WM masks aligned to corresponding mean late frame (45–60 min) PET images were derived from the individual T1-weighted tissue segmentation in SPM 12 (http://www.ﬁl.ion.ucl.ac.uk) with the tissue segmented image thresholded to include voxels with tissue prob-abilities > 0.9 or > 0.8, respectively.",1,0,0
10.1016/j.neuroimage.2022.119261,store.teamplay.siemens.com/apps,caliPER will be uploaded to the Siemens Healthineers Digital Marketplace and made available to users after creating and registering a free account (https://store.teamplay.siemens.com/apps).,0,0,1
10.1016/j.neuroimage.2022.119340,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,"humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release ; Van Essen et al., 2013).",0,0,1
10.1016/j.neuroimage.2022.119340,antsregistrationsyn.sh,"Comparison to Morel thalamic atlas Linear rigid and aﬃne transformations and non-linear warps be-tween native anatomical space and MNI space were generated us-ing the Advanced Normalization Tools (ANTs) package (Version 2.3.5, Ecphorella) script antsRegistrationSyN.sh (Avants et al., 2008).",1,0,0
10.1016/j.neuroimage.2022.119190,consortium.org,The project was launched by an open international call for participation via the Global Brain Consortium (https://globalbrain consortium.org/).,0,0,1
10.1016/j.neuroimage.2022.119190,informatics-collaboratory.org,"Bosch-Bayard), pedro.valdes@neuro informatics-collaboratory.org (P.A.",0,0,1
10.1016/j.neuroimage.2022.119190,bezout.dm.unipi.it/software/mmtoolbox,"In particular, we use the software implemented by Bini and Ian-nazzo (2013) (http://bezout.dm.unipi.it/software/mmtoolbox/) based on the Richardson-like iteration.",1,0,0
10.1016/j.neuroimage.2022.119190,2.3.2.1,"NeuroImage 256 (2022) 119190 2.3.2.1., To obtain 𝐲 λ𝑖., we only apply steps 2.3.2.1 and 2.3.2.3 for preprocessing.",0,0,1
10.1016/j.neuroimage.2022.119190,github.com/ccc-members/bc-v_group_stat/blob/master/data_gatherer.m,(GitHub lo-cation of the script: https://github.com/CCC-members/BC-V_group_stat/blob/master/data_gatherer.m).,1,0,0
10.1016/j.neuroimage.2022.119190,synapse.org,"Data and code available The shared raw cross-spectra with encrypted ID is hosted at Synapse.org (10.7303/syn26712693) and complete access is possible after login in the system., The multinational harmonized norms (HarM-NqEEG norms) of traditional log-spectra and Riemannian cross-spectra are hosted at Synapse.org (10.7303/syn26712979).",0,1,0
10.1016/j.neuroimage.2022.119190,alz.confex.com/alz/20amsterdam/meetingapp.cgi/paper/47837,"Country Dataset sites N individuals (Female; Male) Age range Device Reference Year recorded Citation Barbados 62 (F28; M34) Barbados_1978 62 (F28; M34) 5.5–11.4 DEDAAS Linked Ears 1978 (Bringas Vega et al., 2019 ; Taboada-Crispi et al., 2018) China 268 (F141; M127) Chengdu_2014 33 (F7; M26) 21–28 BrainAmp DC REST 2014 (Li et al., 2015) Chongqing_2016 235 (F134; M101) 15–26 BrainAmp MR plus common 2016 -2019 (Duan et al., 2021) Colombia 21 (F13; M8) Colombia_2019 21 (F13; M8) 22–45 Neuro scan average 2019 https://alz.confex.com/alz/20amsterdam/meetingapp.cgi/Paper/47837 Cuba 367 (F153; M214) Cuba_1990 195 (F98; M97) 5.5–97 Medicid-3M common 1990 (Bosch-Bayard et al., 2020) Cuba_2003 48 (F28; M20) 5–69 Medicid-4 common 2003 (Hernandez-Gonzalez et al., 2011) CHBMP 124 (F27; M97) 17–62 Medicid-5 Linked Ears 2004–2008 (Valdes-Sosa et al., 2021) Germany 178 (F113; M65) Germany_2013 178 (F113; M65) 22.5–77.5 BrainAmp MR plus common 2013 (Babayan et al., 2019) Malaysia 26 (F24; M2) Malaysia_2017 26 (F24; M2) 19–60 ANT Neuro average 2017/2020 –Russia203 (F104; M99)) Russia_2013 58 (F34; M24) 18–49 nvx136 Cz 2013/2019 (Ivanov et al., 2022) 145 (F70; M75) 16–57 actiCHamp Cz 2013/2019 Switzerland209 (F98; M111) Bern_1980 44 (F18;26) 10–16 Nihon Kohden common 1980 (Koenig et al., 2002); Zurich_2017 165 (F80; M85) 18–90 EGI-256 HCGSN average 2012/2017–2019 (Langer et al., 2013) USA230 (F109; M121) New York_1970s 230 (F109; M121) 6–80.5 DEDAAS common 1970s-1980s (Ahn et al., 1980) Total 1564 (F783; M781) 24 M.",0,0,1
10.1016/j.neuroimage.2022.119190,github.com/lmnonlinear/harmnqeeg,"Additionally, the corresponding HarMNqEEG code for calculating the z-scores based on the HarMNqEEG norm opened in GitHub, see: https://github.com/LMNonlinear/HarMNqEEG.",1,0,0
10.1016/j.neuroimage.2022.119190,3design.github.io/globalbrainconsortium.org/projects.html#,The search for such biomarkers in COVID-Induced Brain Dysfunction is another project of the Global Brain Consortium (https://3design.github.io/GlobalBrainConsortium.org/projects.html#).,0,0,1
10.1016/j.neuroimage.2022.119190,2.3.2.3,"2.3.2.3., To obtain 𝐲 λ𝑖., we only apply steps 2.3.2.1 and 2.3.2.3 for preprocessing.",1,0,0
10.1016/j.neuroimage.2022.119190,2.3.2.4,"The estimator of 𝜅𝑖 is the geometric mean of power spectrum, 𝜅𝑖 = 2 log (𝛾𝑖) , ̂𝜅𝑖 = 1 𝑁𝜔 Nc 𝑁𝜔 ∑𝜔 =3Δ𝜔 Nc ∑𝑐=1 log (⌢ 𝑠 𝑖,𝑐,𝑐 (𝜔))(10) Then the GSF-corrected cross-spectrum can be represented as, ⌣ 𝐒 𝑖 (𝜔) = ⌢ 𝐒 𝑖 (𝜔) ∕ exp (̂𝜅𝑖)(11) 2.3.2.4.",0,0,1
10.1016/j.neuroimage.2022.119190,2.1.2.2,2.1.2.2.,0,0,1
10.1016/j.neuroimage.2022.119190,3design.github.io/globalbrainconsortium.org/project-norms.html,It is one of the main projects of the Global Brain Consor-tium (https://3design.github.io/GlobalBrainConsortium.org/project-norms.html).,0,0,1
10.1016/j.neuroimage.2022.119190,2.3.2.2,2.3.2.2.,0,0,1
10.1016/j.neuroimage.2022.119190,2.1.2.1,2.1.2.1.,0,0,1
10.1016/j.neuroimage.2022.119651,coins.trendscenter.org,Data and code availability The data used in this article will be made publicly available through the COINS framework at the completion of the study (https://coins.trendscenter.org/).,0,1,0
10.1016/j.neuroimage.2022.119651,github.com/nichrishayes/artifactscantool,"Using cus-tom lab software (https://github.com/nichrishayes/ArtifactScanTool), amplitude and gradient metrics for each epoch were computed, and epochs containing outlier values were rejected using an individualized ﬁxed threshold method, supplemented with visual inspection.",1,0,0
10.1016/j.neuroimage.2022.119180,github.com/mandymejia/bayesfmri,Model ﬁtting was performed in using the BayesfMRI R package (version 1.8) (https://github.com/mandymejia/BayesfMRI/).,1,0,0
10.1016/j.neuroimage.2022.119180,github.com/mandymejia/als-bayesianglm-paper,All code used to perform the data anal-yses presented in this paper is available at the following Github reposi-tory: https://github.com/mandymejia/ALS-BayesianGLM-paper/. 10 A.F.,1,0,0
10.1016/j.neuroimage.2022.119044,gitlab.com/clh96/face,Code availability The code to recreate ﬁgures and statistics is available at the cor-responding author’s GitLab repository (https://gitlab.com/CLH96/face 8 C.L.,1,0,0
10.1016/j.neuroimage.2022.119227,osf.io/qgdn9/declaration,"The raw dataset is not publicly available due to inherently identiﬁable nature of the data, but other data (stimuli, behavioral response data, and resulting group con-trasts) are available in the OSF repository: https://osf.io/qgdn9/Declaration of Competing Interest The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to inﬂuence the work reported in this paper.",0,1,0
10.1016/j.neuroimage.2021.118783,warwick.ac.uk/snpm,"Next, to test which regions showed higher than chance orientation de-coding performance, we used SnPM13 (http://warwick.ac.uk/snpm) to assess signiﬁcance using nonparametric permutation tests (Nichols and Holmes, 2001) with 10,000 permutations and 6 mm FWHM variance smoothing.",1,0,0
10.1016/j.neuroimage.2021.118783,osf.io/uwb48,"Code is available at: https://osf.io/uwb48/3., Code is available at: https://osf.io/uwb48/.",1,0,0
10.1016/j.neuroimage.2021.118783,osf.io/uwb48/3,Code is available at: https://osf.io/uwb48/3.,1,0,0
10.1016/j.neuroimage.2021.118783,ac.uk/spm,ac.uk/spm/).,0,0,1
10.1016/j.neuroimage.2022.119002,github.com/thomasyeolab/cbig/tree/master/stable_projects/brain_parcellation/schaefer2018_localglobal,The parcellation atlas is publicly available on Github (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal).,0,1,0
10.1016/j.neuroimage.2022.119002,dsi-studio.labsolver.org,"Diﬀusion MRI data were reconstructed in DSI Studio (http://dsi-studio.labsolver.org) using q-space diﬀeomorphic re-construction (QSDR) (Yeh and Tseng, 2011) with a mean diﬀusion dis-tance ratio of 1.25.",1,0,0
10.1016/j.neuroimage.2022.119002,humanconnectome.org/study/hcp-young-adult,Data and code availability The diﬀusion MRI and functional MRI data are available through Human Connectome Project (https://www.humanconnectome.org/study/hcp-young-adult).,0,1,0
10.1016/j.neuroimage.2022.119136,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"gray matter (GM) segmentation was conducted in EPI space using the SPM tissue probability map, with p > 0.8 (http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12).",1,0,0
10.1016/j.neuroimage.2022.119136,nitrc.org,Publically available cPESTICA package cPESTICA software is (will be) available on the NeuroImaging Tools & Resources Collaboratory (NITRC) website: www.nitrc.org.,1,0,0
10.1016/j.neuroimage.2022.119551,github.com/mrtrix3/mrtrix3,"Processing of the imaging data The anatomical scans and the dMRI data were processed with our MRX-Brain ANT pipeline that uses the CSD tractography framework (Tournier et al. 2004 , 2007 , 2012) of the MRtrix3 software (https://github.com/MRtrix3/mrtrix3).",1,0,0
10.1016/j.neuroimage.2022.119551,clinicaltrials.gov,"The MORE registry data support the eﬀectiveness and safety of ANT-DBS therapy in a real-world setting in the years following implantation (MORE ClinicalTrials.gov Identiﬁer: NCT01521754, ﬁrst posted January 31, 2012).",0,1,0
10.1016/j.neuroimage.2022.119551,ida.loni.usc.edu/login.jsp,Acknowledgment In the preparation of this work we used data obtained from the MGH-USC Human Connectome Project (HCP) database (https://ida.loni.usc.edu/login.jsp); from Prof.,0,1,0
10.1016/j.neuroimage.2022.119551,protocols.humanconnectome.org/hcp/7t,"The data of 10 subjects with the full MRI acqui-sition pipeline of 2 structural, 4 resting state, 7 tasks and 1 DWI session were selected (for complete scanning protocols for each 7T imaging ses-sion see http://protocols.humanconnectome.org/HCP/7T).",0,1,0
10.1016/j.neuroimage.2022.119011,osf.io/6waj9/?view_only,"Schütz-Bosbach NeuroImage 251 (2022) 119011 advocated by the funding body of this research (Deutsch Forschungs-gemeinschaft, DFG), all data and code are also publicly available via the Open Science Framework repository under the following link: https://osf.io/6waj9/?view_only = f9e25c0bbf2140c4a813eb41695dca1e., Data availability Data and materials of this study are archived online at: https://osf.io/6waj9/?view_only = f9e25c0bbf2140c4a813eb41695dca1e.",0,1,0
10.1016/j.neuroimage.2021.118869,nitrc.org/projects/unc_brain_atlas/50,"Name Number of persons considered in template construction Age range (years) Voxel-size MIITRA_0.5mm 222 65–95 0.5 ×0.5 ×0.5mm 3 MIITRA_1mm (Ridwan et al., 2021) 222 65–95 1 ×1 ×1mm 3 SS 222 65–95 0.5 ×0.5 ×0.5mm 3 MCALT (version 1.4) (Schwarz et al., 2017) 202 30–92 0.5 ×0.5 ×0.5mm 3 ICBM2009b (nonlinear asymmetric) (Fonov et al., 2009 , 2011) 152 18–44 0.5 ×0.5 ×0.5mm 3 Colin27 (version 2008) (Aubert-Broche et al., 2006 , Holmes et al., 1998) 1(27 scans) 33 0.5 ×0.5 ×0.5mm 3 HCP-1200 (S1200 group average) (Glasser et al., 2013) 1113 22–35 0.7 ×0.7 ×0.7mm 3 ICBM2009c (nonlinear asymmetric) (Fonov et al., 2009 , 2011) 152 18–44 1 ×1 ×1mm 3 OASIS (version 2) (Avants and Tustison, 2018) 30 18–90 1 ×1 ×1mm 3 SRI24 (version 2.0) (Rohlﬁng et al., 2010) 24 19–84 1 ×1 ×1mm 3 UNC-Adult (version 2) https://www.nitrc.org/projects/unc_brain_atlas/50 20–50 1 ×1 ×1mm 3 IXI-ANTs (version 2) (Avants and Tustison, 2018) 560 20–90 1.2 ×0.94 ×0.94mm 3 Fig.",1,0,0
10.1016/j.neuroimage.2021.118869,adni.loni.usc.edu,"Arfanakis). 1 A portion of the data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf a voxel size of 1 ×1 ×1mm 3 or larger., The introduction of advanced im-age reconstruction techniques (Jia et al., 2016 ; Jafari-Khouzani, 2014 ; Manjón et al., 2010 a, 2010 b; Zhang et al., 2009), AI-based resolu-tion enhancements (Chen et al., 2018 ; Pham et al., 2019 ; Sánchez and Vilaplana, 2018 ; Zeng et al., 2018), and advances in neuroimaging soft-ware (Manjón et al., 2020 ; Park et al., 2014 ; Sone et al., 2016) and MRI hardware have sparked an interest in studying the older adult brain at submillimeter resolution (Bookheimer et al., 2019 ; de Flores et al., 2015 ; Yushkevich et al., 2015 ; ADNI3 http://adni.loni.usc.edu)., Dataset 2 consisted of T1w brain MRI data collected on 222 non-demented older adults (65-95 age-range, mean ± sd age = 80.1 ± 5.7 years, 50% female) participating in the Alzheimer’s Disease Neuroimag-ing Initiative 3 (ADNI3) (http://adni.loni.usc.edu).",0,0,1
10.1016/j.neuroimage.2021.118869,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf a voxel size of 1 ×1 ×1mm 3 or larger.,0,0,1
10.1016/j.neuroimage.2021.118869,adni.loni.usc.edu,"A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf a voxel size of 1 ×1 ×1mm 3 or larger., The introduction of advanced im-age reconstruction techniques (Jia et al., 2016 ; Jafari-Khouzani, 2014 ; Manjón et al., 2010 a, 2010 b; Zhang et al., 2009), AI-based resolu-tion enhancements (Chen et al., 2018 ; Pham et al., 2019 ; Sánchez and Vilaplana, 2018 ; Zeng et al., 2018), and advances in neuroimaging soft-ware (Manjón et al., 2020 ; Park et al., 2014 ; Sone et al., 2016) and MRI hardware have sparked an interest in studying the older adult brain at submillimeter resolution (Bookheimer et al., 2019 ; de Flores et al., 2015 ; Yushkevich et al., 2015 ; ADNI3 http://adni.loni.usc.edu)., Dataset 2 consisted of T1w brain MRI data collected on 222 non-demented older adults (65-95 age-range, mean ± sd age = 80.1 ± 5.7 years, 50% female) participating in the Alzheimer’s Disease Neuroimag-ing Initiative 3 (ADNI3) (http://adni.loni.usc.edu).",0,0,1
10.1016/j.neuroimage.2021.118869,nitrc.org/projects/unc_brain_atlas,"Comparison of MIITRA_0.5mm to other standardized and study-speciﬁc templates The MIITRA_0.5mm template was compared to other standard-ized and study-speciﬁc templates with diﬀerent voxel-sizes: three stan-dardized templates with 0.5mm isotropic voxels, namely (a) MCALT (Schwarz et al., 2017), (b) ICBM2009b (Fonov et al., 2009 , 2011), and (c) Colin27 (Aubert-Broche et al., 2006 ; Holmes et al., 1998), (d) a study speciﬁc (SS) template with 0.5 ×0.5 ×0.5mm 3 voxel-size con-structed by applying Steps 1 through 3 (Sections 2.3.1 –2.3.3) to Dataset 2, (e) HCP-1200 with 0.7mm isotropic voxels (Glasser et al., 2013), ﬁve templates with 1mm isotropic voxels, namely (f) MIITRA_1mm (Ridwan et al., 2021), (g) ICBM2009c (Fonov et al., 2009 , 2011), (h) OASIS (Avants and Tustison, 2018), (i) SRI24 (Rohlﬁng et al., 2010), (j) UNC-Adult (https://www.nitrc.org/projects/unc_brain_atlas/), and (k) IXI-ANTs with 1.2 ×0.9375 ×0.9375mm 3 voxel-size (Avants and Tustison, 2018)., Name Number of persons considered in template construction Age range (years) Voxel-size MIITRA_0.5mm 222 65–95 0.5 ×0.5 ×0.5mm 3 MIITRA_1mm (Ridwan et al., 2021) 222 65–95 1 ×1 ×1mm 3 SS 222 65–95 0.5 ×0.5 ×0.5mm 3 MCALT (version 1.4) (Schwarz et al., 2017) 202 30–92 0.5 ×0.5 ×0.5mm 3 ICBM2009b (nonlinear asymmetric) (Fonov et al., 2009 , 2011) 152 18–44 0.5 ×0.5 ×0.5mm 3 Colin27 (version 2008) (Aubert-Broche et al., 2006 , Holmes et al., 1998) 1(27 scans) 33 0.5 ×0.5 ×0.5mm 3 HCP-1200 (S1200 group average) (Glasser et al., 2013) 1113 22–35 0.7 ×0.7 ×0.7mm 3 ICBM2009c (nonlinear asymmetric) (Fonov et al., 2009 , 2011) 152 18–44 1 ×1 ×1mm 3 OASIS (version 2) (Avants and Tustison, 2018) 30 18–90 1 ×1 ×1mm 3 SRI24 (version 2.0) (Rohlﬁng et al., 2010) 24 19–84 1 ×1 ×1mm 3 UNC-Adult (version 2) https://www.nitrc.org/projects/unc_brain_atlas/50 20–50 1 ×1 ×1mm 3 IXI-ANTs (version 2) (Avants and Tustison, 2018) 560 20–90 1.2 ×0.94 ×0.94mm 3 Fig.",1,0,0
10.1016/j.neuroimage.2021.118869,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org/).,0,0,1
10.1016/j.neuroimage.2021.118869,nitrc.org/projects/miitra,"2) and is available for download at www.nitrc.org/projects/miitra., MIITRA_0.5mm is available for download at www.nitrc.org/projects/miitra., Discussion The present work constructed a 0.5mm isotropic resolution standard-ized T1w template of the older adult brain, termed MIITRA_0.5mm, using principles of super resolution (available for download at www.nitrc.org/projects/miitra)., The MIITRA_0.5mm template is available for download at www.nitrc.org/projects/miitra., MIITRA_0.5mm is available for download at www.nitrc.org/projects/miitra.",1,0,0
10.1016/j.neuroimage.2021.118869,radc.rush.edu,"Data and template availability The data used in this work can be assessed by submitting a re-quest to www.radc.rush.edu., Data and template availability statement The data used in this work can be assessed by submitting a re-quest to www.radc.rush.edu.",0,1,0
10.1016/j.neuroimage.2021.118869,buildtemplateparallel.sh,"A shape update transformation (Avants, and Gee, 2004) was also generated from all resulting aﬃne transforma-tions (shape update was conducted using the built-in algorithm of the ANTs tool buildtemplateparallel.sh).",1,0,0
10.1016/j.neuroimage.2022.119297,dbm.neuro.uni-jena.de/cat12,"The GMV was calculated by using Statis-tical Parametric Mapping (SPM12, https://www.ﬁl.ion.ucl.ac.uk/spm/) (Ashburner, 2012) and the Computational Anatomy Toolbox (CAT12, http://dbm.neuro.uni-jena.de/cat12/) (Iglesias et al., 2015).",1,0,0
10.1016/j.neuroimage.2022.119297,github.com/dezhengtian/dered-harmonization,The source code and trained models are available on GitHub (https://github.com/DezhengTian/DeRed-Harmonization).,1,0,0
10.1016/j.neuroimage.2022.119297,bicr-resource.atr.jp/srpbsts,"Materials and T1 data processing To minimize sampling bias across sites, we trained our harmoniza-tion framework using a traveling subject dataset from the DecNef Project Brain Data Repository (https://bicr-resource.atr.jp/srpbsts/), which was gathered by the Strategic Research Program for the Promotion of Brain Science (SRPBS) (Tanaka et al., 2021 ; Yamashita et al., 2019)., Data and code availability All MRI data used in this study are publicly available to anyone agreeing to the Open Access Data Use Terms at the DecNef Project Brain Data Repository website (https://bicr-resource.atr.jp/srpbsts/).",0,1,0
10.1016/j.neuroimage.2022.119297,ﬁl.ion.ucl.ac.uk/spm,"The GMV was calculated by using Statis-tical Parametric Mapping (SPM12, https://www.ﬁl.ion.ucl.ac.uk/spm/) (Ashburner, 2012) and the Computational Anatomy Toolbox (CAT12, http://dbm.neuro.uni-jena.de/cat12/) (Iglesias et al., 2015).",1,0,0
10.1016/j.neuroimage.2022.119176,r-project.org,"URL https://www.R-project.org/) with the tidyverse package was used for all statistical analyses (Wickham et al., 2019).",1,0,0
10.1016/j.neuroimage.2021.118838,2.2.1.1,fMRI analysis 2.2.1.1.,0,0,1
10.1016/j.neuroimage.2021.118838,ftp://imaging.wustl.edu/pub/raichlab/4dfp_tools,"Functional data pre-processing -eLABE Data were processed through a standard toddler EPI (BOLD) preprocessing pipeline using the 4dfp tool suite (ftp://imaging.wustl.edu/pub/raichlab/4dfp_tools/; Shulman et al., 2010) to remove non-neuronal variance.",1,0,0
10.1016/j.neuroimage.2022.119757,github.com/sccn/labstreaminglayer,"Stimulus triggers were generated with the software Lab Stream-ing Layer (https://github.com/sccn/labstreaminglayer), which was also used for collecting and synchronizing the other streams of data (EEG, be-havioural data).",1,0,0
10.1016/j.neuroimage.2021.118802,github.com/daducci/commit,"For interested readers, we provide more reason-ing for this claim in the Discussion section “Robust modeling or outlier replacement ”. 1 https://github.com/daducci/COMMIT. 2 V., Software code used in the paper is distributed freely via Github repository https://github.com/daducci/COMMIT.",1,0,0
10.1016/j.neuroimage.2022.119523,github.com/abhogal-lab/seevr,Data and code availability The analysis tools used to generate the results presented in this manuscript are freely available via the open-source seeVR toolbox (https://github.com/abhogal-lab/seeVR).,1,0,0
10.1016/j.neuroimage.2022.118875,github.com/zewangnew/bentbx,"Code is available from https://github.com/zewangnew/BENtbx., The brain entropy mapping tool is freely available from our website https://cfn.upenn.edu/zewang/software.html or https://github.com/zewangnew/BENtbx.",1,0,0
10.1016/j.neuroimage.2022.118875,cfn.upenn.edu/zewang/software.html,The brain entropy mapping tool is freely available from our website https://cfn.upenn.edu/zewang/software.html or https://github.com/zewangnew/BENtbx.,1,0,0
10.1016/j.neuroimage.2022.119216,optics.martinos.org/ﬂexnirs/for,"The hardware (circuit schematic and 3D drawing) and the software ﬁles are available on https://optics.martinos.org/ﬂexnirs/for non-commercial use., The hardware (circuit schematic and 3D drawing) and the software ﬁles are available on https://optics.martinos.org/ﬂexnirs/for non-commercial use.",1,0,0
10.1016/j.neuroimage.2022.119412,mgh.harvard.edu,mgh.harvard.edu) segmented MRIs from 40 healthy elderly subjects spatially aligned in MNI standard space.,0,0,1
10.1016/j.neuroimage.2022.119412,github.com/caai/brainpetnr,"The source code of the artiﬁcial neural network, from which the pre-sented models are derived, preprocessing steps for model input, making inferences, and metrics calculation are available at https://github.com/CAAI/brainPETNR (Daveau, 2022)., The code and models for inference are available at https://github.com/CAAI/brainPETNR.",1,0,0
10.1016/j.neuroimage.2022.119166,ukbiobank.ac.uk,"Data and code availability statement Data used in this work were obtained from the publicly avail-able databases: UK Biobank (http://www.ukbiobank.ac.uk/) database, WU-Minn Human Connectome Project (HCP) (http://www.human connectomeproject.org/data/hcp-project/) database and Parkinson Progression Marker Initiative (PPMI) (https://www.ppmi-info.org/) database.",0,1,0
10.1016/j.neuroimage.2022.119166,connectomeproject.org/data/hcp-project,"Data and code availability statement Data used in this work were obtained from the publicly avail-able databases: UK Biobank (http://www.ukbiobank.ac.uk/) database, WU-Minn Human Connectome Project (HCP) (http://www.human connectomeproject.org/data/hcp-project/) database and Parkinson Progression Marker Initiative (PPMI) (https://www.ppmi-info.org/) database.",0,1,0
10.1016/j.neuroimage.2022.119166,ppmi-info.org,"Data and code availability statement Data used in this work were obtained from the publicly avail-able databases: UK Biobank (http://www.ukbiobank.ac.uk/) database, WU-Minn Human Connectome Project (HCP) (http://www.human connectomeproject.org/data/hcp-project/) database and Parkinson Progression Marker Initiative (PPMI) (https://www.ppmi-info.org/) database.",0,1,0
10.1016/j.neuroimage.2022.119682,3.1.2.1,"We preprocessed the pre-recorded EEG data to match the data as it would be processed in real-time (de-trending, re-referencing to global average; see as well chapter 3.1.2.1 Simulation of real-time detection of state transitions)., Procedure 3.1.2.1.",0,1,0
10.1016/j.neuroimage.2022.119682,edfplus.info,"Oﬄine sleep scoring was performed by two independent raters, according to AASM guidelines, using the soft-ware Polyman (http://www.edfplus.info/).",1,0,0
10.1016/j.neuroimage.2022.119682,4.1.3.7,4.1.3.7.,0,0,1
10.1016/j.neuroimage.2022.119682,3.1.2.2,3.1.2.2.,0,0,1
10.1016/j.neuroimage.2022.119682,easycap.de,EEG was recorded using 64 channel BrainCaps MR caps with sintered Ag/AgCl Multitrodes by EASYCAP (www.easycap.de) and two 32 channel ampliﬁers from BrainAmp (www.brainproducts.com).,1,0,0
10.1016/j.neuroimage.2022.119682,4.1.3.6,Data analysis 4.1.3.6.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.3.13,4.1.3.13.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.3.8,NeuroImage 264 (2022) 119682 Table 2 Average number of stimulations per condition Condition Fz Fz sham C4 C4 sham CPz CPz sham average trials 42.17 42.92 40.5 39.92 41.83 42.67 SD 28.35 28.1 26.95 24.08 28.92 28.05 4.1.3.8.,0,0,1
10.1016/j.neuroimage.2022.119682,osf.io/ecvq8,"Data availability The relevant code and data for this research is available at the Open Science Framework (https://osf.io/ecvq8/)., Data availability The relevant code and data for this research is available at the Open Science Framework (https://osf.io/ecvq8/).",0,1,0
10.1016/j.neuroimage.2022.119682,4.1.3.12,4.1.3.12.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.2.4,4.1.2.4.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.3.10,4.1.3.10.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.2.3,Data collection 4.1.2.3.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.2.5,4.1.2.5.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.3.11,4.1.3.11.,0,0,1
10.1016/j.neuroimage.2022.119682,4.1.3.9,4.1.3.9.,0,0,1
10.1016/j.neuroimage.2022.119682,brainproducts.com,"EEG was recorded using 64 channel BrainCaps MR caps with sintered Ag/AgCl Multitrodes by EASYCAP (www.easycap.de) and two 32 channel ampliﬁers from BrainAmp (www.brainproducts.com)., The software BrainVision Recorder (www.brainproducts.com) was used to record the EEG data.",1,0,0
10.1016/j.neuroimage.2021.118852,enigma.usc.edu/protocols/imaging-protocols,"We followed the ENIGMA protocol for quality assurance, including performing visual checks on all cortical segmentations (http://enigma.usc.edu/protocols/imaging-protocols) and checking for motion among other artifacts.",0,0,1
10.1016/j.neuroimage.2021.118852,71.96.28.21,Measure N M SD Range Amygdala volume T1 171 1.05.13.11 to 1.40 Amygdala volume T2 119 1.02.23.19 to 1.40 Amygdala volume T3 71.96.28.21 to 1.32 SC connectivity T1 172.51.16.15 to.98 SC connectivity T2 118.53.16.15 to.89 SC connectivity T3 64.48.17.12 to.81 CC connectivity T1 172.095.076 -.048 to.35 CC connectivity T2 118.11.099 -.021 to.43 CC connectivity T3 64.11.098 -.026 to.42 SC-CC connectivity T1 172.094.086 -.13 to.37 SC-CC connectivity T2 118.092.12 -.25 to.42 SC-CC connectivity T3 64.048.13 -.43 to.36 “N ”= the number of participants who had adequate data for a given variable and a given study time point.,0,0,1
10.1016/j.neuroimage.2021.118852,172.51.16.15,Measure N M SD Range Amygdala volume T1 171 1.05.13.11 to 1.40 Amygdala volume T2 119 1.02.23.19 to 1.40 Amygdala volume T3 71.96.28.21 to 1.32 SC connectivity T1 172.51.16.15 to.98 SC connectivity T2 118.53.16.15 to.89 SC connectivity T3 64.48.17.12 to.81 CC connectivity T1 172.095.076 -.048 to.35 CC connectivity T2 118.11.099 -.021 to.43 CC connectivity T3 64.11.098 -.026 to.42 SC-CC connectivity T1 172.094.086 -.13 to.37 SC-CC connectivity T2 118.092.12 -.25 to.42 SC-CC connectivity T3 64.048.13 -.43 to.36 “N ”= the number of participants who had adequate data for a given variable and a given study time point.,0,0,1
10.1016/j.neuroimage.2021.118852,surfer.nmr.mgh.harvard.edu,The T1-weighted structural brain images of all participants were processed using Freesurfer software version 5.3 (http://surfer.nmr.mgh.harvard.edu).,1,0,0
10.1016/j.neuroimage.2021.118852,mni.mcgill.ca,"Data were warped into Montreal Neurological Institute (MNI) space (http://www.mni.mcgill.ca), and subsequently rewarped to a study-speciﬁc template due to the age range of the participants (Agcaoglu et al., 2020 , 2019).",0,1,0
10.1016/j.neuroimage.2021.118852,118.53.16.15,Measure N M SD Range Amygdala volume T1 171 1.05.13.11 to 1.40 Amygdala volume T2 119 1.02.23.19 to 1.40 Amygdala volume T3 71.96.28.21 to 1.32 SC connectivity T1 172.51.16.15 to.98 SC connectivity T2 118.53.16.15 to.89 SC connectivity T3 64.48.17.12 to.81 CC connectivity T1 172.095.076 -.048 to.35 CC connectivity T2 118.11.099 -.021 to.43 CC connectivity T3 64.11.098 -.026 to.42 SC-CC connectivity T1 172.094.086 -.13 to.37 SC-CC connectivity T2 118.092.12 -.25 to.42 SC-CC connectivity T3 64.048.13 -.43 to.36 “N ”= the number of participants who had adequate data for a given variable and a given study time point.,0,0,1
10.1016/j.neuroimage.2021.118852,64.48.17.12,Measure N M SD Range Amygdala volume T1 171 1.05.13.11 to 1.40 Amygdala volume T2 119 1.02.23.19 to 1.40 Amygdala volume T3 71.96.28.21 to 1.32 SC connectivity T1 172.51.16.15 to.98 SC connectivity T2 118.53.16.15 to.89 SC connectivity T3 64.48.17.12 to.81 CC connectivity T1 172.095.076 -.048 to.35 CC connectivity T2 118.11.099 -.021 to.43 CC connectivity T3 64.11.098 -.026 to.42 SC-CC connectivity T1 172.094.086 -.13 to.37 SC-CC connectivity T2 118.092.12 -.25 to.42 SC-CC connectivity T3 64.048.13 -.43 to.36 “N ”= the number of participants who had adequate data for a given variable and a given study time point.,0,0,1
10.1016/j.neuroimage.2022.119275,github.com/jadecci/partialcorr_factors,The code for the partial correlations used in this study can be found at: https://github.com/jadecci/partialcorr_factors.,1,0,0
10.1016/j.neuroimage.2022.119275,github.com/yalemrrc/cpm,"One famous approach is called the connectome-based predictive modeling (CPM) approach, developed by (Finn et al., 2015), the term CPM established by (Rosenberg et al., 2016), its protocol published by (Shen et al., 2017), codes deposited at https://github.com/YaleMRRC/CPM.",1,0,0
10.1016/j.neuroimage.2022.119569,ida.loni.usc.edu,"The GSP imaging and phenotype data were downloaded from the LONI Imaging Data Archive (https://ida.loni.usc.edu) after accepting the GSP Data Use Terms and GSP Restricted Data Use Terms., The GSP imaging and phenotype data were downloaded from the LONI Imaging Data Archive (https://ida.loni.usc.edu) after accepting the GSP Data Use Terms and GSP Restricted Data Use Terms.",0,1,0
10.1016/j.neuroimage.2022.119569,nda.nih.gov,"The HCP-A imaging and phenotype data were downloaded from the NIMH Data Archive (NDA; https://nda.nih.gov), after applying 5 J., The HCP-A imaging and phenotype data were downloaded from the NIMH Data Archive (NDA; https://nda.nih.gov), after applying for the Data Use Certiﬁcation.",0,1,0
10.1016/j.neuroimage.2022.119569,coins.trendscenter.org,The eNKI-RS imaging data were downloaded from the COINS data exchange (https://coins.trendscenter.org).,0,1,0
10.1016/j.neuroimage.2022.119569,db.humanconnectome.org,"The unrestricted and restricted phenotype data were downloaded from the ConnectomeDB (https://db.humanconnectome.org) after accepting the Open Access Data User Terms and Restricted Access Data Use Terms, respectively., The un-restricted and restricted phenotype data were downloaded from the ConnectomeDB (https://db.humanconnectome.org) after accepting the Open Access Data User Terms and Restricted Access Data Use Terms, respectively.",0,1,0
10.1016/j.neuroimage.2022.119569,github.com/inm7/cbpp,"All codes are publicly available at https://github.com/inm7/cbpp., All codes are publicly available at https://github.com/inm7/cbpp.",1,0,0
10.1016/j.neuroimage.2022.119569,registry.opendata.aws/hcp-openaccess,"The HCP-YA imaging data were accessed via the public Data-Lad dataset provided at https://github.com/datalad-datasets/human-connectome-project-openaccess (2e2a8a70-3eaa-11ea-a9a5-b4969157768c@a33e528) which interfaces the HCP Open Access dataset (https://registry.opendata.aws/hcp-openaccess) on AWS S3., Data and code availability The HCP-YA imaging data were downloaded from the HCP Open Access dataset (https://registry.opendata.aws/hcp-openaccess).",0,1,0
10.1016/j.neuroimage.2022.119569,github.com/datalad-datasets/human-connectome-project-openaccess,The HCP-YA imaging data were accessed via the public Data-Lad dataset provided at https://github.com/datalad-datasets/human-connectome-project-openaccess (2e2a8a70-3eaa-11ea-a9a5-b4969157768c@a33e528) which interfaces the HCP Open Access dataset (https://registry.opendata.aws/hcp-openaccess) on AWS S3.,0,1,0
10.1016/j.neuroimage.2022.119554,nisox.org/software/snpm13,"For the second level within group and between group compar-isons, we performed non-parametric permutation procedures, not as-suming a particular distribution (Statistical Non-Parametric Mapping SnPM13.1.08, http://nisox.org/Software/SnPM13/, 10.000 permuta-tions)(Nichols and Holmes, 2002).",1,0,0
10.1016/j.neuroimage.2022.119554,neurosipe.nl,neurosipe.nl.,0,0,1
10.1016/j.neuroimage.2022.119554,ﬁl.ion.ucl.ac.uk/spm,"fMRI and EMG pre-processing The fMRI data was analyzed with the statistical parametric map-ping software, SPM12 (Wellcome Department of Cognitive Neurology, UCL, London, UK; https://www.ﬁl.ion.ucl.ac.uk/spm/) (Friston et al., 1995) including standardized pre-processing steps containing realign-ment, slice-timing, coregistration and normalization to MNI space.",1,0,0
10.1016/j.neuroimage.2022.119730,dpz.eu/en/home.html,Data and code availability statement The information about the non-human primate research can be found on the website of the German Primate Center (https://www.dpz.eu/en/home.html).,0,0,1
10.1016/j.neuroimage.2022.119739,csie.ntu.edu.tw/∼cjlin/libsvm,We used a leave-one-out cross-validation approach via libsvmtrain function and libsvmpredict function implemented in libsvm program (http://www.csie.ntu.edu.tw/∼cjlin/libsvm).,1,0,0
10.1016/j.neuroimage.2022.119356,github.com/ekaden/smt,"The intra-cellular volume fraction map was obtained using the Spherical Mean Technique method (Kaden et al., 2015) available at https://github.com/ekaden/smt (ﬁtm-cmicro command).",1,0,0
10.1016/j.neuroimage.2022.119462,representational-dynamics.herokuapp.com,The interactive web application accompanying Fig. 2 is published at https://doi.org/10.5281/zenodo.6579997 and is hosted at https://representational-dynamics.herokuapp.com/.,0,0,1
10.1016/j.neuroimage.2022.119462,userpage.fu-berlin.de/rmcichy/fusion_project_page/main.html,"Data and code availability The data used in Results Section 4 is from a previously published work (Cichy et al., 2016); it is publicly available for download at http://userpage.fu-berlin.de/rmcichy/fusion_project_page/main.html.",0,1,0
10.1016/j.neuroimage.2022.119462,representational-dynamics.herokuapp.com,"In order to fur-ther illustrate the representational dynamics of instantaneous signal decoding, we created Representational Dynamics Simulator , a web ap-plication analogous to Fig. 2 , where the user can interactively change the parameters of the evoked spectrum and see the resulting informa-tion content (Van Es et al, 2022 ; hosted at https://representational-dynamics.herokuapp.com)., For illustrative purposes, we created a web application where these parameters can be changed interactively (Van Es et al., 2022 ; hosted at https://representational-dynamics.herokuapp.com)., The interactive web application accompanying Fig. 2 is published at https://doi.org/10.5281/zenodo.6579997 and is hosted at https://representational-dynamics.herokuapp.com/.",0,0,1
10.1016/j.neuroimage.2022.119462,representational-dynamics.herokuapp.com,"In order to fur-ther illustrate the representational dynamics of instantaneous signal decoding, we created Representational Dynamics Simulator , a web ap-plication analogous to Fig. 2 , where the user can interactively change the parameters of the evoked spectrum and see the resulting informa-tion content (Van Es et al, 2022 ; hosted at https://representational-dynamics.herokuapp.com)., For illustrative purposes, we created a web application where these parameters can be changed interactively (Van Es et al., 2022 ; hosted at https://representational-dynamics.herokuapp.com)., The interactive web application accompanying Fig. 2 is published at https://doi.org/10.5281/zenodo.6579997 and is hosted at https://representational-dynamics.herokuapp.com/.",0,0,1
10.1016/j.neuroimage.2022.119462,github.com/ohba-analysis/representationaldynamicsmodelling,The code to perform all the analysis and example simulations pub-lished in this paper is publicaly available at https://github.com/OHBA-analysis/RepresentationalDynamicsModelling.,1,0,0
10.1016/j.neuroimage.2022.119165,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"In this demonstration and the accompanying tutorial scripts, the procedure was done with functions from SPM12 (www.ﬁl.ion.ucl.ac.uk/spm/software/spm12) through the FieldTrip toolbox for MEG/EEG analysis.",1,0,0
10.1016/j.neuroimage.2022.119165,zenodo.org/record/5053234,"The data is available at: https://zenodo.org/record/5053234., The data is publicly available under the CC-BY 4.0 license at: https://zenodo.org/record/5053234.",0,1,0
10.1016/j.neuroimage.2022.119165,github.com/mcvinding/warpimg/tree/main/benchmarking,"Data and code availability statement Tutorials scripts demonstrating the pipeline is available at https://github.com/mcvinding/warpimg , and analysis scripts for all analysis presented in the paper are available at https://github.com/mcvinding/warpimg/tree/main/benchmarking The data was acquired according to local ethical regulations at Karolinska Institutet and is used for this study with the participant’s consent.",1,0,0
10.1016/j.neuroimage.2022.119165,github.com/mcvinding/warpimg,"Tutorial scripts are available at https://github.com/mcvinding/warpimg., Data and code availability statement Tutorials scripts demonstrating the pipeline is available at https://github.com/mcvinding/warpimg , and analysis scripts for all analysis presented in the paper are available at https://github.com/mcvinding/warpimg/tree/main/benchmarking The data was acquired according to local ethical regulations at Karolinska Institutet and is used for this study with the participant’s consent.",1,0,0
10.1016/j.neuroimage.2022.119248,github.co,The code will be publicly ac-cessible at: https://github.co m/z czam/rRAKI.,1,0,0
10.1016/j.neuroimage.2022.119505,adni.loni.usc.edu,Dataset Data used in the preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) and AIBL (aibl.csiro.au).,0,1,0
10.1016/j.neuroimage.2022.119505,adni-info.org,"For up-to-date in-formation, see www.adni-info.org.",0,0,1
10.1016/j.neuroimage.2022.119505,aibl.csiro.au,loni.usc.edu and https://aibl.csiro.au.,0,0,1
10.1016/j.neuroimage.2022.119505,github.com/ai-med/daft,The implementation of all models is available at https://github.com/ai-med/DAFT and Table 2 reports the number of parameters for each of the diﬀerent models.,1,0,0
10.1016/j.neuroimage.2022.119505,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119505,loni.usc.edu,"Dataset Data used in the preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) and AIBL (aibl.csiro.au)., loni.usc.edu and https://aibl.csiro.au.",0,1,0
10.1016/j.neuroimage.2022.119505,aibl.csiro.au,"Dataset Data used in the preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) and AIBL (aibl.csiro.au)., loni.usc.edu and https://aibl.csiro.au.",0,1,0
10.1016/j.neuroimage.2022.119593,github.com/espenhgn/csdplotter,"We computed the CSD signal using the spline-iCSD method (Pettersen et al., 2006) as implemented in the CSDplotter toolbox (https://github.com/espenhgn/CSDplotter) with custom MATLAB (R2021b, The MathWorks) scripts.",1,0,0
10.1016/j.neuroimage.2022.118925,nitrc.org/projects/brainstemnavig,"Deﬁnition of seed and target regions For the subsequent analyses, a probabilistic atlas of brainstem nuclei in MNI space developed by our team (Brainstem Navigator toolkit v0.9, https://www.nitrc.org/projects/brainstemnavig/) and FreeSurfer’s cor-tical and subcortical parcellation (Destrieux et al., 2010) were used to deﬁne seed and target regions., We deﬁned as target regions the 27 seed regions, the 164 elements of the FreeSurfer cortical and subcortical parcellation, and 18 (31 counting bilateral nuclei) probabilistic atlas labels (binarized by setting a thresh-old at 35%) (Brainstem Navigator toolkit v0.9 developed by this group (https://www.nitrc.org/projects/brainstemnavig/) involved in wakeful arousal and motor function (the functional connectome of the latter was computed in (Bianciardi et al., 2016 ; Singh et al., 2021a)).",1,0,0
10.1016/j.neuroimage.2022.119193,tensortoolbox.org,"The ALS algorithm is provided open access from the tensor toolbox (https://www.tensortoolbox.org)., The alternat-ing least-squares (ALS) algorithm can be free accessed from ten-sor toolbox (https://www.tensortoolbox.org).",1,0,0
10.1016/j.neuroimage.2022.119193,humanconnectome.org,Data and Code Availability Statement The data used in the manuscript are from the human con-nectome project (HCP; www.humanconnectome.org).,0,1,0
10.1016/j.neuroimage.2022.119193,github.com/ghu-dut/tensor_spectral_clustering,The tensor spectral clustering software is available at https://github.com/GHu-DUT/Tensor_Spectral_Clustering.,1,0,0
10.1016/j.neuroimage.2022.119193,mlsp.umbc.edu/simulated_fmri_data.html,(http://mlsp.umbc.edu/simulated_fmri_data.html) are shown in Fig.,0,1,0
10.1016/j.neuroimage.2022.119193,github.com/ghu-dut/tensor-components-analysis-for-naturalistic-stimuli-fmri,"The analysis code for the tensor compo-nent analysis framework is available at https://github.com/GHu-DUT/Tensor-components-analysis-for-naturalistic-stimuli-fMRI., The analysis code of tensor components analysis framework is available at https://github.com/GHu-DUT/Tensor-components-analysis-for-naturalistic-stimuli-fMRI.",1,0,0
10.1016/j.neuroimage.2022.119193,mialab.mrn.org/software,"The simulated spatial maps and time courses for 29 ICA components (SimBT (Erhardt et al., 2012) http://mialab.mrn.org/software) and time courses 4 G.",1,0,0
10.1016/j.neuroimage.2022.119766,3.1.9.7,"To reveal the practical signiﬁcance of ZT-dependent omission responses, the four indexes were used to calculate cohen’s d with G ∗ power 3.1.9.7 software (Heinrich-Heine-Universität Düsseldorf, Germany).",1,0,0
10.1016/j.neuroimage.2022.119047,neuosc.com/ﬂux,"Below we describe the in-dividual steps which are fully detailed on the FLUX website (https://www.neuosc.com/ﬂux/)., We are also considering complementing the tuto-rials (https://www.neuosc.com/ﬂux) with JSON ﬁles deﬁning the essen-tial parameters for a given analysis.",0,0,1
10.1016/j.neuroimage.2022.119047,neuosc.com/ﬂux,"These steps are detailed with documented scripts applied to a dataset in http://www.neuosc.com/ﬂux/., The section ‘artifact Anno-tations’ on the FLUX website (http://www.neuosc.com/ﬂux) provides concrete examples of some of these artifacts and how they appear in the data.",0,0,1
10.1016/j.neuroimage.2022.119047,neuosc.com/ﬂux,"The FLUX pipeline is implemented as speciﬁc code with documented tu-torials and an example data set (https://neuosc.com/ﬂux/)., com/Neuronal-Oscillations/FLUX/) organized in an open access website (https://neuosc.com/ﬂux/) together with an example dataset., The FLUX pipelines can be accessed via a website (https://neuosc.com/ﬂux/) and the scripts themselves are maintained on a GitHub repository (https://github.com/Neuronal-Oscillations/FLUX/).",0,1,0
10.1016/j.neuroimage.2022.119047,neuosc.com,"A single data set is used in the FLUX pipeline which is publicly available via https://www.neuosc.com., Below we describe the in-dividual steps which are fully detailed on the FLUX website (https://www.neuosc.com/ﬂux/)., We are also considering complementing the tuto-rials (https://www.neuosc.com/ﬂux) with JSON ﬁles deﬁning the essen-tial parameters for a given analysis.",0,0,1
10.1016/j.neuroimage.2022.119047,neuosc.com/ﬂux,These steps are detailed with documented scripts applied to a dataset in http://www.neuosc.com/ﬂux/.,1,0,0
10.1016/j.neuroimage.2022.119047,neuosc.com/ﬂux,Below we describe the in-dividual steps which are fully detailed on the FLUX website (https://www.neuosc.com/ﬂux/).,0,0,1
10.1016/j.neuroimage.2022.119047,neuosc.com/ﬂux,"The FLUX pipeline is implemented as speciﬁc code with documented tu-torials and an example data set (https://neuosc.com/ﬂux/)., com/Neuronal-Oscillations/FLUX/) organized in an open access website (https://neuosc.com/ﬂux/) together with an example dataset., The FLUX pipelines can be accessed via a website (https://neuosc.com/ﬂux/) and the scripts themselves are maintained on a GitHub repository (https://github.com/Neuronal-Oscillations/FLUX/)., Data code statement The data and code are available from https://neuosc.com/ﬂux.",0,1,0
10.1016/j.neuroimage.2022.119047,surfer.nmr.mgh.harvard.edu/for,see https://surfer.nmr.mgh.harvard.edu/for details).,0,0,1
10.1016/j.neuroimage.2022.119047,github.com/neuronal-oscillations/flux,The FLUX pipelines can be accessed via a website (https://neuosc.com/ﬂux/) and the scripts themselves are maintained on a GitHub repository (https://github.com/Neuronal-Oscillations/FLUX/).,1,0,0
10.1016/j.neuroimage.2022.119030,marmosetbrainconnectome.org/download.html,"(B) The data download page (https://www.marmosetbrainconnectome.org/download.html) allows the user to download all raw (BIDS standard formated) (Gorgolewski et al., 2016) and pre-processed data., Directing to https://www.marmosetbrainconnectome.org/download.html allows for download of the “raw ” structural and functional images (3D Neu-roimaging Informatics Technology Initiative (NIfTI) format) contribut-ing to the FC maps shown in the resource –for convenience, these data are in a standard format (BIDS) (Gorgolewski et al., 2016)., Silva: Conceptualization, Methodology, Writing, Supervision, Funding Acquisition Schaeﬀer data availability statement All raw and preprocessed data are openly available for download at: https://www.marmosetbrainconnectome.org/download.html All code used for processing data is openly available at: https://gitlab.com/cfmm/marmoset-connectivity All code for the online viewer is available at: https://gitlab.com/cfmm/marmoset Acknowledgments Support was provided by the Canadian Institutes of Health Research (FRN 148365 , FRN 353372), a Brain Canada Platform Support Grant and the Canada First Research Excellence Fund to BrainsCAN.",0,1,0
10.1016/j.neuroimage.2022.119030,gitlab.com/cfmm/marmoset,"The development of the Marmoset Functional Connectivity Resource is described in full detail at https://gitlab.com/cfmm/marmoset., Users can also download all code used to generate the functional connectivity maps from https://gitlab.com/cfmm/marmoset-connectivity., (D) in method 2, the user employs the supplied code (downloaded from https://gitlab.com/cfmm/marmoset-connectivity) to transform the FC map to their native animals’ anatomical MRI space., Silva: Conceptualization, Methodology, Writing, Supervision, Funding Acquisition Schaeﬀer data availability statement All raw and preprocessed data are openly available for download at: https://www.marmosetbrainconnectome.org/download.html All code used for processing data is openly available at: https://gitlab.com/cfmm/marmoset-connectivity All code for the online viewer is available at: https://gitlab.com/cfmm/marmoset Acknowledgments Support was provided by the Canadian Institutes of Health Research (FRN 148365 , FRN 353372), a Brain Canada Platform Support Grant and the Canada First Research Excellence Fund to BrainsCAN.",1,0,0
10.1016/j.neuroimage.2022.119030,gitlab.com/cfmm/marmoset-connectivity,"Users can also download all code used to generate the functional connectivity maps from https://gitlab.com/cfmm/marmoset-connectivity., (D) in method 2, the user employs the supplied code (downloaded from https://gitlab.com/cfmm/marmoset-connectivity) to transform the FC map to their native animals’ anatomical MRI space., Silva: Conceptualization, Methodology, Writing, Supervision, Funding Acquisition Schaeﬀer data availability statement All raw and preprocessed data are openly available for download at: https://www.marmosetbrainconnectome.org/download.html All code used for processing data is openly available at: https://gitlab.com/cfmm/marmoset-connectivity All code for the online viewer is available at: https://gitlab.com/cfmm/marmoset Acknowledgments Support was provided by the Canadian Institutes of Health Research (FRN 148365 , FRN 353372), a Brain Canada Platform Support Grant and the Canada First Research Excellence Fund to BrainsCAN.",1,0,0
10.1016/j.neuroimage.2022.119030,rii-mango.github.io/papaya,"The resource makes use of the Papaya viewer (https://rii-mango.github.io/Papaya/), with several additional features (illustrated in Fig. 1 C & D), including (1) calculation of surface over-lay maps on-demand based on the threshold chosen in volume space, (2) the ability to display atlas borders in surface space, (3) support for rotating the underlying volume, overlaying functional connectiv-ity map, and atlas boundaries together –s u c h obliquing of the images can be of utility for presurgical planning, and (4) the ability to choose between group-and subject-level topologies.",1,0,0
10.1016/j.neuroimage.2022.119030,marmosetbrain.org,"Explicitly, we focused on a tracer map from an area 46 injection (left; CJ801-DY; marmosetbrain.org for notes on these injections)., To demonstrate the additional information oﬀered by our resource, we systematically plotted connectivity across (within) area TE3 for com-parison with available tracer injections within that region (left; CJ180-CTBr and CJ180-DY; marmosetbrain.org for notes on these injections) (Majka et al., 2020)., As shown in Fig. 9 , we systematically plotted connectivity across (within) area TE3 and compared available tracer injections within that region (left; CJ180-CTBr and CJ180-DY; marmosetbrain.org for notes on these injections) (Majka et al., 2020)., Green labeled ROIs indicate FC data, whereas purple labeled ROIs show where tracer data is publicly available within area TE3 from marmosetbrain.org., Tracer maps (B & E) were downloaded from marmosetbrain.org , and FC maps (A through H) were generated from marmosetbrainconnectome.org. 10 D.J., shows an example of how our brain-wide functional connec-tivity data can complement existing resources (e.g., marmosetbrain.org) (Majka et al., 2020), demonstrating a gradient of connectivity between cortical tracer injection sites.",0,0,1
10.1016/j.neuroimage.2022.119030,marmosetbrainconnectome.org,"To accelerate such progress, we present the Marmoset Functional Brain Connectivity Resource (marmosetbrainconnectome.org), currently consisting of over 70 h of resting-state fMRI (RS-fMRI) data acquired at 500 μm isotropic resolution from 31 fully awake marmosets in a common stereotactic space., To promote progress in understanding the functional organization of the marmoset brain, we present a resource that allows for online viewing and download of three-dimensional functional connectivity (FC) maps from over 70 h of RS-fMRI collected at ultra-high ﬁeld from 31 fully awake adult marmosets: marmosetbrainconnectome.org., A resampled ver-sion of this atlas (at 100 μm) allows for additional anatomical detail over the in vivo template but will still load suﬃciently fast as an under-lay image on marmosetbrainconnectome.org., Features of the web portal: marmosetbrainconnectome.org., (B) The data download page (https://www.marmosetbrainconnectome.org/download.html) allows the user to download all raw (BIDS standard formated) (Gorgolewski et al., 2016) and pre-processed data., Resource The Marmoset Functional Connectivity Resource is publicly accessi-ble at marmosetbrainconnectome.org., Directing to https://www.marmosetbrainconnectome.org/download.html allows for download of the “raw ” structural and functional images (3D Neu-roimaging Informatics Technology Initiative (NIfTI) format) contribut-ing to the FC maps shown in the resource –for convenience, these data are in a standard format (BIDS) (Gorgolewski et al., 2016)., This resource allows users to instantaneously view and use FC topologies from any gray matter voxel in the marmoset brain online (marmosetbrainconnectome.org ; Fig., Tracer maps (B & E) were downloaded from marmosetbrain.org , and FC maps (A through H) were generated from marmosetbrainconnectome.org. 10 D.J., Individual-level topologies can be loaded via the marmosetbrainconnectome.org viewer without any analysis., Silva: Conceptualization, Methodology, Writing, Supervision, Funding Acquisition Schaeﬀer data availability statement All raw and preprocessed data are openly available for download at: https://www.marmosetbrainconnectome.org/download.html All code used for processing data is openly available at: https://gitlab.com/cfmm/marmoset-connectivity All code for the online viewer is available at: https://gitlab.com/cfmm/marmoset Acknowledgments Support was provided by the Canadian Institutes of Health Research (FRN 148365 , FRN 353372), a Brain Canada Platform Support Grant and the Canada First Research Excellence Fund to BrainsCAN.",0,1,0
10.1016/j.neuroimage.2021.118821,neurovault.org/collections/9720,"We extracted fMRI time series from individual ROIs, using their principal eigenvariates: we relied on group-level anatomical maps manually generated for the Amg and the aHip (available at: https://neurovault.org/collections/9720/), and we used spherical ROIs (8 mm radius) for the mPFC.",0,0,1
10.1016/j.neuroimage.2021.118821,drugabuse.gov,"XG is supported by National Institute on Drug Abuse (R01DA043695, R21DA049243) https://www.drugabuse.gov/.",0,0,1
10.1016/j.neuroimage.2021.118821,gin.g-node.org/g-node,"Data availability Single subject behavior, datacode of the 4 models described, GLM results and associated DCM estimations are fully available in the form of a G-node repository  https://gin.g-node.org/G-Node –link provided upon acceptance.",0,1,0
10.1016/j.neuroimage.2021.118821,ﬁl.ion.ucl.ac.uk/spm,"We used standard Statistical Parametric Mapping algorithms (SPM12, Wellcome Department of Imaging Neu-roscience; www.ﬁl.ion.ucl.ac.uk/spm/) for data preprocessing, includ-ing motion realignment to the ﬁrst volume, coregistration to the par-ticipant’s anatomical scan, MNI normalization, and spatial smoothing, using an isotropic 8-mm full-width at half-maximum (FWHM) Gaussian kernel.",1,0,0
10.1016/j.neuroimage.2022.119417,adni.loni.usc.edu,"Bennett b , Konstantinos Arfanakis a , b , ∗ a Department of Biomedical Engineering, Illinois Institute of Technology, Chicago, IL USA b Rush Alzheimer’s Disease Center, Rush University Medical Center, Chicago, Illinois USA c A portion of the data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf USA a r t i c l e i n f o Keywords: Brain Template Multimodal T 1 -weighted Diﬀusion tensor imaging Aging a b s t r a c t High-quality T 1 -weighted (T 1 w) and diﬀusion tensor imaging (DTI) brain templates that are representative of the individuals under study enhance the accuracy of template-based neuroimaging investigations, and when they are also located in a common space they facilitate optimal integration of information on brain morphometry and diﬀusion characteristics., Dataset 2 consisted of T 1 w and DTI data from 202 non-demented older adults (50% male; 65–93.2 years age range; mean ± sd age = 78.3 ± 6.02 years of age; 122 with no cognitive impairment and 80 with mild cognitive impairment) participating in the Alzheimer’s Disease Neuroimaging Initiative 3 (ADNI3) (http://adni.loni.usc.edu).",0,0,1
10.1016/j.neuroimage.2022.119417,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,"A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf USA a r t i c l e i n f o Keywords: Brain Template Multimodal T 1 -weighted Diﬀusion tensor imaging Aging a b s t r a c t High-quality T 1 -weighted (T 1 w) and diﬀusion tensor imaging (DTI) brain templates that are representative of the individuals under study enhance the accuracy of template-based neuroimaging investigations, and when they are also located in a common space they facilitate optimal integration of information on brain morphometry and diﬀusion characteristics.",0,0,1
10.1016/j.neuroimage.2022.119417,adni.loni.usc.edu,"A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf USA a r t i c l e i n f o Keywords: Brain Template Multimodal T 1 -weighted Diﬀusion tensor imaging Aging a b s t r a c t High-quality T 1 -weighted (T 1 w) and diﬀusion tensor imaging (DTI) brain templates that are representative of the individuals under study enhance the accuracy of template-based neuroimaging investigations, and when they are also located in a common space they facilitate optimal integration of information on brain morphometry and diﬀusion characteristics., Dataset 2 consisted of T 1 w and DTI data from 202 non-demented older adults (50% male; 65–93.2 years age range; mean ± sd age = 78.3 ± 6.02 years of age; 122 with no cognitive impairment and 80 with mild cognitive impairment) participating in the Alzheimer’s Disease Neuroimaging Initiative 3 (ADNI3) (http://adni.loni.usc.edu).",0,0,1
10.1016/j.neuroimage.2022.119417,antsmultivariatetemplateconstruction.sh,"The second one, based on T 1 w and FA infor-mation, is available in ANTs (antsMultivariateTemplateConstruction.sh) (Avants et al., 2011) and will be referred to as MC-ANTS.",1,0,0
10.1016/j.neuroimage.2022.119417,git.fmrib.ox.ac.uk/cart/mm-template-construction,"The third one, based on T 1 w and full tensor information, utilizes the MMORF tool (run_template_construction.py) (Lange et al., 2020a ; https://git.fmrib.ox.ac.uk/cart/mm-template-construction) and will be referred to as MC-MMORF., This was probably due to the higher FA values in white matter of the MC-MMORF template, which in turn might be due to log-Euclidean averaging of diﬀusion tensors used by MC-MMORF (Lange et al., 2020b ; Roumazeilles et al., 2021 ; https://git.fmrib.ox.ac.uk/cart/mm-template-construction).",1,0,0
10.1016/j.neuroimage.2022.119417,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org/).,0,0,1
10.1016/j.neuroimage.2022.119417,nitrc.org/projects/miitra,"6) are available for download at www.nitrc.org/projects/miitra (version 1.5)., Overall, the present work brought new insights into multimodal template con-struction, generated much-needed high quality T 1 w and DTI templates of the older adult brain in a common space (available for download at www.nitrc.org/projects/miitra; version 1.5), and conducted a thorough, quantitative evaluation of available multimodal template construction methods., A script named reg_to_MIITRA_T1_and_DTI.sh that executes this iterative registration of an individual’s T1w and DTI data to the templates constructed in this work is available for download at www.nitrc.org/projects/miitra., Overall, the present work brought new insights into multimodal template construc-tion and generated much-needed high quality T 1 w and DTI templates of the older adult brain in a common space (available for download as part of the MIITRA atlas at www.nitrc.org/projects/miitra; version 1.5)., All templates are available for download at www.nitrc.org/projects/miitra (version 1.5).",1,0,0
10.1016/j.neuroimage.2022.119417,radc.rush.edu,Data and template availability statement The data used in this work can be assessed by submitting a re-quest to www.radc.rush.edu.,0,1,0
10.1016/j.neuroimage.2022.119424,github.com/nimh-sﬁm/hcp7t_fv_sleep,"Processing scripts publicly available at https://github.com/nimh-sﬁm/hcp7t_fv_sleep 3., Processing scripts publicly available at https://github.com/nimh-sﬁm/hcp7t_fv_sleep Credit author statement Javier Gonzalez-Castillo : Conceptualization, Methodology, Soft-ware, Formal Analysis, Writing – Original Draft, Visualization.",1,0,0
10.1016/j.neuroimage.2022.119424,humanconnectome.org/storage/app/media/documentation/s1200/hcp_s1200_release_reference_manual.pdf,Additional details can be found on the Reference Manual for the 1200 HCP Release available online at https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf.,0,0,1
10.1016/j.neuroimage.2022.119424,github.com/bbfrederick/rapidtide,Voxel-wise correlation analyses: cross-correlation and lag maps The RapidTide2 software (https://github.com/bbfrederick/rapidtide ; v2.0.8 + 11.gb17c48f) was used to perform cross-correlation analyses.,1,0,0
10.1016/j.neuroimage.2022.119424,biowulf.nih.gov,"Portions of this study used the high-performance computational capa-bilities of the Biowulf Linux cluster at the National Institutes of Health, Bethesda, MD (biowulf.nih.gov).",1,0,0
10.1016/j.neuroimage.2022.119024,osf.io/mds9q,"Processed data are available at https://osf.io/mds9q/., Author statement Magdalena Kachlicka: Methodology, Software, Investigation, Writ-ing —Original Draft Aeron Laﬀere: Methodology, Software, Investigation Fred Dick: Conceptualization, Methodology, Writing —Review and Edit-ing, Project administration Adam Tierney: Conceptualization, Method-ology, Software, Formal analysis, Writing —Original Draft, Project ad-ministration Processed data are available at https://osf.io/mds9q/.",0,0,1
10.1016/j.neuroimage.2022.119490,et.al,"The prewhitening process removed correlated noise and brought the regularization parameter to the correct scale, where the gradiometer and magnetometer were combined into a single estimate (Hämäläinen et.al, 2010).",1,0,0
10.1016/j.neuroimage.2022.119490,martinos.org/mne/stable/in-dex.html,Eye movement artifacts were removed using independent component analysis (MNE; martinos.org/mne/stable/in-dex.html).,1,0,0
10.1016/j.neuroimage.2022.119490,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"For the source-level analysis, the brain result of each sub-ject was projected to the Montreal Neurological Institute tem-plate with 2 mm isotropic resolution using FSL software (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/).",1,0,0
10.1016/j.neuroimage.2022.119596,github.com/andlab-um/emotion_neurophysio_is-rsa,The code used in this manuscript is available on GitHub (https://github.com/andlab-um/Emotion_Neurophysio_IS-RSA).,1,0,0
10.1016/j.neuroimage.2022.119596,labeling.ucsd.edu/tutorial/overview,"In detail, we mainly used the ICAla-bel plug-in (https://labeling.ucsd.edu/tutorial/overview) to remove the artifacts based on a threshold for rejecting the component, which is above 0.7 for both the muscle and ocular artifacts (Pion-Tonachini et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119596,naturalistic-data.org,"Acknowledgement The authors would like to thank the naturalistic data analysis tutorial (Chang et al., 2020) (http://naturalistic-data.org/) for the data analysis methods.",0,0,1
10.1016/j.neuroimage.2022.119596,github.com/andlab-um/emotion_eeg,we used an embedded Python script (https://github.com/andlab-um/Emotion_EEG) in the Unity program for sending marks to the EEG and ECG hardware for synchronization.,1,0,0
10.1016/j.neuroimage.2022.119286,human.brain-map.org,"The microarray data for this study are derived from the Allen Human Brain Atlas (http://www.human.brain-map.org), which comprises over 62000 gene probes per proﬁle col-lected in about 500 samples per hemisphere in six donor brains (M., fz-juelich.de/inm/inm-1/jugex), the gene names of the list were converted into a format of the Allen Human Brain microarray dataset (http://www.human.brain-map.org), so that the gene expres-sion of a tissue block can be accessed by the Allen Human Brain probe-ID., The Allen Human Brain microarray dataset is available at http://www.human.brain-map.org.",0,1,0
10.1016/j.neuroimage.2022.119286,kg.ebrains.eu,"The cytoarchitectonic and re-ceptorarchitectonic data are available in the Julich-Brain and accessible via Ebrains (https://kg.ebrains.eu)., Detailed information on cytoarchitectonic areas is stored in the ebrains knowledge graph (https://kg.ebrains.eu) Competing Interest Statement none declared Supplementary materials Supplementary material associated with this article can be found, in the online version, at doi: 10.1016/j.neuroimage.2022.119286.",0,1,0
10.1016/j.neuroimage.2022.119286,ebrains.eu/service/jugex,2018; publicly available https://ebrains.eu/service/jugex/).,0,0,1
10.1016/j.neuroimage.2022.119286,fz-juelich.de/inm/inm-1/jugex),"fz-juelich.de/inm/inm-1/jugex), the gene names of the list were converted into a format of the Allen Human Brain microarray dataset (http://www.human.brain-map.org), so that the gene expres-sion of a tissue block can be accessed by the Allen Human Brain probe-ID.",0,1,0
10.1016/j.neuroimage.2022.119286,jubrain.fz-juelich.de,"Julich-Brain Atlas The Julich-Brain atlas (http://www.jubrain.fz-juelich.de) is part of the HBP Human Brain Atlas (https://www.humanbrainproject., Sections with the areal borders were 3D reconstructed and transferred to the Julich-Brain Atlas (http://www.jubrain.fz-juelich.de)., The Julich-Brain Atlas is available at http://www.jubrain.fz-juelich.de.",0,0,1
10.1016/j.neuroimage.2022.119286,fz-juelich.de/inm/inm-1/jugex,Data availability The JuGEx Toolbox is available at http://www.fz-juelich.de/inm/inm-1/jugex.,1,0,0
10.1016/j.neuroimage.2022.119286,help.brain-map.org/display/humanbrain/documentation,For de-tailed information please consult the white papers (http://help.brain-map.org/display/humanbrain/Documentation).,0,0,1
10.1016/j.neuroimage.2022.119534,neurosynth.org,"To present a functional interpretation of the most important fea-tures in the prediction, a data-driven decoding analysis was conducted based on a large-scale neuroimaging database (https://neurosynth.org/; version 0.7).",0,1,0
10.1016/j.neuroimage.2022.119534,osf.io/4q87k,We provide our code for model training and training statistics at https://osf.io/4q87k/.,1,0,0
10.1016/j.neuroimage.2022.119534,ukbiobank.ac.uk,NeuroImage 262 (2022) 119534 website https://www.ukbiobank.ac.uk/.,0,0,1
10.1016/j.neuroimage.2022.119534,scikit-learn.org,"We applied MinMaxScaler from scikit-learn (version 0.22.2.post1; https://scikit-learn.org/) (Pedregosa et al., 2011), normalizing each covariate to be between zero and one.",1,0,0
10.1016/j.neuroimage.2021.118822,adni.loni.usc.edu,"Chen). 1 These authors contributed equally to this work. 2 Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf et al., 2010; Reig et al., 2009; Wonderlick et al., 2009)., ADNI data analysis Data for our primary analysis are obtained from ADNI (http://adni.loni.usc.edu/and processed using the ANTs longitu-dinal single-subject template pipeline (Tustison et al., 2019) with code available on GitHub (https://github.com/ntustison/CrossLong).",0,0,1
10.1016/j.neuroimage.2021.118822,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,"A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf et al., 2010; Reig et al., 2009; Wonderlick et al., 2009).",0,0,1
10.1016/j.neuroimage.2021.118822,adni.loni.usc.edu/and,"ADNI data analysis Data for our primary analysis are obtained from ADNI (http://adni.loni.usc.edu/and processed using the ANTs longitu-dinal single-subject template pipeline (Tustison et al., 2019) with code available on GitHub (https://github.com/ntustison/CrossLong).",1,0,0
10.1016/j.neuroimage.2021.118822,github.com/ntustison/crosslong,"ADNI data analysis Data for our primary analysis are obtained from ADNI (http://adni.loni.usc.edu/and processed using the ANTs longitu-dinal single-subject template pipeline (Tustison et al., 2019) with code available on GitHub (https://github.com/ntustison/CrossLong).",1,0,0
10.1016/j.neuroimage.2021.118822,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2021.118822,github.com/jfortin1/combatharmonization,Reference implementa-tions for ComBat are available in R and Matlab (https://github.com/Jfortin1/ComBatHarmonization) and in Python (https://github.com/Jfortin1/neuroCombat).,1,0,0
10.1016/j.neuroimage.2021.118822,github.com/jfortin1/neurocombat,Reference implementa-tions for ComBat are available in R and Matlab (https://github.com/Jfortin1/ComBatHarmonization) and in Python (https://github.com/Jfortin1/neuroCombat).,1,0,0
10.1016/j.neuroimage.2022.118930,mrtrix.org,"(2019) (www.mrtrix.org), as well as all Python code required to perform CSS and map smoothed connectomes at ei-ther the resolution of vertices or an atlas, are provided in our git repository.",1,0,0
10.1016/j.neuroimage.2022.118930,humanconnectome.org,Data and code availability All imaging data used in this study was sourced from the Human Connectome Project (HCP) (www.humanconnectome.org).,0,1,0
10.1016/j.neuroimage.2022.118930,github.com/10,This code repository can be accessed from github.com/10 S.,1,0,0
10.1016/j.neuroimage.2022.119433,cran.r-project.org/web/packages/lmertest/index.html,"lmerTest (Kuznetsova et al., 2017) https://cran.r-project.org/web/packages/lmerTest/index.html.",1,0,0
10.1016/j.neuroimage.2022.119433,github.com/marlow17/fluctuation,FluctuationAnalysis https://github.com/marlow17/Fluctuation Analysis.,1,0,0
10.1016/j.neuroimage.2022.119433,github.com/jvohryzek/ghostattractors,Ghost Attractors https://github.com/jvohryzek/GhostAttractors.,0,0,1
10.1016/j.neuroimage.2022.119433,mathworks.com/matlabcentral/ﬁleexchange/22099-intraclass-correlation-coeﬃcient-icc,"Intraclass Correlation Coeﬃcient (ICC) (https://www.mathworks.com/matlabcentral/ﬁleexchange/22099-intraclass-correlation-coeﬃcient-icc), MATLAB Central File Exchange.",1,0,0
10.1016/j.neuroimage.2022.119433,mathworks.com/matlabcentral/ﬁleexchange/69652-ﬁlled-area-plot,"(https://www.mathworks.com/matlabcentral/ﬁleexchange/69652-ﬁlled-area-plot), MATLAB Central File Exchange.",1,0,0
10.1016/j.neuroimage.2022.119433,github.com/easystats/performance,"Performance (Lüdecke et al., 2021) https://github.com/easystats/performance.",1,0,0
10.1016/j.neuroimage.2022.119433,github.com/fundyn/dfcwalk,Acknowledgements The authors would like to acknowledge the use of the following freely available code: MATLAB Toolbox dFCwalk https://github.com/FunDyn/dFCwalk.,1,0,0
10.1016/j.neuroimage.2022.119433,humanconnectome.org,A complete description of the acquisition and pre-processing details may be found at the HCP website https://www.humanconnectome.org/.,0,0,1
10.1016/j.neuroimage.2022.119433,github.com/franhancock/complexity-science-in-dfc,"Code availability statement The Matlab and R code developed for this analysis is available at github.com/franhancock/Complexity-science-in-dFC together with the 5 phase-locking mode centroids for AAL parcellation in NIFTI and in Matlab format., Code availability statement The Matlab and R code developed for this analysis is available github.com/franhancock/Complexity-science-in-dFC together with the 5 phase-locking mode centroids for AAL parcellation in NIFTI and in Matlab format.",1,0,0
10.1016/j.neuroimage.2022.119433,github.com/taiyun/corrplot,"(Version 0.92), https://github.com/taiyun/corrplot.",1,0,0
10.1016/j.neuroimage.2022.119433,github.com/scottclowe/superbar,"superbar (https://github.com/scottclowe/superbar), GitHub.",1,0,0
10.1016/j.neuroimage.2022.119466,3.2.2.2,3.2.2.2.,0,0,1
10.1016/j.neuroimage.2022.119466,osf.io/y3k5f,Data and code availability statement All relevant data (including the raw EEG data and the scripts used for the analysis) will be made publicly available at the OSF (https://osf.io/y3k5f/) after acceptance for publication.,0,1,0
10.1016/j.neuroimage.2022.119466,3.2.1.2,3.2.1.2.,0,0,1
10.1016/j.neuroimage.2022.119466,3.2.1.1,Experiment 1 3.2.1.1.,0,0,1
10.1016/j.neuroimage.2022.119466,3.2.2.1,Experiment 2 3.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119445,osf.io/eufd6,"Data and code availability statement Data and code are available on the Open Science Framework acces-sible via https://osf.io/eufd6/., Data and code availability statement Data and code are available on the Open Science Framework acces-sible via https://osf.io/eufd6/.",0,1,0
10.1016/j.neuroimage.2022.119769,surfer.nmr.mgh.harvard.edu,"The resulting map was registered to Talairach space and visualized using FreeSurfer (version 6.0, http://surfer.nmr.mgh.harvard.edu).",1,0,0
10.1016/j.neuroimage.2022.119769,github.com/brainneuro/multi-face-attribution,Data availability Python and matlab codes for training the CNN models and analysis are available at https://github.com/brainneuro/Multi-Face-attribution.,1,0,0
10.1016/j.neuroimage.2022.119630,neurovault.org/collections/eqmwbxpz,Group-level statistics for the data reported in the manuscript can be downloaded here: neurovault.org/collections/EQMWBXPZ And the R code and behavioural/demographic data used for statistical anal-yses can be found here: github.com/frdarya/LearningRate.,0,1,0
10.1016/j.neuroimage.2022.119630,dbm.neuro.uni-jena.de/tfce,We conducted whole-brain analyses using a threshold-free cluster enhancement (TFCE) approach with 5000 permutations and default parameters (E = 0.5 and H = 2) using the TFCE tool (version r223) for CAT12 toolbox in SPM (http://dbm.neuro.uni-jena.de/tfce).,1,0,0
10.1016/j.neuroimage.2022.119630,r-project.org,"To assess the learning rate across trials, we ﬁt a linear mixed-eﬀects model of the number of items freely recalled in each immediate trial, as a func-tion of the recall trial (ﬁrst, second, and third) using the lme4 package in R 4.0.2 (https://www.r-project.org/).",1,0,0
10.1016/j.neuroimage.2022.119630,ﬁl.ion.ucl.ac.uk/spm,Grey matter VBM The analysis was carried out in SPM12 (version r6225; https://www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119630,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"For preprocessing these images, the FSL toolbox (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki) was used for motion and eddy cur-rent correction, the extraction of non-brain voxels and, lastly, the cal-culation of voxel-wise diﬀusion maps (FA and MD) for each participant., Individual FA and MD maps were then used in the FSL TBSS pipeline (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/TBSS/UserGuider ; detailed meth-ods described by Smith et al.",1,0,0
10.1016/j.neuroimage.2022.119630,github.com/frdarya/learningrate,Group-level statistics for the data reported in the manuscript can be downloaded here: neurovault.org/collections/EQMWBXPZ And the R code and behavioural/demographic data used for statistical anal-yses can be found here: github.com/frdarya/LearningRate.,0,1,0
10.1016/j.neuroimage.2022.119630,fsl.fmrib.ox.ac.uk/fsl/fslwiki/tbss/userguider,Individual FA and MD maps were then used in the FSL TBSS pipeline (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/TBSS/UserGuider ; detailed meth-ods described by Smith et al.,1,0,0
10.1016/j.neuroimage.2022.119630,rii.uthscsa.edu/mango,"AAL3 atlas neuroanatomical labels were used to describe neuroanatom-ical loci (Rolls et al., 2020) and Mango software was used to produce the ﬁgure (http://rii.uthscsa.edu/mango/).",1,0,0
10.1016/j.neuroimage.2022.119754,github.com/gifale95/eeg_encoding,Code availability The code to reproduce all the results is available on GitHub at https://github.com/gifale95/eeg_encoding.,1,0,0
10.1016/j.neuroimage.2022.119754,things-initiative.org,"To address this challenge, the so-called THINGS initiative promotes using the THINGS database to col-lect and share behavioral and neuroscientiﬁc datasets for the same set of images -also used here -among vision researchers (https://things-initiative.org/).",0,1,0
10.1016/j.neuroimage.2022.119556,ac.uk/spm/software,ac.uk/spm/software/) in MATLAB environment.,1,0,0
10.1016/j.neuroimage.2022.119556,osf.io/9q4rp/?view_only,Data Availability Raw data and code on OSF : https://osf.io/9q4rp/?view_only = 20d 21199ecf8425abda1f8cfe533a788.,0,1,0
10.1016/j.neuroimage.2022.119556,osf.io/9q4rp/?,"Data and code availability statement The raw data are available as Nifti ﬁles (MRI) and EyeLink EDF ﬁles (eye movements) in an OSF project (https://osf.io/9q4rp/?, Data Availability Raw data and code on OSF : https://osf.io/9q4rp/?view_only = 20d 21199ecf8425abda1f8cfe533a788.",0,1,0
10.1016/j.neuroimage.2022.119690,audiotext.com.br,"To measure the semantic similarity between two oral reports, the reports were transcribed by professional personnel blind to the experi-ment (http://www.audiotext.com.br/), transformed to lowercase, word-tokenized and cleared from non-alphabetic tokens and stop-words using the nltk Portuguese stop-word list (Bird et al., 2009).",1,0,0
10.1016/j.neuroimage.2022.119453,atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core:,"The probability maps and the MPM are publicly and freely available for download at the Julich-Brain atlas (https://jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php##mitte) and the Human Brain Project (https://atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core: referencespace:v1.0.0:tmp-fsaverage/p:minds:core:parcellationatlas: v1.0.0:94c1125b-b87e-45e4-901c-00daee7f2579-290/@:0.0.0.-W000.., Data availability The probability maps and the maximum probability maps are pub-licly and freely available for download at the Julich-Brain atlas (https://jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php# #mitte) and the Human Brain Project (https://atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core:referencespace:v1.0.",1,0,0
10.1016/j.neuroimage.2022.119453,jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php##mitte,The probability maps and the MPM are publicly and freely available for download at the Julich-Brain atlas (https://jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php##mitte) and the Human Brain Project (https://atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core: referencespace:v1.0.0:tmp-fsaverage/p:minds:core:parcellationatlas: v1.0.0:94c1125b-b87e-45e4-901c-00daee7f2579-290/@:0.0.0.-W000..,0,0,1
10.1016/j.neuroimage.2022.119453,ebrains.eu/news/new-maps-features-ebrains-multilevel-human-brain-atlas,"These maps are openly available in the multi-modal atlas of the Human Brain Project at the EBRAINS platform (https://ebrains.eu/service/human-brain-atlas/), together with a surface map in the FreeSurfer reference space (https://ebrains.eu/news/new-maps-features-ebrains-multilevel-human-brain-atlas/). 5 J.",0,0,1
10.1016/j.neuroimage.2022.119453,github.com/jquab/insula-cytoarchitecure-,0:tmp-fsaverage/p:minds:core:parcellationatlas:v1.0.0:94c1125b-b87e-45e4-901c-00daee7f2579-290/@:0.0.0.-W000..2_qztu.-8_uv.–2o6B.2_iz3G..23x6..0.0.0..1) Matlab script for multidimensional scaling and bins of microstructural areas as used in our analysis can be downloaded from: https://github.com/JQuab/Insula-cytoarchitecure-.,1,0,0
10.1016/j.neuroimage.2022.119453,ebrains.eu/service/human-brain-atlas,"These maps are openly available in the multi-modal atlas of the Human Brain Project at the EBRAINS platform (https://ebrains.eu/service/human-brain-atlas/), together with a surface map in the FreeSurfer reference space (https://ebrains.eu/news/new-maps-features-ebrains-multilevel-human-brain-atlas/). 5 J.",0,0,1
10.1016/j.neuroimage.2022.119453,atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core:referencespace:v1.0,Data availability The probability maps and the maximum probability maps are pub-licly and freely available for download at the Julich-Brain atlas (https://jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php# #mitte) and the Human Brain Project (https://atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core:referencespace:v1.0.,0,1,0
10.1016/j.neuroimage.2022.119453,jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php#,"The probability maps and the MPM are publicly and freely available for download at the Julich-Brain atlas (https://jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php##mitte) and the Human Brain Project (https://atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core: referencespace:v1.0.0:tmp-fsaverage/p:minds:core:parcellationatlas: v1.0.0:94c1125b-b87e-45e4-901c-00daee7f2579-290/@:0.0.0.-W000.., Data availability The probability maps and the maximum probability maps are pub-licly and freely available for download at the Julich-Brain atlas (https://jubrain.fz-juelich.de/apps/cytoviewer2/cytoviewer-maintenance.php# #mitte) and the Human Brain Project (https://atlases.ebrains.eu/viewer/-/a:juelich:iav:atlas:v1.0.0:1/t:minds:core:referencespace:v1.0.",1,0,0
10.1016/j.neuroimage.2022.119389,github.com/viniciuslima94/pygc,"We used non-parametric meth-ods (Dhamala et al., 2008a , 2008b) with modiﬁed online codes (https://github.com/ViniciusLima94/pyGC) to estimate Granger causality av-eraged for the low-frequency band within a 0 –1 s time window.",1,0,0
10.1016/j.neuroimage.2022.119134,clinicaltrials.gov,"Participants Sixty-two healthy individuals (52 female, mean age = 38.5 years, sd = 10) were recruited from the general population in the city of Berlin in the context of a longitudinal intervention study on the eﬀects of a Mindfulness-based Stress Reduction (MBSR) program, pre-registered at clinicaltrials.gov NCT03035669.",0,0,1
10.1016/j.neuroimage.2022.119134,fmrib.ox.ac.uk/fsl,"Data pre-processing and analysis were carried out with FSL 6.0 (FM-RIB’s Software Library, www.fmrib.ox.ac.uk/fsl).",1,0,0
10.1016/j.neuroimage.2022.119062,afni.nimh.nih.gov/afni,Data and code availability statement Publicly available software used for analyses is Analysis of Func-tional NeuroImages (AFNI) https://afni.nimh.nih.gov/afni/.,1,0,0
10.1016/j.neuroimage.2022.119062,surfer.nmr.mgh.harvard.edu/fswiki,Individual inﬂated results were obtained on Freesurfer (https://surfer.nmr.mgh.harvard.edu/fswiki/) and AFNI’s SUMA.,1,0,0
10.1016/j.neuroimage.2021.118844,github.com/xiaochunhan/nps_measurement_properties,Data and code availability statements Code for all analyses and ﬁgures is available at https://github.com/XiaochunHan/NPS_measurement_properties.,1,0,0
10.1016/j.neuroimage.2021.118844,osf.io/v9px7,Data for all analyses and ﬁgures is available at https://osf.io/v9px7/.,0,1,0
10.1016/j.neuroimage.2022.118940,db.humanconnectome.org,"The datasets are publicly available on openneuro.org with acces-sion numbers ds000157, ds000205 and ds000051 for FNF, AV and BW respectively and on ConnectomeDB (https://db.humanconnectome.org) for the HCP dataset., For the HCP dataset, we used the al-ready preprocessed ﬁles for task data as available on ConnectomeDB (https://db.humanconnectome.org)., Data/Code Availability Statement All datasets are publicly available from https://openneuro.org and https://db.humanconnectome.org and codes are available at bitbucket repository https://bitbucket.org/vtripathi/iisc/src/master/Acknowledgements The authors would like to thanks Jaspreet Kaur and Varun Kumar for their valuable comments and inputs.",0,1,0
10.1016/j.neuroimage.2022.118940,openneuro.org,"The datasets are publicly available on openneuro.org with acces-sion numbers ds000157, ds000205 and ds000051 for FNF, AV and BW respectively and on ConnectomeDB (https://db.humanconnectome.org) for the HCP dataset., Data/Code Availability Statement All datasets are publicly available from https://openneuro.org and https://db.humanconnectome.org and codes are available at bitbucket repository https://bitbucket.org/vtripathi/iisc/src/master/Acknowledgements The authors would like to thanks Jaspreet Kaur and Varun Kumar for their valuable comments and inputs.",0,1,0
10.1016/j.neuroimage.2022.118940,bitbucket.org/vtripathi/iisc/src/master/acknowledgements,Data/Code Availability Statement All datasets are publicly available from https://openneuro.org and https://db.humanconnectome.org and codes are available at bitbucket repository https://bitbucket.org/vtripathi/iisc/src/master/Acknowledgements The authors would like to thanks Jaspreet Kaur and Varun Kumar for their valuable comments and inputs.,0,1,0
10.1016/j.neuroimage.2022.118940,openneuro.org,Data/Code Availability Statement All datasets are publicly available from https://openneuro.org and https://db.humanconnectome.org and codes are available at bitbucket repository https://bitbucket.org/vtripathi/iisc/src/master/Acknowledgements The authors would like to thanks Jaspreet Kaur and Varun Kumar for their valuable comments and inputs.,0,1,0
10.1016/j.neuroimage.2022.119610,cognitiveatlas.org/concept/id/trm_55b6b9d7c9435,"Each feature corresponded to an entity in the Atlas, such as “working memory maintenance ”(https://www.cognitiveatlas.org/concept/id/trm_55b6b9d7c9435).",0,0,1
10.1016/j.neuroimage.2022.119610,nmr.mgh.harvard.edu/~greve/fbirn/b0/epidewarp.fsl,"Greve’s “epidewarp.fsl ”s c r i p t (https://www.nmr.mgh.harvard.edu/~greve/fbirn/b0/epidewarp.fsl) and further improvements of HCP Pipelines (Glasser et al., 2013).",1,0,0
10.1016/j.neuroimage.2022.119610,github.com/waltersjonathon/cognitive_encoding_models,"All anal-ysis code is available at https://github.com/waltersjonathon/cognitive_encoding_models., All analysis code is available at https://github.com/waltersjonathon/cognitive_encoding_models.",1,0,0
10.1016/j.neuroimage.2021.118835,sylabs.io/guides/3.5/admin-guide/installation.html,Software requirements and dependencies Ironsmith requires a Linux distribution with Singularity ver-sion 3.5 or higher installed (https://sylabs.io/guides/3.5/admin-guide/installation.html) and Bash Unix shell version 4.2.46(2) or later (GNU coreutils).,1,0,0
10.1016/j.neuroimage.2021.118835,surfer.nmr.mgh.harvard.edu/fswiki/fstutorial/anatomical,"The script then uses these ﬁles to create anatomical ROIs using the FSL, fslmaths function with the –bin option and a threshold corresponding to the Freesurfer label number of a particular anatomical structure (https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/Anatomical ROI/FreeSurferColorLUT).",1,0,0
10.1016/j.neuroimage.2021.118835,github.com/korbinian90/aspire,"Alternatively, if correct coil combination cannot be achieved with the default scanner software, then uncombined, raw scanner data can be collected instead and reconstructed using software toolboxes such as ASPIRE (Eckstein et al., 2018 ; https://github.com/korbinian90/ASPIRE).",1,0,0
10.1016/j.neuroimage.2021.118835,surfer.nmr.mgh.harvard.edu/fswiki/corticalparcellation,The AFNI command 3dmerge is also used by the script to combine anatomical structures into lobes as recommended by Freesurfer (https://surfer.nmr.mgh.harvard.edu/fswiki/CorticalParcellation).,1,0,0
10.1016/j.neuroimage.2021.118835,cds.ismrm.org/protected/18mproceedings/pdfﬁles/4992.html,"PS-AC can address coil combination issues (see https://cds.ismrm.org/protected/18MProceedings/PDFﬁles/4992.html) that can lead to phase artifacts (Bernstein et al., 1994).",1,0,0
10.1016/j.neuroimage.2021.118800,python.org,"Classiﬁcation of connectivity matrices The classiﬁcation of emotion categories was performed in Python 2.7.11 (Python Software Foundation, http://www.python.org) using the Scikit learn package (Pedregosa et al., 2011).",1,0,0
10.1016/j.neuroimage.2021.118800,github.com/eglerean/hfasdmodules/blob/master/abide/bramila_mantel.m,(2016) (function bramila_mantel in https://github.com/eglerean/hfASDmodules/blob/master/ABIDE/bramila_mantel.m).,1,0,0
10.1016/j.neuroimage.2021.118800,web.archive.org/web/20160127134525/http://nil.wustl,We ex-tracted the BOLD time course for each node by averaging the activity of voxels within a 1-cm diameter sphere centered at each node’s coordinates (list of coordinates and module assignments available at https://web.archive.org/web/20160127134525/http://www.nil.wustl.,0,0,1
10.1016/j.neuroimage.2021.118800,mathworks.com,"fMRI data were preprocessed using FSL (FMRIB’s Software Library, www.fmrib.ox.ac.uk/fsl) and inhouse MATLAB (The MathWorks, Inc., Natick, Massachusetts, USA, http://www.mathworks.com) tools (code available at: https://version.aalto.ﬁ/gitlab/BML/bramila).",1,0,0
10.1016/j.neuroimage.2021.118800,fmrib.ox.ac.uk/fsl,"fMRI data were preprocessed using FSL (FMRIB’s Software Library, www.fmrib.ox.ac.uk/fsl) and inhouse MATLAB (The MathWorks, Inc., Natick, Massachusetts, USA, http://www.mathworks.com) tools (code available at: https://version.aalto.ﬁ/gitlab/BML/bramila).",1,0,0
10.1016/j.neuroimage.2021.118800,github.com/hpsaarimaki/aalto-emotion-networks,Code for run-ning the analyzes can be found at https://github.com/hpsaarimaki/Aalto-emotion-networks.,1,0,0
10.1016/j.neuroimage.2022.119342,surfer.nmr.mgh.harvard.edu/fswiki/fstutorial/controlpoints_freeview,"In these three patients, the pial surface was manually delineated using the Control Point function implemented in the FreeSurfer software package (https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/ControlPoints_freeview/; Deoni et al., 2015 ; Croteau-Chonka et al., 2016 ; Remer et al., 2017 ; Fig.",1,0,0
10.1016/j.neuroimage.2022.119342,surfer.nmr.mgh.harvard.edu/fswiki/infantfs,"For those younger than two years, we used the Infant FreeSurfer software pack-age (https://surfer.nmr.mgh.harvard.edu/fswiki/infantFS; Zöllei et al., 2020).",1,0,0
10.1016/j.neuroimage.2022.119342,mathworks.com/help/stats/ﬁtlme.html,We employed the MATLAB ﬁtlme command (https://www.mathworks.com/help/stats/ﬁtlme.html) to ﬁt the mixed models speciﬁed by the following formula: ’high-gamma responses ∼1 + age + sex + seizure onset zone + MRI + sleep + number of anti-seizure medications + (1|patient)’.,1,0,0
10.1016/j.neuroimage.2022.119031,github.com/enetterevilla/cod_motiondetection,"All the motion detection source code is available on GitHub (https://github.com/enetterevilla/COD_MotionDetection)., Code availability The source code used in this study is openly available in GitHub at https://github.com/enetterevilla/COD_MotionDetection.",1,0,0
10.1016/j.neuroimage.2022.119330,nitrc.org/projects/biggaba,"Dataset Data were obtained from the NITRIC Big GABA repository (www.nitrc.org/projects/biggaba/), which consists of single voxel MRS data acquired on 3T MRI scanners from the three major vendors, Gen-eral Electric (GE), Philips, and Siemens (see Mikkelsen et al., 2017 for details on data acquisition)., Data and code availability statement Data were obtained from the NITRIC Big GABA repository (www.nitrc.org/projects/biggaba/).",0,1,0
10.1016/j.neuroimage.2022.119330,github.com/harrisbrainlab/lm4hz,"Scripts for the linear model procedure can be found at https://github.com/HarrisBrainLab/lm4hz., Scripts for the linear model procedure can be found at https://github.com/HarrisBrainLab/lm4hz.",1,0,0
10.1016/j.neuroimage.2022.119330,r-project.org,Residualizing Site eﬀects were estimated for each metabolite using linear re-gression implemented in R version 4.0.3 (R Core Team (2020) https://www.R-project.org/) using the package “stats ” version 4.1.2.,1,0,0
10.1016/j.neuroimage.2022.119330,github.com/jfortin1/combat,ComBat Data were harmonized using the “neuroComBat ”f u n c t i o n version 1.0.5 (available at https://github.com/Jfortin1/ComBat Harmonization/tree/master/R).,1,0,0
10.1016/j.neuroimage.2022.119296,github.com/mbnebel/deconfoundedfmri,"Toy example and tutorial We simulate a dataset with bias and estimate the de-confounded group diﬀerence in a tutorial available at https://github.com/mbnebel/DeconfoundedFMRI/blob/thebrisklab-main/DeconfoundGroupDiﬀerence_Tutorial.Rmd., Code to reproduce this ex-ample is available at https://github.com/mbnebel/DeconfoundedFMRI/blob/thebrisklab-main/DeconfoundGroupDiﬀerence_Tutorial.Rmd., The code for recreating all analyses, tables, and ﬁgures in this study is available at https://github.com/mbnebel/DeconfoundedFMRI. 7 M.B.",1,0,0
10.1016/j.neuroimage.2022.119296,github.com/mbnebel/deconfoundedfmri/blob/thebrisklab-main/deconfoundgroupdiﬀerence_tutorial.rmd,"Toy example and tutorial We simulate a dataset with bias and estimate the de-confounded group diﬀerence in a tutorial available at https://github.com/mbnebel/DeconfoundedFMRI/blob/thebrisklab-main/DeconfoundGroupDiﬀerence_Tutorial.Rmd., Code to reproduce this ex-ample is available at https://github.com/mbnebel/DeconfoundedFMRI/blob/thebrisklab-main/DeconfoundGroupDiﬀerence_Tutorial.Rmd.",1,0,0
10.1016/j.neuroimage.2022.119296,trendscenter.org/software/gift,"Group ICA and partial correlations Thirty components were estimated using group independent compo-nent analysis (Group ICA) with 85 principal components retained in the initial subject-level dimension reduction step from the scans that passed lenient motion QC (GIFT v3.0b: https://trendscenter.org/software/gift/; Medical Image Analysis Lab, Albuquerque, New Mexico; Calhoun et al., 2001; Erhardt et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119296,github.com/kki-cnir/cnir-fmri_preproc_toolbox,"Rs-fMRI scans for the remaining 478 participants in the complete predictor case set were vi-sually inspected for artifacts and preprocessed using SPM12 (Wellcome Trust Centre for Neuroimaging, London, United Kingdom) and custom code written in MATLAB (The Mathworks, Inc., Natick Massahusetts), which is publicly available (https://github.com/KKI-CNIR/CNIR-fmri_preproc_toolbox).",1,0,0
10.1016/j.neuroimage.2022.119411,github.com/yonsei-milab/mri-motion-artifact-simulation-tool,We make our motion simulation tool ‘view2Dmotion’ publicly available at https://github.com/Yonsei-MILab/MRI-Motion-Artifact-Simulation-Tool.,1,0,0
10.1016/j.neuroimage.2022.119063,mrtrix.org,"Diﬀusion data preprocessing All preprocessing steps employed MRtrix3 (Tournier et al., 2012) (www.mrtrix.org) commands or used Mrtrix3 scripts that linked exter-nal software packages.",1,0,0
10.1016/j.neuroimage.2022.119063,github.com/starklabuci/noddiﬀusion_rocs,"The code used for analysis is in a GitHub repos-itory: https://github.com/StarkLabUCI/NODDiﬀusion_ROCs., The code used for analysis is avail-able here: https://github.com/StarkLabUCI/NODDiﬀusion_ROCs.",1,0,0
10.1016/j.neuroimage.2022.119658,graphpad.com,"Outliers were identiﬁed using ROUT outlier identiﬁcation (Q = 1%) in Graph-Pad Prism version 9.1.0 for MacOS (GraphPad Software, San Diego, Cal-ifornia USA, www.graphpad.com).",1,0,0
10.1016/j.neuroimage.2022.119658,surfer.nmr.mgh.harvard.edu,MPRAGE images were processed in native space in FreeSurfer version 5.3.0 (http://surfer.nmr.mgh.harvard.edu/).,1,0,0
10.1016/j.neuroimage.2022.119658,ﬁl.ion.ucl.ak.uk/spm,"We generated a study-speciﬁc template using a standard pipeline in Statistical Parametric Mapping 12 software (SPM12, www.ﬁl.ion.ucl.ak.uk/spm) to facilitate warping to MNI space.",1,0,0
10.1016/j.neuroimage.2022.119658,nitrc.org/projects/mango,"Brieﬂy, accurate coregistration to MNI space FMT Ki maps (nearest-neighbor interpolation) was conﬁrmed in Mango (https://www.nitrc.org/projects/mango/).",0,0,1
10.1016/j.neuroimage.2022.119658,neurovault.org/collections/mpdbczkt,"Primary FMT analyses relied on a MNI-space LC ROI developed from six existing LC templates (Dahl et al., 2022) that we previously described (Ciampa et al., 2022) (Fig. 2 A; ROI available: https://neurovault.org/collections/MPDBCZKT/)., Analyses measured FMT in the dorsal raphe (Kranz et al., 2012) using an unsmoothed ROI that has previ-ously been used in PET imaging (Doppler et al., 2021) (ROI available: https://neurovault.org/collections/MPDBCZKT/; Fig. 2 A)., LC and Raphe ROIs are available at https://neurovault.org/collections/MPDBCZKT/.",0,1,0
10.1016/j.neuroimage.2022.119679,osf.io/5mqpb,Data and code availability Data and code are available through the Open Science Framework at https://osf.io/5mqpb/.,0,1,0
10.1016/j.neuroimage.2022.119226,mgh.harvard.edu,mgh.harvard.edu/) to derive cerebral and cerebellar GM and WM seg-mentations.,0,0,1
10.1016/j.neuroimage.2022.119226,clinicaltrials.gov,"Generation 100 Study and brain MRI sub-study The Generation 100 Study is a randomized controlled trial (RCT) (NCT01666340, ClinicalTrials.gov registry, and Regional Committee for Medical Research Ethics, Central Norway 2012/381 B) investigating the eﬀect of exercise intervention on overall mortality in older adults from the general population (Stensvold et al., 2015).",0,0,1
10.1016/j.neuroimage.2022.119226,ﬁl.ion.ucl.ac.uk/spm,"The intracranial volume (ICV) was estimated using the auto-mated reverse brain mask (ARBM) method (Hansen et al., 2015) in SPM8 (http://www.ﬁl.ion.ucl.ac.uk/spm), with default parameters, us-ing both the T 1 -and T 2 -weighted 3D images (Fig.",1,0,0
10.1016/j.neuroimage.2022.119226,github.com/chiaramarzi/fractalbrain-toolkit,Fractal analysis The fractal analysis was carried out using the fractalbrain toolkit version 1.0 (freely available at https://github.com/chiaramarzi/fractalbrain-toolkit) and described in detail in Marzi et al.,1,0,0
10.1016/j.neuroimage.2022.119715,mccauslandcenter.sc.edu/mricrogl,All results were visualized using MRIcroG L (https://www.mccauslandcenter.sc.edu/mricrogl/).,1,0,0
10.1016/j.neuroimage.2022.119715,nitrc.org/projects/conn,"The analysis was performed with the resting state (RS)fMRI data and the task-data of all sessions using the CONN toolbox (version 18b; Whitﬁeld-Gabrieli and Nieto-Castanon, 2012 ; http://www.nitrc.org/projects/conn) for Matlab (version 2019a).",1,0,0
10.1016/j.neuroimage.2022.119715,nist.mni.mcgill.ca/?p,"The realigned images were stereotactically normalized into the standard anatomical space de-ﬁned by the Montreal Neurological Institute (MNI) template by means of the DARTEL algorithm including geodesic shooting using an existing MNI-template (http://nist.mni.mcgill.ca/?p = 904) and the CAT12 tool-box (version 1450) (Ashburner, 2007).",1,0,0
10.1016/j.neuroimage.2022.119715,github.com/koenhaak/congrads.git,"Connectopic mapping Using the previously deﬁned ROIs, we examined the connec-topic mapping of all egomotion hubs-CSv, PcM/pCi, h7a, VPS (left and right separately) and the uvula with the Congrads toolbox (https://github.com/koenhaak/congrads.git) (Haak et al., 2018) and fslpython (Python version 3.7).",1,0,0
10.1016/j.neuroimage.2022.119715,arringtonresearch.com,"search Software (http://www.arringtonresearch.com/; width = 320 px, height = 240 px and 60 Hz).",1,0,0
10.1016/j.neuroimage.2022.119715,"systems,mrc-systems.de","Eye movements were recorded with an infrared VOG unit (MRI-compatible camera, MRC systems,www.mrc-systems.de).",1,0,0
10.1016/j.neuroimage.2022.119249,hmri.info,"MPM generation The generation of (semi-) quantitative maps were per-formed using the hMRI toolbox (version 0.2.0, www.hmri.info, Tabelow et al.",1,0,0
10.1016/j.neuroimage.2022.119249,neuromorphometrics.com,For this purpose we used the neuromorphometrics atlas (neuromorphometrics.com) to ﬁrst ag-gregate ROI-level mean values of unsmoothed normalized MPMs inside 57 gray matter regions in cortical and subcortical locations in template space.,1,0,0
10.1016/j.neuroimage.2022.119211,bd2kccd.github.io/docs/causal-cmd,"We computed FGES with causal-cmd v1.2.0 (https://bd2kccd.github.io/docs/causal-cmd/) using default parameters (BIC penalized likelihood score, penalty dis-count = 1 corresponding to the classic BIC score)., Fast Greedy Equiv-alence Search (FGES) was conducted using causal-cmd software, available at https://bd2kccd.github.io/docs/causal-cmd/.",1,0,0
10.1016/j.neuroimage.2022.119211,humanconnectome.org/data/projects/hcp_1200,"Subjects All analyses used publicly available resting-state functional neu-roimaging data from 442 unrelated healthy young adult subjects re-cruited as part of the Washington University – Minnesota (WU-Minn) Human Connectome Project Consortium (56%  n = 248 female; aged 22–35 mean age = 28.6 years; https://db.humanconnectome.org/data/projects/HCP_1200) (Barch et al., 2013 ; Glasser et al., 2013 ; Marcus et al., 2013 ; Smith et al., 2013 ; U ğurbil et al., 2013 ; Van Essen et al., 2013)., humanconnectome.org/data/projects/HCP_1200.",0,1,0
10.1016/j.neuroimage.2022.119211,db.humanconnectome.org/data/projects/hcp_1200,"Subjects All analyses used publicly available resting-state functional neu-roimaging data from 442 unrelated healthy young adult subjects re-cruited as part of the Washington University – Minnesota (WU-Minn) Human Connectome Project Consortium (56%  n = 248 female; aged 22–35 mean age = 28.6 years; https://db.humanconnectome.org/data/projects/HCP_1200) (Barch et al., 2013 ; Glasser et al., 2013 ; Marcus et al., 2013 ; Smith et al., 2013 ; U ğurbil et al., 2013 ; Van Essen et al., 2013).",0,1,0
10.1016/j.neuroimage.2022.119211,sites.google.com/site/bctnet,"All graph theory metrics were computed using the Brain Connectivity Toolbox (BCT), available at https://sites.google.com/site/bctnet/.",1,0,0
10.1016/j.neuroimage.2022.119532,ﬁl.ion.ucl.ac.uk/spm,ﬁl.ion.ucl.ac.uk/spm/).,0,0,1
10.1016/j.neuroimage.2022.119532,nda.nih.gov/edit_collection.html?id,Data/code availability statement Data has been uploaded to NIH Data Archive (https://nda.nih.gov/edit_collection.html?id = 2645) and can be ac-cessed by submitting requests to NIH Data Archive.,0,1,0
10.1016/j.neuroimage.2022.119579,vislab.ucl.ac.uk,"The experiment was presented using Cogent 2000 (version 1.32, vislab.ucl.ac.uk) on Matlab.",1,0,0
10.1016/j.neuroimage.2022.119579,gitlab.com/kojala/threatlearning_fmri,"Code and data availability The code for the experiment, data analysis and ﬁgures are available in a public repository gitlab.com/kojala/threatlearning_fmri., Data and code availability statement The code for the experiment, data analysis and ﬁgures are available in a public repository gitlab.com/kojala/threatlearning_fmri.",1,0,0
10.1016/j.neuroimage.2022.119579,bachlab.github.io/pspm,"Eye-Link data ﬁles were converted and imported into the Psychophysiologi-cal Modeling (PsPM) toolbox (version 4.0.1, bachlab.github.io/PsPM/) in MATLAB2018a for further preprocessing and analysis.",1,0,0
10.1016/j.neuroimage.2022.119329,humanconnectome.org,"humanconnectome.org., Data and code availability statement MRI data included in our manuscript can be accessed from https://www.humanconnectome.org.",0,1,0
10.1016/j.neuroimage.2022.119329,humanconnectome.org,Data and code availability statement MRI data included in our manuscript can be accessed from https://www.humanconnectome.org.,0,1,0
10.1016/j.neuroimage.2022.119454,people.eecs.berkeley.edu/∼chunlei.liu/software.html,"The Laplacian-based algorithm for phase unwrapping, V-SHARP and iLSQR are implemented in STI Suite (MATLAB toolbox, available at https://people.eecs.berkeley.edu/∼chunlei.liu/software.html from UC Berkeley, Berkeley, CA, USA).",1,0,0
10.1016/j.neuroimage.2022.119767,uzh.ch/keyinst/loreta.htm,"Source localization The sLORETA software (http://www.uzh.ch/keyinst/loreta.htm) (Fuchs et al., 2002 ; Jurcak et al., 2007) was used to estimate the neu-ral sources of the N1 and P2 responses to vocal pitch perturbations across the two stimulation sessions.",1,0,0
10.1016/j.neuroimage.2022.119387,netneurotools.readthedocs.io,"Path-length-based SC and communicability-based SC were implemented us-ing the GRETNA toolbox (Wang et al., 2015b) and netneurotools (https://netneurotools.readthedocs.io), respectively.",1,0,0
10.1016/j.neuroimage.2022.119387,ﬁl.ion.ucl.ac.uk/spm,"To further reduce the eﬀects of nuisance covariates, we regressed out the white matter, cerebrospinal ﬂuid, global signals, and the 12 head motion parame-ters and performed temporal bandpass ﬁltering (0.01–0.1 Hz) using SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/) and GRETNA (Wang et al., 2015a).",1,0,0
10.1016/j.neuroimage.2022.119387,github.com/canlab/mediationtoolbox,"Mediation analysis To investigate whether the eﬀect of SC variability on FC variability was mediated by the regional distribution of SC-FC coupling, a boot-strapped mediation analysis was employed using the MATLAB package Mediation ToolBox (https://github.com/canlab/MediationToolbox).",1,0,0
10.1016/j.neuroimage.2022.119387,github.com/sunlianglong/structural-insight-into-individual-functional-variability,"Intermediate data sup-porting the results are available at https://github.com/sunlianglong/Structural-Insight-into-Individual-Functional-Variability., Codes used for the neuroimaging analysis and the statis-tical models are available at https://github.com/sunlianglong/Structural-Insight-into-Individual-Functional-Variability.",0,1,0
10.1016/j.neuroimage.2022.119387,db.humanconnectome.org,Data and code availability statement MRI data are redeposited and publicly available in the HCP Connec-tomeDB (https://db.humanconnectome.org/).,0,1,0
10.1016/j.neuroimage.2022.119387,github.com/danizoeller/mypls,Partial least-squares (PLS) analysis We then performed a partial least-squares correlation analysis with the myPLS toolbox (https://github.com/danizoeller/myPLS) to evaluate the implications of the structural-functional uniqueness correspondence in individual cognitive and behavioral performance.,1,0,0
10.1016/j.neuroimage.2022.119647,uibk.ac.at/psychologie/fachbereiche/pdd/personality_assessment/proms/take-the-test/brief-proms,The test can be accessed via this link (https://www.uibk.ac.at/psychologie/fachbereiche/pdd/personality_assessment/proms/take-the-test/brief-proms/) and takes 30 minutes to complete.,0,0,1
10.1016/j.neuroimage.2022.119647,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"Image analysis Analysis of all MRI data was conducted using SPM12 (The Wellcome Trust Centre for Human Neuroimaging, University College London; available at https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) with Matlab 2020b.",1,0,0
10.1016/j.neuroimage.2022.119647,fmri.wfubmc.edu/software/pickatlas,"These masks were made using the WFU Pickat-las (http://fmri.wfubmc.edu/software/pickatlas) and were based on the neuropsychological literature (bilateral inferior parietal lobule, includ-ing supramarginal and postcentral gyrus (Roswandowitz et al., 2018)), and previous neuroimaging literature (bilateral inferior frontal gyri and insulae (Andics et al. 2010 ; Latinus et al.",1,0,0
10.1016/j.neuroimage.2022.119657,ﬁl.ion.ucl.ac.uk/spm,"Dynamic causal modeling with Bayesian model selection The DCM for ERPs framework, as implemented in SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm), was used to estimate source level ef-fective connectivity between brain regions in response to auditory stim-ulation.",1,0,0
10.1016/j.neuroimage.2022.119320,jpnd.eu,The project is supported through the following funding organisations un-der the aegis of JPND -www.jpnd.eu (B.C.M.v.W.: the Netherlands Or-ganisation for Health Research and Development (ZonMw) -The Nether-lands; A.H.: the Deutsches Zentrum für Luft-und Raumfahrt -Germany).,0,0,1
10.1016/j.neuroimage.2022.119320,ﬁl.ion.ucl.ac.uk/spm,Data and code availability statements The open source Matlab toolboxes that were used in this study can be obtained from: Lead-DBS: https://www.lead-dbs.org SPM12: http://www.ﬁl.ion.ucl.ac.uk/spm Fieldtrip: http://ﬁeldtriptoolbox.org Custom-written Matlab scripts are available for sharing upon re-quest.,1,0,0
10.1016/j.neuroimage.2022.119320,lead-dbs.org,Data and code availability statements The open source Matlab toolboxes that were used in this study can be obtained from: Lead-DBS: https://www.lead-dbs.org SPM12: http://www.ﬁl.ion.ucl.ac.uk/spm Fieldtrip: http://ﬁeldtriptoolbox.org Custom-written Matlab scripts are available for sharing upon re-quest.,1,0,0
10.1016/j.neuroimage.2022.119038,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"fMRI data acquisition and pre-processing All data analysis was conducted using Statistical Parametric Map-ping software (SPM12, www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).",1,0,0
10.1016/j.neuroimage.2022.119310,ﬁl.ion.ucl.ac.uk/spm,"Imaging data Preprocessing of fMRI data was carried out in SPM 12 (Wellcome Trust Centre for Neuroimaging, www.ﬁl.ion.ucl.ac.uk/spm) within Mat-lab (Version 9.4, Mathworks, Nattick, MA, USA, 2018), using custom-made scripts.",1,0,0
10.1016/j.neuroimage.2022.119310,osf.io,Aggregated data and analysis scripts will be made available on https://osf.io after acceptance of this manuscript.,1,0,0
10.1016/j.neuroimage.2022.119037,mrtrix.org,The structural and diﬀusion-weighted MRI data of each subject were downloaded from the HCP and were processed using MRtrix3 (http://www.mrtrix.org/).,1,0,0
10.1016/j.neuroimage.2022.119037,ﬁl.ion.ucl.ac.uk,"The realigned volumes were registered to the structural T1 data us-ing rigid-body registration (SPM12, https://www.ﬁl.ion.ucl.ac.uk), and were detrended (i.e., constant, linear, quadratic) to remove signal drifts.",0,1,0
10.1016/j.neuroimage.2022.119531,humanconnectome.org/study/hcp-young-adult,"Data/code availability Data is openly available as part of the WU-Minn HCP 1200 Sub-jects Data Release of HCP Young Adult study, part of the Human Connectome Project (https://www.humanconnectome.org/study/hcp-young-adult/)., Data Availability Data is openly available as part of the WU-Minn HCP 1200 Subjects Data Release of HCP Young Adult study, part of the Human Connectome Project (https://www.humanconnectome.org/study/hcp-young-adult/).",0,1,0
10.1016/j.neuroimage.2022.119531,nihtoolbox.org,"Measures of cognition The present study used all behavioural data from the domain of cog-nition (Barch et al., 2013) obtained with tasks from the Blueprint for Neuroscience Research–funded NIH Toolbox for Assessment of Neuro-logical and Behavioral function (http://www.nihtoolbox.org) and tasks from the Penn computerized neurocognitive battery (Gur et al., 2010).",0,1,0
10.1016/j.neuroimage.2022.119531,nitrc.org/projects/artifact_detect,"Brieﬂy, images were realigned, slice-timing correction was conducted, and outlier detection of functional images for scrubbing was performed with Artefact Detection Tools (ART, https://www.nitrc.org/projects/artifact_detect/).",1,0,0
10.1016/j.neuroimage.2022.119531,github.com/mclit/sc-fc-cc,Materials and methods Codes used to implement the below analysis are available on GitHub (https://github.com/MCLit/SC-FC-CC).,1,0,0
10.1016/j.neuroimage.2022.118902,github.com/martin3141/neuro_metabolic_phenotyping,"All MRS analysis, statistics and machine-learning was performed with the R statistical computing platform (R Core Team, 2021) and anal-ysis scripts used to generate the ﬁgures and tables in this paper will be available from https://github.com/martin3141/neuro_metabolic_phenotyping upon publication., Analysis scripts used to generate the Fig.s and tables in this paper will be available from https://github.com/martin3141/neuro_metabolic_phenotyping upon publication.",1,0,0
10.1016/j.neuroimage.2022.119533,bobspunt.com/software/bspmview,"As FBA is typically smaller in size than the other ROIs, is more variable, and can show signiﬁcant overlap with FFA (Schwarzlose et al., 2005), we chose the 40 most activated voxels for FBA, rather than 100 (see Fig. 2 for a heat map of subject-speciﬁc ROIs, visualised using bspmview toolbox; DOI: 10.5281/zenodo.168074, see also https://www.bobspunt.com/software/bspmview/)).",1,0,0
10.1016/j.neuroimage.2022.119533,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"Those steps, and general linear model (GLM) estimation were performed with SPM12 (ﬁl.ion.ucl.ac.uk/spm/software/spm12).",1,0,0
10.1016/j.neuroimage.2022.119337,coins.trendscenter.org,NeuroImage 258 (2022) 119337 Data and code availability statement The data used in this article will be made publicly avail-able through the COINS framework at the completion of the study (https://coins.trendscenter.org/).,0,1,0
10.1016/j.neuroimage.2022.119337,neuroimage.usc.edu/brainstorm,"Data processing pipelines fol-lowed previous studies (Niso et al., 2019) using a combination of Brainstorm (Tadel et al., 2011), which is documented and freely available for download online under the GNU general public li-cense (http://neuroimage.usc.edu/brainstorm), and CAT12 (Gaser and Dahnke, 2016) toolboxes.",1,0,0
10.1016/j.neuroimage.2022.119292,icatb.sourceforge.net,Statistical analyses: source-based morphometry We ran three source-based morphometry analyses using the Group ICA of fMRI Toolbox (GIFT; http://icatb.sourceforge.net) in MATLAB R2015b.,1,0,0
10.1016/j.neuroimage.2022.119292,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fslvbm,"FSL-VBM pipeline Next, we ran each AC-PC aligned and aﬃne registered brain scan through the FSL-VBM pipeline (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLVBM).",1,0,0
10.1016/j.neuroimage.2022.119292,3dslicer.org,"For each set of scans, following acquisition, we imported the raw DICOM ﬁles into 3D Slicer 4 (www.3Dslicer.org) and converted each into NifTI format (Fedorov et al., 2012 ; Kikinis et al., 2014) for use with the various pro-2 M.M.",1,0,0
10.1016/j.neuroimage.2022.119292,chimpanzeebrain.org,"The MRI data that support the ﬁndings of this study are available from the National Chimpanzee Brain Resource at https://www.chimpanzeebrain.org., Data availability statement Data are available from the National Chimpanzee Brain Resource at https://www.chimpanzeebrain.org.",0,1,0
10.1016/j.neuroimage.2021.118865,nitrc.org/projects/gretna,"We provided graph analysis metrics for the full connectome: in particular we employed the GRETNA (GRaph thEoreTical Network Analysis) toolbox (http://www.nitrc.org/projects/gretna/) (Wang et al., 2015) in Matlab to compute global graph measures (assortativity, rich club, synchronization, hierarchy, clustering coeﬃcient, characteristic path length, gamma, lambda, sigma, global eﬃciency and local eﬃ-ciency) and, for each seed, nodal ones (betweenness centrality, degree centrality, local eﬃciency, clustering coeﬃcient, shortest path length, normalized participant coeﬃcient).",1,0,0
10.1016/j.neuroimage.2021.118865,nitrc.org/projects/brainstemnavig,"2.2b (i) Deﬁning seed and target regions for 2D connectome genera-tion: As seed regions, we used the labels of 18 brainstem nuclei (13 bilateral and 5 unilateral, for a total of 31 seed regions) involved in arousal and motor function (Brainstem Navigator toolkit v0.9 devel-oped by this group (https://www.nitrc.org/projects/brainstemnavig/) (Bianciardi et al., 2018 , 2015 ; Garcia-Gomar et al., 2021 ; Singh et al., 2021), see also Table 1: • Serotonergic nuclei of median raphe (MnR), paramedian raphe (PMnR) and dorsal raphe (DR) (Parvizi and Damasio, 2001 ; Saper et al., 2010 , 2001); • Mesolimbic dopaminergic nuclei of Substantia Nigra-subregion1 (SN1, compatible with pars reticulata), Substantia Nigra-subregion2 (SN2, compatible with pars compacta), (Datta et al., 1991 ; Lima et al., 2008 , 2007 ; Olszewski and Baxter, 2014), Caudal-Rostral Linear Raphe (CLi-RLi), Periaqueductal gray (PAG); • Meso-pontine reticular formation nuclei of (Moruzzi and Magoun, 1949 ; Parvizi and Damasio, 2001) mesencephalic retic-ular formation (mRt), cuneiform nucleus (CnF), isthmic reticular formation (isRt), and pontine reticular formation (PnO-PnC); • Noradrenergic nuclei of locus coeruleus (LC) (Parvizi and Dama-sio, 2001 ; Saper et al., 2010 , 2001); • Cholinergic nuclei of pedunculotegmental (PTg) and Laterodorsal Tegmental Nucleus-Central Gray of the rhombencephalon (LDTg-CGPn) (Parvizi and Damasio, 2001 ; Saper et al., 2010 , 2001) and • Motor function nuclei of Red nucleus-subregion1 (RN1), Red Nucleus-subregion2 (RN2), Subcoeruleus nucleus (SubC), Inferior Olivary Nucleus (ION) (Merel et al., 2019)., As targets, we used the 31 seeds as well as other 15 brainstem nuclei labels (12 bilateral and 3 unilateral, for a total of 27 target regions, Brainstem Navigator toolkit v0.9 developed by this group (https://www.nitrc.org/projects/brainstemnavig/) (Bianciardi et al., 2018 , 2016 ; Garcia-Gomar et al., 2021 ; García-Gomar et al., 2019 ; Singh et al., 2021 , 2019), namely: Superior Colliculus (SC), Inferior Col-liculus (IC), Ventral Tegmental Area-Parabrachial Pigmented Nucleus (VTA-PBP), Microcellular Tegmental Nucleus-Prabigeminal nucleus (MiTg-PBG), Lateral Parabrachial Nucleus (LPB), Medial Parabrachial Nucleus (MPB), Vestibular nuclei complex (Ve), Parvocellular Reticu-lar nucleus Alpha (PCRtA), Superior Olivary Complex (SOC), Superior Medullary Reticular formation (sMRt), Viscero-Sensory Motor nuclei complex (VSM), Inferior Medullary Reticular formation (iMRt), Raphe Magnus (RMg), Raphe Obscurus (ROb) and Raphe Pallidus (RPa).",1,0,0
10.1016/j.neuroimage.2022.119034,pls.rotman-baycrest.on.ca,All analyses were performed in Matlab or R using code from openly available software packages (MDMR anal-ysis: https://cran.r-project.org/web/packages/MDMR ; PLS analyses: http://pls.rotman-baycrest.on.ca).,1,0,0
10.1016/j.neuroimage.2022.119034,cran.r-project.org/web/packages/mdmr,All analyses were performed in Matlab or R using code from openly available software packages (MDMR anal-ysis: https://cran.r-project.org/web/packages/MDMR ; PLS analyses: http://pls.rotman-baycrest.on.ca).,1,0,0
10.1016/j.neuroimage.2021.118782,brain-connectivity-toolbox.net,"Graph mea-sures were derived using the Brain Connectivity Toolbox (Rubinov and Sporns, 2010) (http://www.brain-connectivity-toolbox.net).",1,0,0
10.1016/j.neuroimage.2021.118782,nitrc.org/projects/cleanline,The CleanLine adaptive ﬁltering technique was applied to reduce line noise and higher harmonics (https://www.nitrc.org/projects/cleanline).,1,0,0
10.1016/j.neuroimage.2021.118782,ﬁl.ion.ucl.ac.uk/spm,"For the EEG forward prob-lem, the segmentation of MRI anatomical images was performed using the Statistical Parametric Mapping (SPM) toolbox (Penny et al., 2011), version SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2021.118782,ru.nl/neuroimaging/ﬁeldtrip,"EEG source reconstruction was implemented using custom MATLAB code and routines from FieldTrip (Oostenveld et al., 2011) (http://www.ru.nl/neuroimaging/ﬁeldtrip).",1,0,0
10.1016/j.neuroimage.2021.118782,osf.io/c4gv6,"Both sensor-space and source-reconstructed EEG data, together with individual lead ﬁeld matrices, are available on the Open Science Framework (https://osf.io/c4gv6/).",0,1,0
10.1016/j.neuroimage.2021.118782,github.com/mattiapagnotta/nonparametricggc_toolbox,Codes for nonparametric spectral esti-mation using complex Morlet wavelet are also available on GitHub (https://github.com/mattiapagnotta/nonparametricGGC_toolbox).,1,0,0
10.1016/j.neuroimage.2022.119213,fsl.fmrib.ox.ac.uk/fsl/fslwiki/bet,The whole algorithm is available as 2 https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET accessed 2022-2-2.,1,0,0
10.1016/j.neuroimage.2022.119213,github.com/gift-surg/niftymic,"Thus, as fourth metric functional connectivity reproducibility measurements are used to evaluate the eﬀect from the motion correction and volumetric reconstruction on cortical connectivity patterns on the fetal brain surface. 3 available in release version 0.9 at https://github.com/gift-surg/NiftyMIC. 4 D., The code of the motion correction and volumetric reconstruction framework is available in release version 0.9 at https://github.com/gift-surg/NiftyMIC.",1,0,0
10.1016/j.neuroimage.2022.119213,crl.med.harvard.edu/research/fetal_brain_atlas/[accessed,"Despite the choice of this challenging setup, we can report for 1.5T as 4 http://crl.med.harvard.edu/research/fetal_brain_atlas/accessed 2022-2-2. 9 D.",0,0,1
10.1016/j.neuroimage.2022.119213,fsl.fmrib.ox.ac.uk/fsl/fslwiki/mcflirt,In 1 https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT accessed 2022-2-2. 3 D.,0,0,1
10.1016/j.neuroimage.2021.118796,surfer.nmr.mgh.harvard.edu/optseq,"The order of the trials and the intertrial interval were set to a pseudo-random optimal sequence generated by the program optseq2 (Dale et al., 1999 , https://surfer.nmr.mgh.harvard.edu/optseq).",1,0,0
10.1016/j.neuroimage.2021.118796,osf.io,Behavioral data and extracted percent signal change as well as R code to analyze these data sets will be uploaded to osf.io.,1,0,0
10.1016/j.neuroimage.2022.119656,github.com/mica-mni/brainstat,"Subﬁeld-isocortical FC measures were mapped using linear and mixed eﬀects models in BrainStat and thresholded at t > 20 to indicate highest connections (https://github.com/MICA-MNI/BrainStat) (Fig. 1 B)., Finally, the FC map of the isocortex to each hippocampal subﬁeld for the re-maining 709 participants was mapped using linear and mixed eﬀects models in BrainStat (https://github.com/MICA-MNI/BrainStat).",1,0,0
10.1016/j.neuroimage.2022.119656,github.com/cng-lab/cngopen/tree/main/hippocampus,All QA steps and analysis scripts used in this study are available at https://github.com/CNG-LAB/cngopen/tree/main/hippocampus.,1,0,0
10.1016/j.neuroimage.2022.119656,humanconnectome.org,"humanconnectome.org/) (Van Essen et al., 2013).",0,0,1
10.1016/j.neuroimage.2022.119656,solar-eclipse-genetics.org,"Heritability and Genetic Correlation Heritability and genetic correlation analysis were conducted with the Sequential Oligogenic Linkage Analysis Routines (SOLAR, v8.5.1, http://www.solar-eclipse-genetics.org/).",1,0,0
10.1016/j.neuroimage.2021.118760,nitrc.org/projects/conn,"Neuroimaging data pre-processing Pre-processing was performed using the CONN-fMRI functional con-nectivity toolbox, Version 18a (http://www.nitrc.org/projects/conn ; Whitﬁeld-Gabrieli and Nieto-Castanon, 2012), based on Statistical Para-metric Mapping 12 (http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2021.118760,neurovault.org/collections/9212,"Data and code availability statement Neuroimaging data at the group-level statistical t maps are openly available in Neurovault at https://neurovault.org/collections/9212/., Data and code availability statement Neuroimaging data at the group-level statistical t maps are openly available in Neurovault at https://neurovault.org/collections/9212/.",0,1,0
10.1016/j.neuroimage.2021.118760,nitrc.org/projects/bnv,"All ﬁgures were created using BrainNet Viewer (http://www.nitrc.org/projects/bnv/; Xia et al., 2013).",1,0,0
10.1016/j.neuroimage.2021.118760,osf.io/uyhra,"Semantic material and script for the task are accessible in the Open Sci-ence Framework at https://osf.io/uyhra/., Semantic material and script for the task are accessible in the Open Sci-ence Framework at https://osf.io/uyhra/.",0,0,1
10.1016/j.neuroimage.2021.118760,ﬁl.ion.ucl.ac.uk/spm,"Neuroimaging data pre-processing Pre-processing was performed using the CONN-fMRI functional con-nectivity toolbox, Version 18a (http://www.nitrc.org/projects/conn ; Whitﬁeld-Gabrieli and Nieto-Castanon, 2012), based on Statistical Para-metric Mapping 12 (http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119345,data.mendeley.com,Data/code availability statement The empirical data used for this paper are available in the public repository “Mendeley Data ”(https://data.mendeley.com/).,0,1,0
10.1016/j.neuroimage.2022.119345,cvcl.mit.edu/mm,"Most of the images have been used in our previous fMRI studies (Guidotti et al., 2020 ; Sestieri et al., 2014), and were originally selected from a large database (Konkle et al., 2010) (http://cvcl.mit.edu/MM).",0,1,0
10.1016/j.neuroimage.2022.119078,ac.uk/fsl/fslview/atlas-descriptions.html#ho,ac.uk/fsl/fslview/atlas-descriptions.html#ho) was used to deﬁne hippocampal ROIs.,1,0,0
10.1016/j.neuroimage.2022.119436,ﬁl.ion.ucl.ac.uk/spm,Data preprocessing The fMRI data were ﬁrstly preprocessed using the software pack-age SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm) with the following steps: realignment (correction for head motion-induced inter-volume displace-ment); spatial normalization to the Montreal Neurological Institute (MNI) space using the uniﬁed normalization-segmentation procedure via the structural T1 images; and spatial smoothing using a Gaussian kernel of 5-mm full-width at half-maximum (FWHM).,1,0,0
10.1016/j.neuroimage.2022.119436,atlas.brainnetome.org,"To test whether and how the individual identiﬁability varies across diﬀerent brain regions, we also divided the cerebrum into 246 re-gions using the Human Brainnetome Atlas (BN-Atlas) (Fan et al., 2016) (http://atlas.brainnetome.org) and obtained within-modality and cross-modality identiﬁcation accuracies at group level and at individual level for each region (i.e., regional identiﬁcation accuracies) by repeating the same individual identiﬁcation analyses described in Analysis 1 but using only the activation pattern within that region.",1,0,0
10.1016/j.neuroimage.2022.119366,github.com/benharvey/vistasoftaddons,Model parameters underlying all statistical analyses and re-sponse data for all model ﬁtting are publicly available at the following DOIs: -Timeseries data: https://doi.org/10.6084/m9.ﬁgshare.19849879 -Response Model parameters: https://doi.org/10.6084/m9.ﬁgshare. 19849264 The code that supports the ﬁndings of this study is available from the following repositories: -vistasoft (https://github.com/vistalab/vistasoft) -vistasoftAddOns (https://github.com/benharvey/vistasoftAddOns) -fMRI_preproc (https://github.com/MvaOosterhuis/fMRI_preproc) -AnalysisScript (https://github.com/MvaOosterhuis/Auditory Timing_Analysis) Declaration of Competing Interests The authors declare no competing interests.,1,0,0
10.1016/j.neuroimage.2022.119366,github.com/vistalab/vistasoft,Model parameters underlying all statistical analyses and re-sponse data for all model ﬁtting are publicly available at the following DOIs: -Timeseries data: https://doi.org/10.6084/m9.ﬁgshare.19849879 -Response Model parameters: https://doi.org/10.6084/m9.ﬁgshare. 19849264 The code that supports the ﬁndings of this study is available from the following repositories: -vistasoft (https://github.com/vistalab/vistasoft) -vistasoftAddOns (https://github.com/benharvey/vistasoftAddOns) -fMRI_preproc (https://github.com/MvaOosterhuis/fMRI_preproc) -AnalysisScript (https://github.com/MvaOosterhuis/Auditory Timing_Analysis) Declaration of Competing Interests The authors declare no competing interests.,1,0,0
10.1016/j.neuroimage.2022.119366,github.com/mvaoosterhuis/fmri_preproc,Model parameters underlying all statistical analyses and re-sponse data for all model ﬁtting are publicly available at the following DOIs: -Timeseries data: https://doi.org/10.6084/m9.ﬁgshare.19849879 -Response Model parameters: https://doi.org/10.6084/m9.ﬁgshare. 19849264 The code that supports the ﬁndings of this study is available from the following repositories: -vistasoft (https://github.com/vistalab/vistasoft) -vistasoftAddOns (https://github.com/benharvey/vistasoftAddOns) -fMRI_preproc (https://github.com/MvaOosterhuis/fMRI_preproc) -AnalysisScript (https://github.com/MvaOosterhuis/Auditory Timing_Analysis) Declaration of Competing Interests The authors declare no competing interests.,1,0,0
10.1016/j.neuroimage.2022.119366,github.com/mvaoosterhuis/auditory,Model parameters underlying all statistical analyses and re-sponse data for all model ﬁtting are publicly available at the following DOIs: -Timeseries data: https://doi.org/10.6084/m9.ﬁgshare.19849879 -Response Model parameters: https://doi.org/10.6084/m9.ﬁgshare. 19849264 The code that supports the ﬁndings of this study is available from the following repositories: -vistasoft (https://github.com/vistalab/vistasoft) -vistasoftAddOns (https://github.com/benharvey/vistasoftAddOns) -fMRI_preproc (https://github.com/MvaOosterhuis/fMRI_preproc) -AnalysisScript (https://github.com/MvaOosterhuis/Auditory Timing_Analysis) Declaration of Competing Interests The authors declare no competing interests.,1,0,0
10.1016/j.neuroimage.2022.119366,afni.nimh.nih.gov,"Preprocessing Co-registration of functional data to the high-resolution anatomi-cal space was performed using a custom pipeline (Paul et al., 2022) in AFNI (afni.nimh.nih.gov)(Cox, 1996).",1,0,0
10.1016/j.neuroimage.2022.119366,github.com/vistalab/vistasoft,"We then imported these data into Vistasoft’s mrVista framework (github.com/vistalab/vistasoft) for analysis and model ﬁtting., Model parameters underlying all statistical analyses and re-sponse data for all model ﬁtting are publicly available at the following DOIs: -Timeseries data: https://doi.org/10.6084/m9.ﬁgshare.19849879 -Response Model parameters: https://doi.org/10.6084/m9.ﬁgshare. 19849264 The code that supports the ﬁndings of this study is available from the following repositories: -vistasoft (https://github.com/vistalab/vistasoft) -vistasoftAddOns (https://github.com/benharvey/vistasoftAddOns) -fMRI_preproc (https://github.com/MvaOosterhuis/fMRI_preproc) -AnalysisScript (https://github.com/MvaOosterhuis/Auditory Timing_Analysis) Declaration of Competing Interests The authors declare no competing interests.",1,0,0
10.1016/j.neuroimage.2022.118927,memory.psych.upenn.edu/ram,"iEEG recordings of these pa-tients were downloaded from a UPENN-RAM consortium hosted data sharing archive (http://memory.psych.upenn.edu/RAM)., Menon NeuroImage 250 (2022) 118927 Data and code availability statement iEEG recordings used in this study can be downloaded from a UPENN-RAM consortium hosted data sharing archive (http://memory.psych.upenn.edu/RAM).",0,1,0
10.1016/j.neuroimage.2022.118927,memory.psych.upenn.edu/word_pools,"Words were selected at random, without replacement, from a pool of high frequency English nouns (http://memory.psych.upenn.edu/Word_Pools).",0,1,0
10.1016/j.neuroimage.2022.118927,github.com/scsnl/das_neuroimage_2022,MATLAB codes used in this study are available through our lab GitHub (https://github.com/scsnl/Das_NeuroImage_2022).,1,0,0
10.1016/j.neuroimage.2022.119711,gaain.org,Data and code availability Data from the core WRAP protocol study in partnership with the WADRC are accessible to qualiﬁed researchers via an online request form and data use agreement which can be linked from the Global Alzheimer’s Association Interactive Network web site (www.gaain.org).,0,1,0
10.1016/j.neuroimage.2022.119711,github.com/uwmri/ﬂow_recon,"The motion correction and reconstruction code can be found at: https://github.com/uwmri/ﬂow_recon., The motion correction and reconstruction code can be found at: https://github.com/uwmri/ﬂow_recon.",1,0,0
10.1016/j.neuroimage.2022.119405,imaging.mrc-cbu.cam.ac.uk/meg/maxﬁlter,"MEG data preprocessing and analysis Oﬄine data was visually inspected to identify noisy channels, prior to application of the MaxMove function of the Elekta Maxﬁlter software (http://imaging.mrc-cbu.cam.ac.uk/meg/Maxﬁlter).",1,0,0
10.1016/j.neuroimage.2022.119405,ﬁl.ion.ucl.ac.uk/spm/software/spm12,fMRI data preprocessing and analysis Analysis was performed in SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).,1,0,0
10.1016/j.neuroimage.2022.119405,osf.io/8qfw4/?view_only,Data availability statement Thresholds t-statistic maps for within modality fMRI category decoding and MEG-fMRI analysis for each time window are freely available on the Open Science Framework (OSF) at the following URL: https://osf.io/8qfw4/?view_only = 3d50679935434bdf84a582ad7db 96231.,0,1,0
10.1016/j.neuroimage.2022.119575,humanconnectome.org/pages/viewpage.action?pageid,"humanconnectome.org/pages/viewpage.action?pageId = 88901591) and visual checks were used to exclude data of poor quality, with major processing issues and non-usable task data.",0,0,1
10.1016/j.neuroimage.2022.119575,cran.r-project.org/web/packages/interactions/index.html,"Interaction plots and simple slope analysis were generated with the interactions package (v.1.1.5; https://cran.r-project.org/web/packages/interactions/index.html) using the inter-act_plot and sim_slope functions, respectively.",1,0,0
10.1016/j.neuroimage.2022.119575,mdt-toolbox.readthedocs.io,"Using the downloaded, preprocessed dMRI data, the NODDI model was ﬁtted using the MDT toolbox v.0.20 (Harms et al., 2017) (www.mdt-toolbox.readthedocs.io) batchﬁt algorithm (Fig.",1,0,0
10.1016/j.neuroimage.2022.119575,github.com/cschifani/hcp_noddivsfmri,"Code is made available via github (https://github.com/cschifani/HCP_NODDIvsfMRI)., Code is made available via github (https://github.com/cschifani/HCP_NODDIvsfMRI).",1,0,0
10.1016/j.neuroimage.2022.119575,cran.r-project.org/web/packages/lmperm/index.html,tation testing with the lmPerm package for R (v.2.1.0; https://cran.r-project.org/web/packages/lmPerm/index.html) using the lmp function with ‘Prob’ permutation option.,1,0,0
10.1016/j.neuroimage.2022.119575,humanconnectome.org/software/connectome-workbench,humanconnectome.org/software/connectome-workbench) were used for visualization.,1,0,0
10.1016/j.neuroimage.2022.119575,surfer.nmr.mgh.harvard.edu,"The native space images were used to generate individual white and pial surfaces (Glasser et al., 2013) using the FreeSurfer (https://surfer.nmr.mgh.harvard.edu/) and HCP pipelines (https://github.com/Washington-University/Pipelines).",1,0,0
10.1016/j.neuroimage.2022.119575,fmrib.ox.ac.uk/fsl,Statistical analyses were performed in FSL (www.fmrib.ox.ac.uk/fsl).,1,0,0
10.1016/j.neuroimage.2022.119575,humanconnectome.org/display/publicdata/hcp,humanconnectome.org/display/PublicData/HCP + Data + Release + Updates%3A + Known + Issues + and + Planned + ﬁxes and https://wiki.,0,0,1
10.1016/j.neuroimage.2022.119575,cran.r-project.org/web/packages/lm.beta/index,The lm.beta package (v.1.5–1; https://cran.r-project.org/web/packages/lm.beta/index.,1,0,0
10.1016/j.neuroimage.2022.119575,cran.r-project.org/web/packages/ggplot2/index,The ggplot2 package (v.3.3.2; https://cran.r-project.org/web/packages/ggplot2/index.,1,0,0
10.1016/j.neuroimage.2022.119575,mig.cs.ucl.ac.uk,"Vali-dated on an internal sample of data, NODDI ﬁtting using the MDT tool-box gave the same results as with the original NODDI matlab toolbox (http://mig.cs.ucl.ac.uk/) (data not shown).",1,0,0
10.1016/j.neuroimage.2022.119575,github.com/washington-university/pipelines,"The native space images were used to generate individual white and pial surfaces (Glasser et al., 2013) using the FreeSurfer (https://surfer.nmr.mgh.harvard.edu/) and HCP pipelines (https://github.com/Washington-University/Pipelines).",1,0,0
10.1016/j.neuroimage.2022.119575,db.humanconnectome.org,"Participants Structural, multishell dMRI and task fMRI data from young, healthy adults were obtained from the WU-Minn Human Connectome Project (Barch et al., 2013 ; Glasser et al., 2016b ; Van Essen et al., 2013), downloaded from ConnectomeDB as part of the S900 release (http://db.humanconnectome.org)., Data and code availability statement Data were provided by the Human Connectome Project, WU-Minn Consortium, downloaded from ConnectomeDB as part of the S900 re-lease (http://db.humanconnectome.org).",0,1,0
10.1016/j.neuroimage.2022.119241,nitrc.org/projects/homer2,The code used to analyse the fNIRS data (HomER2) is available at  https://www.nitrc.org/projects/homer2  and a MATLAB script of the pre-processing stream used in this study is available on OSF  https://osf.io/mv47n/.,1,0,0
10.1016/j.neuroimage.2022.119241,osf.io/m58qs,"In the second test session, participants completed a blocked version of the ECITT that was adapted for use with fNIRS, see Fig. 3 for exam-ple trial sequences (a video of an infant completing the task is available at https://osf.io/m58qs/).",1,0,0
10.1016/j.neuroimage.2022.119241,osf.io/qs4h8,We justify this decision in full detail in SM 7b and in the pre-registration (https://osf.io/qs4h8/).,0,0,1
10.1016/j.neuroimage.2022.119241,osf.io/mpt5g,"fNIRS group-level analyses Using a custom MATLAB script (https://osf.io/mpt5g/), the block averaged haemoglobin concentration data were organised such that there was a separate variable for each channel, time-bin, chromophore, and block type (control, experimental, baseline), following the approach taken by de Klerk et al.",1,0,0
10.1016/j.neuroimage.2022.119241,osf.io/mv47n,"A MATLAB version of this ﬁgure is available on OSF (https://osf.io/mv47n/)., Similar t -statistic images of signiﬁ-cant haemoglobin concentration change by block type (compared to baseline) are presented in SM 7a (Supplementary Figures 5 and 6) and MATLAB versions of the t -statistic ﬁgures are available on OSF (https://osf.io/mv47n/)., Data and code availability statement The data that support the ﬁndings of this study and the custom MATLAB scripts used to analyse the fNIRS data is available on the Open Science Framework (OSF) website  https://osf.io/mv47n/ under a CC-By Attribution 4.0 International license (please cite this article if using any of these materials)., The code used to analyse the fNIRS data (HomER2) is available at  https://www.nitrc.org/projects/homer2  and a MATLAB script of the pre-processing stream used in this study is available on OSF  https://osf.io/mv47n/., The reconstructed images presented in this paper are also available as a MATLAB ﬁgure in the OSF project associated with this paper  https://osf.io/mv47n/.",0,1,0
10.1016/j.neuroimage.2022.119241,toastplusplus.org,"To model the transport of near-infrared light through the head model, TOAST ++ ((Schweiger and Arridge, 2014), see http://toastplusplus.org) was employed to produce a forward model for each wavelength.",1,0,0
10.1016/j.neuroimage.2022.119241,github.com/dot-hub,"Data preparation, meshing, forward modelling, and reconstruction were facilitated by the DOT-HUB Tool-box (www.github.com/DOT-HUB)., The code used to pro-duce the head model and reconstruct images has been developed and released via  http://www.github.com/DOT-HUB and  https://github.com/liamhywelcj/ReconstructionTenMonthCohortData .",1,0,0
10.1016/j.neuroimage.2022.119241,github.com/dot-hub,The code used to pro-duce the head model and reconstruct images has been developed and released via  http://www.github.com/DOT-HUB and  https://github.com/liamhywelcj/ReconstructionTenMonthCohortData .,1,0,0
10.1016/j.neuroimage.2022.119241,github.com/liamhywelcj/reconstructiontenmonthcohortdata,The code used to pro-duce the head model and reconstruct images has been developed and released via  http://www.github.com/DOT-HUB and  https://github.com/liamhywelcj/ReconstructionTenMonthCohortData .,1,0,0
10.1016/j.neuroimage.2022.119241,osf.io/qs4h8,"After this, in a set of pre-registered analyses (https://osf.io/qs4h8) using the pre-identiﬁed channels and time windows, we investigated whether neural activation was associ-2 A., NeuroImage 257 (2022) 119241 ysis plan for the individual diﬀerences analyses (https://osf.io/qs4h8)., We justify this decision in full detail in SM 7b and in the pre-registration (https://osf.io/qs4h8/).",0,0,1
10.1016/j.neuroimage.2022.118901,2.2.1.1,More speciﬁcally: 2.2.1.1.,0,0,1
10.1016/j.neuroimage.2022.118901,2.2.1.2,2.2.1.2.,0,0,1
10.1016/j.neuroimage.2022.118901,2.5.1.2,"2.5.1.2., The shape of the 𝜇𝑣𝑎𝑠𝑐 (𝑡) curve was then manipulated while holding the AUC constant, by using the same 𝜇𝑣𝑎𝑠𝑐 (𝑡) ﬁt as in 2.5.1.2 above., Sets of three decay constants, one per each decreasing exponential, were randomly generated in each simu-lation and combined with the original corresponding initial magnitude values from 2.5.1.2 above.",1,0,0
10.1016/j.neuroimage.2022.118901,2.2.1.4,2.2.1.4.,0,0,1
10.1016/j.neuroimage.2022.118901,2.2.1.3,NeuroImage 249 (2022) 118901 2.2.1.3.,0,0,1
10.1016/j.neuroimage.2022.118901,2.5.1.3,2.5.1.3.,0,0,1
10.1016/j.neuroimage.2022.118901,github.com/elizabeth-bartlett/stare,"Discussion We present the theory and initial validation with human  18 FFDG scans, for a new, publicly-available PET quantiﬁcation approach –STARE –that quantiﬁes the net inﬂux rate (K i) of radiotracers with ir-reversible kinetics (https://github.com/elizabeth-bartlett/STARE)., The newly developed code for STARE is publicly available (https://github.com/elizabeth-bartlett/STARE).",1,0,0
10.1016/j.neuroimage.2022.118901,2.5.1.1,Eﬀects of μvasc (t) on STARE performance 2.5.1.1.,0,0,1
10.1016/j.neuroimage.2022.119307,mne.tools,"Data processing pipelines for the EEG data used the freely available FieldTrip (Oostenveld et al., 2011 , https://www.ﬁeldtriptoolbox.org/) and MNE toolboxes (Gramfort et al., 2014 , https://mne.tools/).",0,1,0
10.1016/j.neuroimage.2022.119307,docs.scipy.org/doc,"Oﬄine, EEG data were ﬁltered with a zero-phase bandpass ﬁnite im-pulse response (FIR) ﬁlter between 1 Hz and 100 Hz using the window design method (“ﬁrwin ”i n SciPy https://docs.scipy.org/doc/; Han-ning window; 1 Hz lower transition bandwidth; 25 Hz upper transition bandwidth; 3.3 s ﬁlter length).",1,0,0
10.1016/j.neuroimage.2022.119666,letswave.cn,"Software Matlab 2018b & Letswave7 (Letswave.cn) Band-pass ﬁltering Butterworth ﬁlter, 0.01-200 Hz, 4 th order, 24 dB/octave, zero-phase Notch ﬁltering Butterworth ﬁlter, 49-51 Hz, 4 th order, 24 dB/octave, zero-phase Channel interpolation Bad channels were identiﬁed manually and interpolated with the mean value of the three surrounding channels Re-reference Re-reference to the mean value of TP9 and TP10 Artifacts removal by ICA Eye movement related ICA components were identiﬁed by visual inspection of their scalp topographies, time courses, and spectra.",1,0,0
10.1016/j.neuroimage.2022.119666,kaggle.com/competitions/eeg-biometric-competition/data,"Data and code availability The public dataset with the code of the baseline methods and an example submission ﬁle are available at • Kaggle: https://www.kaggle.com/competitions/eeg-biometric-competition/data., An example code and the submission ﬁle with the method of PSD + L2 with were provided on the website of Kaggle (https://www.kaggle.com/competitions/eeg-biometric-competition/data)., • Kaggle: https://www.kaggle.com/competitions/eeg-biometric-competition/data.",0,1,0
10.1016/j.neuroimage.2022.119666,kaggle.com/competitions/eeg-biometric-competition,"The link to the plat-form for our competition are as follows, • Kaggle: https://www.kaggle.com/competitions/eeg-biometric-competition 3.5.4., Data and code availability The public dataset with the code of the baseline methods and an example submission ﬁle are available at • Kaggle: https://www.kaggle.com/competitions/eeg-biometric-competition/data., An example code and the submission ﬁle with the method of PSD + L2 with were provided on the website of Kaggle (https://www.kaggle.com/competitions/eeg-biometric-competition/data)., • Kaggle: https://www.kaggle.com/competitions/eeg-biometric-competition/data., The detail description about the EEG-based biometric competition can be referred to • Kaggle: https://www.kaggle.com/competitions/eeg-biometric-competition.",0,1,0
10.1016/j.neuroimage.2022.119666,psychtoolbox.org,"For the other tasks, A 24.5-inch screen (1920 ∗ 1080) with a 240-Hz refreshing rate (Alienware AW2518H, Miami, USA) was used to present the visual stimuli or cues by programmed using Psychtoolbox-3 (http://psychtoolbox.org/) in Matlab.",1,0,0
10.1016/j.neuroimage.2022.119675,surfer.nmr.mgh.harvard.edu,The masks of primary motor (M1) and sensory (S1) cortices were generated by using FreeSurfer (http://surfer.nmr.mgh.harvard.edu/) as shown in Fig. 2 C.,1,0,0
10.1016/j.neuroimage.2022.119675,fmrib.ox.ac.uk/fsl,"The distortion of the EPI images was corrected with the B 0 ﬁeld map from the FLASH sequence, performed with PRELUDE and FUGUE from the FSL package (http://www.fmrib.ox.ac.uk/fsl).",1,0,0
10.1016/j.neuroimage.2022.119086,3.1.9.7,"An a priori power analysis was conducted with G ∗ Power 3.1.9.7 (F tests, Linear multiple regression, Fixed model, R 2 deviation from zero; Faul et al., 2007) based on the criterion of 𝛼= 0.005; eﬀect size f 2 = 0.25.",1,0,0
10.1016/j.neuroimage.2022.119600,github.com/ekaden/smt,"In our case, a ma-1 https://github.com/ekaden/smt trix entry reﬂects the connectivity strength between two regions, de-ﬁned as the sum of the bundles cross-sectional area each multiplied by the corresponding weight estimated by COMMIT from the ﬁber den-sity map.",0,0,1
10.1016/j.neuroimage.2022.118906,github.com/quantitative-mri-and-in-vivo-histology/ls_axon_segmentation,Code and data availability The source code and training data used in this study will be made publicly available upon publication of this study on https://github.com/quantitative-mri-and-in-vivo-histology/ls_axon_segmentation.,0,1,0
10.1016/j.neuroimage.2022.119671,mrtrix.org,"DWI data was pre-processed using the MRtrix3 package (Tournier et al., 2019) (https://www.mrtrix.org/).",1,0,0
10.1016/j.neuroimage.2022.119671,netneurolab.github.io/neuromaps,"Volumetric PET receptor images can be found on neuromaps (https://netneurolab.github.io/neuromaps/(Markello et al., 2022)) and at https://github.com/netneurolab/hansen_receptors (Hansen et al., 2021)., (2021) and is available in neuromaps (https://netneurolab.github.io/neuromaps/) (Markello et al., 2022).",0,0,1
10.1016/j.neuroimage.2022.119671,github.com/netneurolab/abagen,"All processing was performed using the abagen toolbox (https://github.com/netneurolab/abagen (Markello et al., 2021)).",1,0,0
10.1016/j.neuroimage.2022.119671,pantherdb.org,"To test this, we use the Panther classiﬁcation system (https://pantherdb.org/) to extract lists of genes coding for re-ceptors within a protein pathway (Mi et al., 2012; Patania et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119671,netneurolab.github.io/neuromaps/(markello,"Volumetric PET receptor images can be found on neuromaps (https://netneurolab.github.io/neuromaps/(Markello et al., 2022)) and at https://github.com/netneurolab/hansen_receptors (Hansen et al., 2021).",0,0,1
10.1016/j.neuroimage.2022.119671,github.com/lts5/cmp,"We created a surface-based representation of the parcellation on the FreeSurfer fsaverage left hemi-sphere surface, via ﬁles from the Connectome Mapper toolkit (https://github.com/LTS5/cmp).",1,0,0
10.1016/j.neuroimage.2022.119671,github.com/netneurolab/hansen_gene-receptor,Methods Data and code availability All code and data used to perform the analyses can be found at https://github.com/netneurolab/hansen_gene-receptor.,0,1,0
10.1016/j.neuroimage.2022.119671,human.brain-map.org,"Microarray gene expression Regional microarray expression data were obtained from six post-mortem brains provided by the Allen Human Brain Atlas (AHBA; http://human.brain-map.org/) (Hawrylycz et al., 2012).",0,1,0
10.1016/j.neuroimage.2022.119671,human.brain-map.org/(hawrylycz,"The Allen Human Brain Atlas is available at https://human.brain-map.org/(Hawrylycz et al., 2012).",0,0,1
10.1016/j.neuroimage.2022.119671,github.com/chrisﬁlo/alleninf,"Next, samples were assigned to brain regions using MNI coor-dinates generated via non-linear registrations (https://github.com/chrisﬁlo/alleninf) by ﬁnding the nearest region, up to 2 mm away.",1,0,0
10.1016/j.neuroimage.2022.119671,github.com/netneurolab/hansen_receptors,"Volumetric PET receptor images can be found on neuromaps (https://netneurolab.github.io/neuromaps/(Markello et al., 2022)) and at https://github.com/netneurolab/hansen_receptors (Hansen et al., 2021).",0,0,1
10.1016/j.neuroimage.2022.119671,db.humanconnectome.org,"Structural connectivity data is collected from the Human Connectome Project, available at https://db.humanconnectome.org/.",0,1,0
10.1016/j.neuroimage.2022.119648,bachlab.github.io/pspm,"Physiological data analysis To analyze skin conductance data we used PsPM 4.3.0 soft-ware (https://bachlab.github.io/PsPM/) running under MATLAB 2018b (MathWorks, Natick, MA, USA).",1,0,0
10.1016/j.neuroimage.2022.119648,github.com/nencki-lobi/emocon-mri,Code replicating analyses re-ported here is available at https://github.com/nencki-lobi/emocon-mri.,1,0,0
10.1016/j.neuroimage.2022.119648,neurovault.org/collections/rsllsftq,"Unthresholded statistical maps from the reported comparisons are also available at Neurovault, https://neurovault.org/collections/RSLLSFTQ/.",0,0,1
10.1016/j.neuroimage.2022.119648,fmriprep.org/en/latest/workﬂows.html,"For more details on the fMRIprep pipeline, see fMRIPrep’s documentation at https://fmriprep.org/en/latest/workﬂows.html.",0,0,1
10.1016/j.neuroimage.2022.119648,osf.io/g3wkq,Data availability Relevant data are stored in an OSF repository and are available at https://osf.io/g3wkq/.,0,1,0
10.1016/j.neuroimage.2022.118987,github.com/vbindex,"(2020) introduce the Vogt-Bailey (VB) index, which quantiﬁes across the cortical man-tle on how sharply inter-areal boundaries are deﬁned and released open tools for VB index calculation and gradient mapping (https://github.com/VBIndex/).",1,0,0
10.1016/j.neuroimage.2022.119497,nitrc.org/projects/artifact_detect,"(For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.) each participant functional image parameters from the alignment pro-cedure (along with global signal intensity) were checked manually us-ing the toolbox ART (http://www.nitrc.org/projects/artifact_detect/).",1,0,0
10.1016/j.neuroimage.2022.119497,surfer.nmr.mgh.harvard.edu,"For the HPC mask, FreeSurfer’s recon-all function (https://surfer.nmr.mgh.harvard.edu/; Fischl, 2012) was used to extract subject-speciﬁc anatomical masks for all 33 partic-ipants.",1,0,0
10.1016/j.neuroimage.2022.119611,prsice.info,"mgh.harvard.edu/purcell/plink/); PRSice-2: https://www.prsice.info/; GCTA v.1.93: https://yanglab.westlake.edu.cn/software/gcta/, statistical analysis and data preprocessing R v.3.6.3: https://www.r-project.org/.",1,0,0
10.1016/j.neuroimage.2022.119611,nda.nih.gov/abcd,DOIs can be found at https://nda.nih.gov/abcd.,0,0,1
10.1016/j.neuroimage.2022.119611,people.virginia.edu/~wc9c/king/manual.html,"2.24 (http://people.virginia.edu/~wc9c/KING/manual.html), and only one individual from a family of twins and ﬁrst siblings was kept for further analyses, with the reasoning that this step will exclude most family relations likely to share close environments without substantially reducing the sample size.",0,0,1
10.1016/j.neuroimage.2022.119611,osf.io/z83mw/?view_only,Scripts for data handling will be made available upon publication at Open Science Framework: https://osf.io/z83mw/?view_only = 1fc11f1d2b8449b6ba0d36f589a34b6b 9 S.,0,1,0
10.1016/j.neuroimage.2022.119611,abcdstudy.org/consortium_members,A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.,0,0,1
10.1016/j.neuroimage.2022.119611,pngu.mgh.harvard.edu/purcell/plink,"Post-imputation QC was performed us-ing plink (Purcell et al., 2007); http://www.pngu.mgh.harvard.edu/purcell/plink/) included the following steps: exclusion of low imputation scores and duplicates, genotype and individual missingness (–geno 0.1 –mind 0.1), ﬁlter out low minor allele frequencies (–maf 0.05), and deviation from Hardy-Weinberg equilibrium (–hwe 1e-6).",1,0,0
10.1016/j.neuroimage.2022.119611,r-project.org/],"mgh.harvard.edu/purcell/plink/); PRSice-2: https://www.prsice.info/; GCTA v.1.93: https://yanglab.westlake.edu.cn/software/gcta/, statistical analysis and data preprocessing R v.3.6.3: https://www.r-project.org/.",1,0,0
10.1016/j.neuroimage.2022.119611,yanglab.westlake.edu.cn/software/gcta/],"mgh.harvard.edu/purcell/plink/); PRSice-2: https://www.prsice.info/; GCTA v.1.93: https://yanglab.westlake.edu.cn/software/gcta/, statistical analysis and data preprocessing R v.3.6.3: https://www.r-project.org/.",1,0,0
10.1016/j.neuroimage.2022.119611,abcdstudy.org,"Data used in the preparation of this arti-cle were obtained from the Adolescent Brain Cognitive Development SM (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA)., A full list of supporters is available at https://abcdstudy.org/federal-partners.html., A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.",0,1,0
10.1016/j.neuroimage.2022.119611,mgh.harvard.edu/purcell/plink,"Post-imputation QC was performed us-ing plink (Purcell et al., 2007); http://www.pngu.mgh.harvard.edu/purcell/plink/) included the following steps: exclusion of low imputation scores and duplicates, genotype and individual missingness (–geno 0.1 –mind 0.1), ﬁlter out low minor allele frequencies (–maf 0.05), and deviation from Hardy-Weinberg equilibrium (–hwe 1e-6)., mgh.harvard.edu/purcell/plink/); PRSice-2: https://www.prsice.info/; GCTA v.1.93: https://yanglab.westlake.edu.cn/software/gcta/, statistical analysis and data preprocessing R v.3.6.3: https://www.r-project.org/.",1,0,0
10.1016/j.neuroimage.2022.119611,abcdstudy.org/federal-partners.html,A full list of supporters is available at https://abcdstudy.org/federal-partners.html.,0,0,1
10.1016/j.neuroimage.2022.119611,umich.edu/wiki/regions_of_high_linkage_disequilibrium_(ld,umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD).,0,0,1
10.1016/j.neuroimage.2022.119486,brainlife.io/pub/60ddea776f0f540a79ca53d8,"Benchmark publication We have published a release of the DBB benchmark on the cloud computing platform brainlife.io at https://brainlife.io/pub/60ddea776f0f540a79ca53d8., We have published a ﬁrst release of the DBB benchmark on the cloud computing platform brainlife.io at https://brainlife.io/pub/60ddea776f0f540a79ca53d8.",0,0,1
10.1016/j.neuroimage.2022.119486,github.com/fbk-nilab/bl_app_dbb_preproc_t1w,"Name application DOI GitHub Branch DBB_preprocessing_t1w https://doi.org/10.25663/brainlife.app.535 https://GitHub.com/FBK-NILab/bl_app_dbb_preproc_t1w 1.5.2 DBB_ANTsCorThickSeg https://doi.org/10.25663/brainlife.app.541 https://GitHub.com/FBK-NILab/bl_app_dbb_ANTsCTSeg 2.5.5 DBB_DisSeg https://doi.org/10.25663/brainlife.app.548 https://GitHub.com/FBK-NILab/bl_app_dbb_DisSeg 1.4.2 DBB_DiceScore https://doi.org/10.25663/brainlife.app.579 https://GitHub.com/FBK-NILab/bl_app_dbb_DiceScore 1.2.2 DBB_ANTsCortThickSeg We implemented the App to per-form the brain tissue segmentation phase of the script antsCroticalThickness.sh as described in Section4.1 , in a custom version where we implemented the input to explicitly provide the brain mask.",1,0,0
10.1016/j.neuroimage.2022.119486,research.cchmc.org/c-mind,"For example, EMEDEA-PED has a better cov-erage of the upper part of paediatric range while C-MIND in the lower part. 2 https://research.cchmc.org/c-mind. 3 https://emedea.it/medea/en/. 4 G., In the case of NIH-PED and C-MIND subjects, we don’t provide the T1-w images because we forward them to their oﬃcial repositories, where the download is available after the registration and the acceptance of the terms of use (NIH-PED: http://pediatricmri.nih.gov ; C-MIND: https://research.cchmc.org/c-mind)., A listing of the participating sites and a complete listing of the study investigators can be found at https://research.cchmc.org/c-mind.",0,0,1
10.1016/j.neuroimage.2022.119486,brainlife.io/docs/cli/install,"Brain-life CLI could be installed on UNIX/Linux-based system following the instruction reported in https://brainlife.io/docs/cli/install/., Brainlife CLI could be installed on UNIX/Linux-based system following the instruction reported in https://brainlife.io/docs/cli/install/.",1,0,0
10.1016/j.neuroimage.2022.119486,brainlife.io,"Benchmark publication We have published a release of the DBB benchmark on the cloud computing platform brainlife.io at https://brainlife.io/pub/60ddea776f0f540a79ca53d8., In addition to the distribution of data, brainlife.io supports the open publication of the code released as brain-life App., The brainlife.io platform also provides the tools for the members of the community to contribute to the improvement of the dataset and enable users to report errors and inaccuracies in the data., Overview of the organization of ﬁles in the publication according to the datatype deﬁnition adopted in brainlife.io., Datatype Datatype tag Files Data description Source neuro/parcellation/volume parc.nii.gz label.json Segmentation image Description of each label EMEDEA-PED C-MIND NIH-PED neuro/anat/t1w t1.nii.gz T1-w preprocessed and defaced EMDEA-PED neuro/mask mask.nii.gz Brain Mask in preprocessed space EMEDEA-PED C-MIND NIH-PED neuro/mask raw mask.nii.gz Brain Mask in rawspace C-MIND NIH-PED neuro/transform linear aﬃne.txt Reorientation Matrix (APCP) C-MIND NIH-PED The organization of the ﬁles follows the deﬁnition of datatypes adopted by brainlife.io (see the full documentation at https://brainlife.io/docs/user/datatypes/)., Usage notes Data can be directly downloaded using the web interface of the brainlife.io platform or through the command-line interface (CLI)., Brain-life CLI could be installed on UNIX/Linux-based system following the instruction reported in https://brainlife.io/docs/cli/install/., The sub-folder is named using a preﬁx dt-followed by the brainlife.io Datatype (e.g., neuro_anat_t1w), then combined with the pre-ﬁx tag-followed by the brainlife.io datatype tag (if present) and ﬁnally the preﬁx id-followed by the id, the unique identiﬁer of the ﬁle., The code can be executed through the brainlife.io Apps thanks to the cloud computing resources provided by the platform., Fur-thermore, the code can be run locally by means of brainlife.io CLI or by cloning it from the GitHub repository, which contains the scripts on which the brainlife.io Apps are based., The brainlife.io Apps implemented in the benchmark could be used to perform the preprocessing of the T1-w images and to obtain the ground truth segmentations., Usage notes The Apps can be run via brainlife.io with both the web interface and the CLI, using the brainlife.io computing resources., With both of these solutions, the results are stored on the brainlife.io platform, under the speciﬁed project., To run the App via CLI you can use the bl app command, as follows: bl app run --id < app_id > --project < project_id > \\ --input < input_key > : < object_id > The option –project is used to specify in which brainlife.io project the output of the App is stored., the id of the input data stored on brainlife.io., We have published a ﬁrst release of the DBB benchmark on the cloud computing platform brainlife.io at https://brainlife.io/pub/60ddea776f0f540a79ca53d8., Brainlife CLI could be installed on UNIX/Linux-based system following the instruction reported in https://brainlife.io/docs/cli/install/., The code can be executed through the brainlife.io Apps thanks to the cloud computing resources provided by the platform., Furthermore, the code can be run locally by means of brainlife.io CLI or by cloning it from the Github repository, which contains the scripts on which the brainlife.io Apps are based.",0,0,1
10.1016/j.neuroimage.2022.119486,brainlife.io/docs/user/datatypes,Datatype Datatype tag Files Data description Source neuro/parcellation/volume parc.nii.gz label.json Segmentation image Description of each label EMEDEA-PED C-MIND NIH-PED neuro/anat/t1w t1.nii.gz T1-w preprocessed and defaced EMDEA-PED neuro/mask mask.nii.gz Brain Mask in preprocessed space EMEDEA-PED C-MIND NIH-PED neuro/mask raw mask.nii.gz Brain Mask in rawspace C-MIND NIH-PED neuro/transform linear aﬃne.txt Reorientation Matrix (APCP) C-MIND NIH-PED The organization of the ﬁles follows the deﬁnition of datatypes adopted by brainlife.io (see the full documentation at https://brainlife.io/docs/user/datatypes/).,1,0,0
10.1016/j.neuroimage.2022.119486,brain-life.io,"Data can be directly downloaded using the web interface of the brain-life.io platform or through the command-line interface (CLI)., This work is partially funded by “Machine Learning Research and Education on Neuroscience Data by connecting Google Cloud and brain-life.io ”-Cloudify Gateways 2020, supported by Google, Omnibond and Science Gateways.",0,0,1
10.1016/j.neuroimage.2022.119486,github.com/fbk-nilab/bl_app_dbb_disseg,"Name application DOI GitHub Branch DBB_preprocessing_t1w https://doi.org/10.25663/brainlife.app.535 https://GitHub.com/FBK-NILab/bl_app_dbb_preproc_t1w 1.5.2 DBB_ANTsCorThickSeg https://doi.org/10.25663/brainlife.app.541 https://GitHub.com/FBK-NILab/bl_app_dbb_ANTsCTSeg 2.5.5 DBB_DisSeg https://doi.org/10.25663/brainlife.app.548 https://GitHub.com/FBK-NILab/bl_app_dbb_DisSeg 1.4.2 DBB_DiceScore https://doi.org/10.25663/brainlife.app.579 https://GitHub.com/FBK-NILab/bl_app_dbb_DiceScore 1.2.2 DBB_ANTsCortThickSeg We implemented the App to per-form the brain tissue segmentation phase of the script antsCroticalThickness.sh as described in Section4.1 , in a custom version where we implemented the input to explicitly provide the brain mask.",1,0,0
10.1016/j.neuroimage.2022.119486,orcid.org/0000-0002-9480-379x,Amorosino). 1 ORCIDs: https://orcid.org/0000-0003-2789-5193. 2 ORCIDs: https://orcid.org/0000-0002-9480-379X. 3 ORCIDs: https://orcid.org/0000-0002-2926-6254. 4 ORCIDs: https://orcid.org/0000-0001-7653-1519. 5 ORCIDs: https://orcid.org/0000-0002-5508-1149. 6 ORCIDs: https://orcid.org/0000-0001-8943-8911.,0,0,1
10.1016/j.neuroimage.2022.119486,itksnap.org,"The NIH-PED and C-MIND data are distributed already including the anonymiza-tion secure by defacing. 4 http://www.itksnap.org/pmwiki/pmwiki.php. 5 https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/fsl_anat., The snake evolves from a very rough 7 http://www.itksnap.org. 8 G.",0,1,0
10.1016/j.neuroimage.2022.119486,antsbrainextraction.sh,"We used ROBEX (Iglesias et al., 2011), fsl_anat 5 (Jenkinson et al., 2012) and antsBrainExtraction.sh (Avants et al., 2010) with default pa-rameters.",1,0,0
10.1016/j.neuroimage.2022.119486,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fsl_anat,The NIH-PED and C-MIND data are distributed already including the anonymiza-tion secure by defacing. 4 http://www.itksnap.org/pmwiki/pmwiki.php. 5 https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/fsl_anat.,0,1,0
10.1016/j.neuroimage.2022.119486,itksnap.org/pmwiki/pmwiki.php,The NIH-PED and C-MIND data are distributed already including the anonymiza-tion secure by defacing. 4 http://www.itksnap.org/pmwiki/pmwiki.php. 5 https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/fsl_anat.,0,1,0
10.1016/j.neuroimage.2022.119486,orcid.org/0000-0001-7653-1519,Amorosino). 1 ORCIDs: https://orcid.org/0000-0003-2789-5193. 2 ORCIDs: https://orcid.org/0000-0002-9480-379X. 3 ORCIDs: https://orcid.org/0000-0002-2926-6254. 4 ORCIDs: https://orcid.org/0000-0001-7653-1519. 5 ORCIDs: https://orcid.org/0000-0002-5508-1149. 6 ORCIDs: https://orcid.org/0000-0001-8943-8911.,0,0,1
10.1016/j.neuroimage.2022.119486,orcid.org/0000-0001-8943-8911,Amorosino). 1 ORCIDs: https://orcid.org/0000-0003-2789-5193. 2 ORCIDs: https://orcid.org/0000-0002-9480-379X. 3 ORCIDs: https://orcid.org/0000-0002-2926-6254. 4 ORCIDs: https://orcid.org/0000-0001-7653-1519. 5 ORCIDs: https://orcid.org/0000-0002-5508-1149. 6 ORCIDs: https://orcid.org/0000-0001-8943-8911.,0,0,1
10.1016/j.neuroimage.2022.119486,antscroticalthickness.sh,"Name application DOI GitHub Branch DBB_preprocessing_t1w https://doi.org/10.25663/brainlife.app.535 https://GitHub.com/FBK-NILab/bl_app_dbb_preproc_t1w 1.5.2 DBB_ANTsCorThickSeg https://doi.org/10.25663/brainlife.app.541 https://GitHub.com/FBK-NILab/bl_app_dbb_ANTsCTSeg 2.5.5 DBB_DisSeg https://doi.org/10.25663/brainlife.app.548 https://GitHub.com/FBK-NILab/bl_app_dbb_DisSeg 1.4.2 DBB_DiceScore https://doi.org/10.25663/brainlife.app.579 https://GitHub.com/FBK-NILab/bl_app_dbb_DiceScore 1.2.2 DBB_ANTsCortThickSeg We implemented the App to per-form the brain tissue segmentation phase of the script antsCroticalThickness.sh as described in Section4.1 , in a custom version where we implemented the input to explicitly provide the brain mask.",1,0,0
10.1016/j.neuroimage.2022.119486,orcid.org/0000-0003-2789-5193,Amorosino). 1 ORCIDs: https://orcid.org/0000-0003-2789-5193. 2 ORCIDs: https://orcid.org/0000-0002-9480-379X. 3 ORCIDs: https://orcid.org/0000-0002-2926-6254. 4 ORCIDs: https://orcid.org/0000-0001-7653-1519. 5 ORCIDs: https://orcid.org/0000-0002-5508-1149. 6 ORCIDs: https://orcid.org/0000-0001-8943-8911.,0,0,1
10.1016/j.neuroimage.2022.119486,orcid.org/0000-0002-2926-6254,Amorosino). 1 ORCIDs: https://orcid.org/0000-0003-2789-5193. 2 ORCIDs: https://orcid.org/0000-0002-9480-379X. 3 ORCIDs: https://orcid.org/0000-0002-2926-6254. 4 ORCIDs: https://orcid.org/0000-0001-7653-1519. 5 ORCIDs: https://orcid.org/0000-0002-5508-1149. 6 ORCIDs: https://orcid.org/0000-0001-8943-8911.,0,0,1
10.1016/j.neuroimage.2022.119486,emedea.it/medea/en,"For example, EMEDEA-PED has a better cov-erage of the upper part of paediatric range while C-MIND in the lower part. 2 https://research.cchmc.org/c-mind. 3 https://emedea.it/medea/en/. 4 G.",1,0,0
10.1016/j.neuroimage.2022.119486,pediatricmri.nih.gov,"Individuals aged between 4.6 and 18.3 years were acquired 1 http://pediatricmri.nih.gov., In the case of NIH-PED and C-MIND subjects, we don’t provide the T1-w images because we forward them to their oﬃcial repositories, where the download is available after the registration and the acceptance of the terms of use (NIH-PED: http://pediatricmri.nih.gov ; C-MIND: https://research.cchmc.org/c-mind).",0,0,1
10.1016/j.neuroimage.2022.119486,github.com/fbk-nilab/bl_app_dbb_antsctseg,"Name application DOI GitHub Branch DBB_preprocessing_t1w https://doi.org/10.25663/brainlife.app.535 https://GitHub.com/FBK-NILab/bl_app_dbb_preproc_t1w 1.5.2 DBB_ANTsCorThickSeg https://doi.org/10.25663/brainlife.app.541 https://GitHub.com/FBK-NILab/bl_app_dbb_ANTsCTSeg 2.5.5 DBB_DisSeg https://doi.org/10.25663/brainlife.app.548 https://GitHub.com/FBK-NILab/bl_app_dbb_DisSeg 1.4.2 DBB_DiceScore https://doi.org/10.25663/brainlife.app.579 https://GitHub.com/FBK-NILab/bl_app_dbb_DiceScore 1.2.2 DBB_ANTsCortThickSeg We implemented the App to per-form the brain tissue segmentation phase of the script antsCroticalThickness.sh as described in Section4.1 , in a custom version where we implemented the input to explicitly provide the brain mask.",1,0,0
10.1016/j.neuroimage.2022.119486,orcid.org/0000-0002-5508-1149,Amorosino). 1 ORCIDs: https://orcid.org/0000-0003-2789-5193. 2 ORCIDs: https://orcid.org/0000-0002-9480-379X. 3 ORCIDs: https://orcid.org/0000-0002-2926-6254. 4 ORCIDs: https://orcid.org/0000-0001-7653-1519. 5 ORCIDs: https://orcid.org/0000-0002-5508-1149. 6 ORCIDs: https://orcid.org/0000-0001-8943-8911.,0,0,1
10.1016/j.neuroimage.2022.119486,antsatroposn4.sh,"The step of n-tissue iterative segmentation is carried out using the method implemented by the script antsAtroposN4.sh , an extended version of Atropos included in ANTs toolkit.",1,0,0
10.1016/j.neuroimage.2022.119486,antscorticalthickness.sh,"In particular we refer to the method for automatic seg-mentation deﬁned in the ANTs Cortical Thickness processing framework (Tustison et al., 2013) and to its implementation available as command line script, namely AntsCorticalThickness.sh.",1,0,0
10.1016/j.neuroimage.2022.119486,github.com/fbk-nilab/bl_app_dbb_dicescore,"Name application DOI GitHub Branch DBB_preprocessing_t1w https://doi.org/10.25663/brainlife.app.535 https://GitHub.com/FBK-NILab/bl_app_dbb_preproc_t1w 1.5.2 DBB_ANTsCorThickSeg https://doi.org/10.25663/brainlife.app.541 https://GitHub.com/FBK-NILab/bl_app_dbb_ANTsCTSeg 2.5.5 DBB_DisSeg https://doi.org/10.25663/brainlife.app.548 https://GitHub.com/FBK-NILab/bl_app_dbb_DisSeg 1.4.2 DBB_DiceScore https://doi.org/10.25663/brainlife.app.579 https://GitHub.com/FBK-NILab/bl_app_dbb_DiceScore 1.2.2 DBB_ANTsCortThickSeg We implemented the App to per-form the brain tissue segmentation phase of the script antsCroticalThickness.sh as described in Section4.1 , in a custom version where we implemented the input to explicitly provide the brain mask.",1,0,0
10.1016/j.neuroimage.2022.119724,github.com/robince/partial-info-decomp,"Analyzes were performed within the Matlab and Python com-puting environments, using open-source toolboxes and libraries such as Fieldtrip (http://www.ﬁeldtriptoolbox.org ; RRID:SCR_004849) (Oostenveld et al., 2011), MNE (Gramfort et al., 2013) and PID library (https://github.com/robince/partial-info-decomp) as well as custom-made code.",1,0,0
10.1016/j.neuroimage.2022.119724,psychtoolbox.org,com ; RRID:SCR_001622) and the PsychToolbox-3 extensions (http://psychtoolbox.org ; RRID:SCR_002881).,0,0,1
10.1016/j.neuroimage.2022.119724,linguistica.sns.it/esploracolﬁs/home.htm,"To avoid possible biases in the participants’ responses, we ensured that rhyming and non-rhyming words were matched for number of sylla-bles and their frequency of use in the Italian language by means of an online software tool (http://linguistica.sns.it/esploracolﬁs/home.htm).",1,0,0
10.1016/j.neuroimage.2022.119357,adni.loni.usc.edu,"# A portion of data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf., ADNI images are available directly through ADNI (http://adni.loni.usc.edu/).",0,0,1
10.1016/j.neuroimage.2022.119357,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.,0,0,1
10.1016/j.neuroimage.2022.119357,nitrc.org/projects/mcalt,"The pipelines were: 1) a previously pub-lished in-house cross-sectional method (Schwarz et al., 2021b) based on SPM12 (Ashburner, 2009), the Mayo Clinic Adult Lifespan Tem-plate (MCALT; https://www.nitrc.org/projects/mcalt/) (Schwarz et al., 2017), and ANTs (Avants et al., 2008), and 2) PETSurfer (Greve et al., 2016) from FreeSurfer version 6.0 (Fischl, 2012).",1,0,0
10.1016/j.neuroimage.2022.119357,adni.loni.usc.edu,"A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf., ADNI images are available directly through ADNI (http://adni.loni.usc.edu/).",0,0,1
10.1016/j.neuroimage.2022.119357,nitrc.org/projects/mri_reface,"Since that publication, we have publicly released this software, free for use by the research community (https://www.nitrc.org/projects/mri_reface)., For all modalities, we will continue to improve mri_reface and release updated software at https://www.nitrc.org/projects/mri_reface , but we propose that the current performance is acceptable and worth using until newer versions are developed. 7 C.G., Our mri_reface software is avail-able at https://www.nitrc.org/projects/mri_reface.",1,0,0
10.1016/j.neuroimage.2022.119022,base2.mpg.de/en,BASE-II information can be found online (https://www.base2.mpg.de/en).,0,0,1
10.1016/j.neuroimage.2022.119499,github.com/tingtingwu222/coat,"E-prime scripts for stimuli presentation and MATLAB and Python scripts for image preprocessing, fMRI modeling, and Morse decomposition are available in the GitHub repository of this study (https://github.com/TingtingWu222/COAT).",1,0,0
10.1016/j.neuroimage.2022.119499,neurovault.org/collections/12519,Group-level result im-ages are available in the NeuroVault collection of this study (https://neurovault.org/collections/12519/).,0,0,1
10.1016/j.neuroimage.2022.119499,gpower.hhu.de,"Power analysis conducted using G ∗ Power 3.1 (Faul et al., 2009) (RRID : SCR_013726, http://www.gpower.hhu.de/) demonstrated that the ﬁnal sample had suﬃcient power to detect the core eﬀects of interest (see Supplementary Results for details).",1,0,0
10.1016/j.neuroimage.2021.118866,dsi-studio.labsolver.org,Final white matter ﬁber tractography was then analyzed using DSI Studio (http://dsi-studio.labsolver.org).,1,0,0
10.1016/j.neuroimage.2021.118866,nitric.org/projects.dke/(tabesh,"For the pre-processed DKI data with b = 0, 1000, and 2000s/mm 2 , Diﬀusional Kurto-sis Estimator (DKE; https://www.nitric.org/projects.dke/(Tabesh et al., 2011)) was applied to obtain both the diﬀusion and kurtosis tensors us-ing both shells.",1,0,0
10.1016/j.neuroimage.2021.118866,github.com/m-ama,"In terms of generalizability, the software needed to pro-cess FBI has been recently described (Dhiman et al., 2021) and is freely available at https://github.com/m-ama.",1,0,0
10.1016/j.neuroimage.2021.118866,readthedocs.io/en/latest,readthedocs.io/en/latest/).,0,0,1
10.1016/j.neuroimage.2021.118866,sites.google.com/site/bctnet,"Whole brain graph theory network properties were obtained from each binarized connectome using DSI-Studio’s network measures func-tion, which employs measures from the brain connectivity toolbox (https://sites.google.com/site/bctnet/).",1,0,0
10.1016/j.neuroimage.2022.119585,github.com/qluo2018/gcsdn,A Matlab toolbox of this algorithm is also available at https://github.com/qluo2018/GCSDN.,1,0,0
10.1016/j.neuroimage.2022.119585,ﬁl.ion.ucl.ac.uk/spm,"(2010) using the SPM (http://www.ﬁl.ion.ucl.ac.uk/spm) with a standard procedure, including the slice-timing correction, motion correction, co-registration, normalization to the Montreal Neurological Institute template, and high-pass ﬁltered (128 s).",1,0,0
10.1016/j.neuroimage.2022.119585,github.com/rhyang2021/data-code4tvgcsdn,Data and code availability statement The datasets and code generated and analysed during the current study are available at https://github.com/rhyang2021/data-code4TVGCSDN.,0,1,0
10.1016/j.neuroimage.2022.119455,stnava.github.io/ants,Both the template generation and the individual warping were based on the tools implemented in the ANTs toolbox (http://stnava.github.io/ANTs/).,1,0,0
10.1016/j.neuroimage.2022.119455,github.com/shinelabusyd/brainstem_dti_attractor_paper,"Brain-state displacement and attractor landscape In order to evaluate the changes of BOLD activity in relation to phasic bursts in neuromodulatory activity, we utilised the approach in-troduced in Munn et al., 2021 (code for this analysis available https://github.com/ShineLabUSYD/Brainstem_DTI_Attractor_Paper)., Code for the analysis is available https://github.com/ShineLabUSYD/Brainstem_DTI_Attractor_Paper.",1,0,0
10.1016/j.neuroimage.2022.119455,mrtrix.org,"These steps were implemented in accordance with previous work (Civier et al., 2019) and were per-formed using the MRtrix software package (http://www.mrtrix.org , (Tournier et al., 2012 , 2019)).",1,0,0
10.1016/j.neuroimage.2022.119455,github.com/macshine/coupling,A 20 TR smoothing parameter was used to then applied to the time-varying con-nectivity estimates so as to diminish the impact of high frequency noise (code is freely available at https://github.com/macshine/coupling/).,1,0,0
10.1016/j.neuroimage.2022.119703,adni.loni.usc.edu,"Data used in preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.",0,1,0
10.1016/j.neuroimage.2022.119703,jpnd.eu,"This publication is an out-come of ESMI, an EU Joint Programme -Neurodegenerative Disease Research (JPND) project (see www.jpnd.eu).",0,0,1
10.1016/j.neuroimage.2022.119703,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.,0,0,1
10.1016/j.neuroimage.2022.119703,clinicaltrials.gov,"Now, that the ﬁrst clinical gene silencing trial has recently started (ClinicalTrials.gov Identiﬁer: NCT05160558), there is an urgent need for non-invasive biomarkers to assess disease manifestation and progression, and to quantify potential treatment ef-fects as clinical scales lack sensitivity during the pre-ataxic stage.",0,0,1
10.1016/j.neuroimage.2022.119703,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119703,github.com/deep-mi/fastsurfer,"We make CerebNet available as source-code (https://github.com/Deep-MI/FastSurfer)., The source code of CerebNet will be made publicly available on Github (https://github.com/Deep-MI/FastSurfer) upon acceptance.",1,0,0
10.1016/j.neuroimage.2021.118823,mcgill.ca/bic/resources/omega,"Code and data availability Data used in the preparation of this work were obtained from the Cam-CAN repository (available at http://www.mrc-cbu.cam.ac.uk/datasets/camcan/; Shafto et al., 2014 ; Taylor et al., 2017) and the OMEGA repository (available at https://www.mcgill.ca/bic/resources/omega ; Niso et al., 2016)., Data and code availability Data used in the preparation of this work were obtained from the Cam-CAN repository (available at http://www.mrc-cbu.cam.ac.uk/datasets/camcan/; Shafto et al., 2014 ; Taylor et al., 2017) and the OMEGA repository (available at https://www.mcgill.ca/bic/resources/omega ; Niso et al., 2016).",0,1,0
10.1016/j.neuroimage.2021.118823,github.com/aiwiesman/rsmeg_stabilityanalysis,"Code for MEG preprocessing and the stability analysis is available at https://github.com/aiwiesman/rsMEG_StabilityAnalysis. 6 A.I., Code for MEG preprocessing and the stability analysis is available at https://github.com/aiwiesman/rsMEG_StabilityAnalysis.",1,0,0
10.1016/j.neuroimage.2021.118823,mrc-cbu.cam.ac.uk/datasets/camcan,"Code and data availability Data used in the preparation of this work were obtained from the Cam-CAN repository (available at http://www.mrc-cbu.cam.ac.uk/datasets/camcan/; Shafto et al., 2014 ; Taylor et al., 2017) and the OMEGA repository (available at https://www.mcgill.ca/bic/resources/omega ; Niso et al., 2016)., Data and code availability Data used in the preparation of this work were obtained from the Cam-CAN repository (available at http://www.mrc-cbu.cam.ac.uk/datasets/camcan/; Shafto et al., 2014 ; Taylor et al., 2017) and the OMEGA repository (available at https://www.mcgill.ca/bic/resources/omega ; Niso et al., 2016).",0,1,0
10.1016/j.neuroimage.2022.119731,fmri.wfubmc.edu/software/pickatlas,"These regions were deﬁned using corresponding anatomical masks divided into left and right hemispheres from the Wake Forest University (WFU) PickAtlas toolbox (Maldjian et al., 2003) (http://fmri.wfubmc.edu/software/PickAtlas).",1,0,0
10.1016/j.neuroimage.2022.119731,osf.io/a8z2u/data,Data and code availability statement The dataset analyzed in the present study as well as the materials for all the ﬁgures are available through the Open Science Framework at https://osf.io/a8z2u/Data Availability Data will be made available on request.,0,1,0
10.1016/j.neuroimage.2022.119731,ﬁl.ion.ucl.ac.uk/spm,The fMRI image preprocessing was done using SPM12 (www.ﬁl.ion.ucl.ac.uk/spm/).,1,0,0
10.1016/j.neuroimage.2022.119731,mathworks.com,All of the data were exported for further analysis using Matlab 2020 (www.mathworks.com).,1,0,0
10.1016/j.neuroimage.2022.119731,pstnet.com/e-prime,"Pittsburgh, PA, USA, www.pstnet.com/e-prime).",0,0,1
10.1016/j.neuroimage.2022.119704,mbb-team.github.io/vba-toolbox),"Similar to previous studies (Klein-Flügge et al., 2015 ; Morris, et al., 2020), a psychometric sigmoid function was ﬁt to the eﬀort-by-reward discount curves for each condition for each subject using the variational Bayes approach to model inversion implemented in the VBA toolbox (available at mbb-team.github.io/VBA-toolbox), under a mixed-eﬀects framework, using MATLAB R2019a.",1,0,0
10.1016/j.neuroimage.2022.118971,github.com/brain-networks/edge-centric_demo,NeuroImage 250 (2022) 118971 Code Availability Edge time-series scripts can be found at https://github.com/brain-networks/edge-centric_demo.,1,0,0
10.1016/j.neuroimage.2022.119751,hmri.info,"Creating MPM maps and cross-sectional cord area The MPM data were processed using the hMRI-toolbox (www.hmri.info ,versions v0.1.1-beta and v0.1.2-beta) (Tabelow et al., 2019) within the SPM12 framework (version 6906; FIL, London, UK) in MATLAB (version R2017b; Mathworks, Natick, Massachusetts, USA on GNU/Linux computers with x86_64 architecture) (Fig. 1 A).",1,0,0
10.1016/j.neuroimage.2022.119751,nisci-2020.eu,"Site MRI vendor MRI system Number of channels of RF receive head & neck coil MRI software version BCN Siemens verio 32 VD13A BSL Siemens Prisma 20 VE11C HD Siemens Verio 16 VB19A NOT Philips Achieva 16 5.3.0 ZH Siemens Skyra 16 VE11B SNS Philips Achieva 16 5.1.7 www.nisci-2020.eu) investigating a therapeutic eﬀect based on the Anti-Nogo-A antibody treatment (Freund et al., 2006) in acute SCI patients.",1,0,0
10.1016/j.neuroimage.2022.119684,github.com/cobralab/magetbrain,"Data processing was done with MAGeT Brain (https://github.com/CoBrALab/MAGeTbrain), MRtrix3 (https://www.",1,0,0
10.1016/j.neuroimage.2022.119684,humanconnectome.org,"Participants 100 unrelated participants (50 females, average age = 29) were randomly selected from a dataset used in a prior study (Steele and Chakravarty, 2018) which used structural imaging data from the Human Connectome Project open-access dataset (www.humanconnectome.org) (Glasser et al., 2016 ; Steele and Chakravarty, 2018 ; Van Essen et al., 2012)., Data and code availability statement Source imaging data is freely available from the Human Connectome Project (www.humanconnectome.org).",0,1,0
10.1016/j.neuroimage.2022.119684,surfer.nmr.mgh.harvard.edu,Segmentation of brainstem structures was performed using a tool implemented by Freesurfer (v6.0) (http://surfer.nmr.mgh.harvard.edu).,1,0,0
10.1016/j.neuroimage.2022.119684,neurovault.org,"All summary maps and parcellations will be made available on NeuroVault (https://neurovault.org/) at the time of publication., All summary maps and parcel-lations will be made available on NeuroVault (https://neurovault.org/) at the time of publication.",0,0,1
10.1016/j.neuroimage.2022.119684,github.com/antsx/ants,"mrtrix.org/), Advanced Normalization Tools (https://github.com/ANTsX/ANTs), and TractREC (https://github.com/CoBrALab/Tract REC).",1,0,0
10.1016/j.neuroimage.2022.119684,github.com/cobralab/tract,"mrtrix.org/), Advanced Normalization Tools (https://github.com/ANTsX/ANTs), and TractREC (https://github.com/CoBrALab/Tract REC).",1,0,0
10.1016/j.neuroimage.2022.119684,mrtrix.org,"mrtrix.org/), Advanced Normalization Tools (https://github.com/ANTsX/ANTs), and TractREC (https://github.com/CoBrALab/Tract REC).",1,0,0
10.1016/j.neuroimage.2022.119425,ixi.org.uk,"& F.J., M.D.) visually inspected eICAB output pre-dicted CW segmentation masks of 75 images randomly selected from the IXI database (http://www.ixi.org.uk) and, 46 from the ICBM database (https://www.loni.usc.edu/)., URL links for public datasets are: -IXI: http://www.ixi.org.uk -OASIS-3: https://www.oasis-brains.org/-MIDAS: http://www.insight-journal.org/midas/community/view/21 -ICBM: https://www.loni.usc.edu/eICAB pipeline GitLab repository: https://gitlab.com/FelixDumais/vessel_segmentation_snaillab.git.",0,1,0
10.1016/j.neuroimage.2022.119425,insight-journal.org/midas/community/view/21,URL links for public datasets are: -IXI: http://www.ixi.org.uk -OASIS-3: https://www.oasis-brains.org/-MIDAS: http://www.insight-journal.org/midas/community/view/21 -ICBM: https://www.loni.usc.edu/eICAB pipeline GitLab repository: https://gitlab.com/FelixDumais/vessel_segmentation_snaillab.git.,0,0,1
10.1016/j.neuroimage.2022.119425,m.sc,"Annotations were equally divided amongst 5 people: 2 medical ex-perts (authors N.A.B., M.D., 3rd year radiology resident and F.J., M.D.), a medical student (author M.P.C., M.Sc.) and two trained annotators (authors F.D., B.Eng., M.Sc., For qualitative assessment, two trained annotators (authors F.D., B.Eng., M.Sc.",0,0,1
10.1016/j.neuroimage.2022.119425,brain-development.org/ixi-dataset,Part of the MR brain images from healthy volunteers used in this paper came from the IXI Dataset https://brain-development.org/ixi-dataset/.,0,1,0
10.1016/j.neuroimage.2022.119425,loni.usc.edu,"& F.J., M.D.) visually inspected eICAB output pre-dicted CW segmentation masks of 75 images randomly selected from the IXI database (http://www.ixi.org.uk) and, 46 from the ICBM database (https://www.loni.usc.edu/)., URL links for public datasets are: -IXI: http://www.ixi.org.uk -OASIS-3: https://www.oasis-brains.org/-MIDAS: http://www.insight-journal.org/midas/community/view/21 -ICBM: https://www.loni.usc.edu/eICAB pipeline GitLab repository: https://gitlab.com/FelixDumais/vessel_segmentation_snaillab.git.",0,1,0
10.1016/j.neuroimage.2022.119425,oasis-brains.org/-midas:,URL links for public datasets are: -IXI: http://www.ixi.org.uk -OASIS-3: https://www.oasis-brains.org/-MIDAS: http://www.insight-journal.org/midas/community/view/21 -ICBM: https://www.loni.usc.edu/eICAB pipeline GitLab repository: https://gitlab.com/FelixDumais/vessel_segmentation_snaillab.git.,0,0,1
10.1016/j.neuroimage.2022.119425,loni.usc.edu/eicab,URL links for public datasets are: -IXI: http://www.ixi.org.uk -OASIS-3: https://www.oasis-brains.org/-MIDAS: http://www.insight-journal.org/midas/community/view/21 -ICBM: https://www.loni.usc.edu/eICAB pipeline GitLab repository: https://gitlab.com/FelixDumais/vessel_segmentation_snaillab.git.,0,0,1
10.1016/j.neuroimage.2022.119425,gitlab.com/felixdumais/vessel_segmentation_snaillab.git,URL links for public datasets are: -IXI: http://www.ixi.org.uk -OASIS-3: https://www.oasis-brains.org/-MIDAS: http://www.insight-journal.org/midas/community/view/21 -ICBM: https://www.loni.usc.edu/eICAB pipeline GitLab repository: https://gitlab.com/FelixDumais/vessel_segmentation_snaillab.git.,0,0,1
10.1016/j.neuroimage.2021.118863,fmrib.ox.ac.uk/fsl,Diﬀusion imaging analysis Diﬀusion imaging data were analyzed separately for each individual using tools from FSL 4.1 (www.fmrib.ox.ac.uk/fsl/).,1,0,0
10.1016/j.neuroimage.2021.118863,mccauslandcenter.sc.edu/mricro/mricron,The anatomical regions associated with the activation clusters were determined in MRI-cron (http://www.mccauslandcenter.sc.edu/mricro/mricron/).,1,0,0
10.1016/j.neuroimage.2021.118863,fsl.fmrib.ox.ac.uk/fsl/fslwiki/tbss,"Group DTI analysis In order to summarize the group anatomical tracts explored in the current study, individual tractography maps from the target site were transformed into the standard space of the MNI152 T1–2 mm template image by using FLIRT and applying the normalization parameters from a tract-based normalization algorithm (tract-based spatial statistics; fsl.fmrib.ox.ac.uk/fsl/fslwiki/TBSS).",1,0,0
10.1016/j.neuroimage.2021.118863,fmrib.ox.ac.uk/fsl),"Then, a standard tensor model was ﬁt at each voxel using the Functional MRI of the Brain’s Diﬀusion Toolbox (FDT; www.fmrib.ox.ac.uk/fsl), in order to create fractional anisotropy (FA) maps for each subject.",1,0,0
10.1016/j.neuroimage.2021.118863,tp://talairach.org/client.html,"80%RMT Insula 13 − 42 − 12 2 276 3.67 Amygdala 14 2 − 26 92 3.35 Insula 13 44 − 14 4 101 3.10 Frontopolar cortex 10 20 68 8 936 − 4.38 Note : Local maxima determined by FSL during cluster analysis (t > 2.78, cluster threshold p < 0.1) evaluated with the Talairach Daemon Client (ht tp://www.talairach.org/client.html) to determine their nearest Broadman Area within the range indicated in the ""Range (mm)"" column., Local maxima determined by FSL during clus-ter analysis (t > 2.45, cluster threshold p < 0.1) evaluated with the Ta-lairach Daemon Client (ht tp://www.talairach.org/client.html) to de-termine their nearest Broadman Area within the range indicated in the ""Range (mm)"" column., Region BA x y z cluster size t Cerebellum 5 − 82 − 20 1012 − 4.33 Hippocampus 12 − 6 − 16 85 4.65 Middle temporal gyrus 22 − 68 − 24 − 10 264 4.28 Anterior temporal lobe 38 − 60 4 − 24 417 4.07 Anterior Cingulate Cortex 24 − 2 44 22 231 3.67 Middle Cingulate Cortex 32 − 2 6 54 136 3.60 ACC/BA10 10 − 6 34 − 6 2502 3.44 Hippocampus − 22 − 14 − 14 143 3.38 Note : Local maxima determined by FSL during cluster analysis (t > 2.78, cluster threshold p < 0.1) evaluated with the Talairach Daemon Client (ht tp://www.talairach.org/client.html) to determine their nearest Broadman Area within the range indicated in the ""Range (mm)"" column.",1,0,0
10.1016/j.neuroimage.2022.119367,github.com/brayneuroimaginglab/bnl_open/blob/main/individualization/individualization.py,"Python script is available at https://github.com/BrayNeuroimagingLab/BNL_open/blob/main/individualization/Individualization.py., Python script is available at https://github.com/BrayNeuroimagingLab/BNL_open/blob/main/individualization/Individualization.py.",1,0,0
10.1016/j.neuroimage.2022.119367,3.0.0.0,"All preprocessing was carried out using custom Python scripts that integrated Nipype functionality (Gorgolewski et al., 2011) using FSL version 6.0.0 (Smith et al., 2004), ANTs version 3.0.0.0 (Avants et al., 2011), and AFNI version 18.3.03 (Cox, 1996).",1,0,0
10.1016/j.neuroimage.2022.119023,saipai-hrv.com,"SAI and PAI were computed using a pub-licly available online software, which can be gathered from www.saipai-hrv.com.",1,0,0
10.1016/j.neuroimage.2022.119246,github.com/genlouvain/genlouvain,"The GenLouvain MATLAB pack-age (Jutla et al., 2011) used for multi-layer community detection is freely available online (https://github.com/GenLouvain/GenLouvain).",1,0,0
10.1016/j.neuroimage.2022.119246,mathworks.com/matlabcentral/ﬁleexchange/44308-randanova2,Code for the statistical analysis (permutation test for two-way ANOVA) is freely available online (https://www.mathworks.com/matlabcentral/ﬁleexchange/44308-randanova2).,1,0,0
10.1016/j.neuroimage.2022.119246,github.com/fvfarahani/time-of-day,"Python code for network and statistical analyzes discussed in the paper, as well as visualizations, can be found via the following link: https://github.com/fvfarahani/time-of-day , in which we used dif-ferent packages such as NumPy, Pandas, SciPy, Statsmodels, Nilearn, Teneto, Matplotlib, and Seaborn.",1,0,0
10.1016/j.neuroimage.2022.119246,nitrc.org/projects/gretna,Graph theory measures were calculated using the Brain Con-nectivity Toolbox (https://sites.google.com/site/bctnet/) and GRETNA toolkit (https://www.nitrc.org/projects/gretna).,1,0,0
10.1016/j.neuroimage.2022.119246,rfmri.org/dpabi,"Data preprocessing Data preprocessing was performed using DPABI software (http://rfmri.org/dpabi) based on Statistical Parametric Mapping 12 (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) on the MATLAB plat-form (MathWorks, Inc., Natick, MA)., The DPABI toolbox used for data preprocessing is freely available online (http://rfmri.org/dpabi).",1,0,0
10.1016/j.neuroimage.2022.119246,ﬁl.ion.ucl.ac.uk/spm,"Data preprocessing Data preprocessing was performed using DPABI software (http://rfmri.org/dpabi) based on Statistical Parametric Mapping 12 (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) on the MATLAB plat-form (MathWorks, Inc., Natick, MA).",1,0,0
10.1016/j.neuroimage.2022.119246,sites.google.com/site/bctnet,Graph theory measures were calculated using the Brain Con-nectivity Toolbox (https://sites.google.com/site/bctnet/) and GRETNA toolkit (https://www.nitrc.org/projects/gretna).,1,0,0
10.1016/j.neuroimage.2022.118984,sammba-mri.github.io,"The code developed to create and manipulate the template has been reﬁned into general procedures for registering small mammal brain MR images, available within a python module sammba-mri (SmAll-maMMals BrAin MRI; https://sammba-mri.github.io).",1,0,0
10.1016/j.neuroimage.2022.118984,sammba-mri.github.io,"Then spatial pre-processing was performed us-ing the python module sammba-mri (SmAll MaMmals BrAin MRI; http://sammba-mri.github.io) (Celestine et al., 2020) which, using nipype for pipelining (Gorgolewski et al., 2011), leverages AFNI (Cox, 1996) for most steps and RATS (Oguz et al., 2014) for brain ex-traction., NeuroImage 251 (2022) 118984 pre-processing is available at http://sammba-mri.github.io.",1,0,0
10.1016/j.neuroimage.2022.118984,nitrc.org/projects/mouselemuratlas,"GluCEST contrast extraction and statistical analysis To calculate regional distribution of gluCEST contrast and its diﬀer-ences between groups, we extracted gluCEST contrast using a mouse lemur digital atlas (https://www.nitrc.org/projects/mouselemuratlas) (Nadkarni et al., 2019), large scale network maps (http://www.nitrc., Data availability The template and atlas used in this study are available for download in NIfTI-1 format at https://www.nitrc.org/projects/mouselemuratlas., The template and atlas used in this study maps are available for download in NIfTI-1 format at https://www.nitrc.org/projects/mouselemuratlas.",1,0,0
10.1016/j.neuroimage.2022.118958,db.humanconnectome.org,"Datasets (References) Site of Acquisition Data Repository Data Description in Brief MGH HCP Adult Diﬀusion (Fan et al., 2016) MGH ConnectomeDB https://db.humanconnectome.org , or Laboratory of Neuro Imaging Image Data Archive (LONI IDA) https://ida.loni.usc.edu • 35 healthy adults • b -values = 1000, 3000, 5000, 10,000 s/mm 2 • voxel size = 1.5 mm isotropic ISBI 2015 Challenge on White Matter Modeling (Ferizi et al., 2017) MGH The Challenge Website: http://cmic.cs.ucl.ac.uk/wmmchallenge • 1 healthy adult • 48 shells acquired (36 for training, 12 for testing) • b -value up to 45,900 s/mm 2 • voxel size = 2 ×2 ×4 mm MICCAI 2017 & 2018 Challenge on Data Harmonization (Tax et al., 2019) (Ning et al., 2019) Cardiﬀ CUBRIC Center https://www.cardiﬀ.ac.uk/cardiﬀ-university-brain-research-imaging-center/research/projects/cross-scanner-and-cross-protocol-diﬀusion-MRI-data-harmonization • 15 healthy adults, • across 3 scanners and 5 acquisition protocols • b -value up to 5000 s/mm 2 • voxel size down to 1.2 mm isotropic Taxon phantom (Fan et al., 2018b) MGH XNAT Central database https://central.xnat.org (ProjectID: dMRI_Phant_MGH) • Biomimetic phantom, • b -values up to 18,250 s/mm 2 • Δ= 20, 30, 40, 50 ms • voxel size = 2 mm isotropic gSlider diﬀusion data (Wang et al., 2021) MGH Dryad https://doi.org/10.5061/dryad.nzs7h44q2 (Part I) https://doi.org/10.5061/dryad.rjdfn2z8g (Part II) • 1 healthy adult, 9 two-hour sessions • b -values = 1000, 2500s/mm 2 • voxel size = 760 μm MICRA (Koller et al., 2021) CardiﬀOpen Science Framework https://osf.io/z3mkn/• 6 healthy adults, each with 5 repetitions • b -value up to 6000 s/mm 2 • voxel size = 2 mm isotropic • Relaxometry and Quantitative Magnetization Transfer (QMT) data also available MGH Connectome Diﬀusion Microstructure Dataset (CDMD) (Tian et al., 2022) MGH FigShare https://doi.org/10.6084/m9.ﬁgshare.c.5315474.v1 • 26 healthy adults • b -value up to 17,800 s/mm 2 • Δ= 19, 49 ms; 8 b -values per Δ• voxel size = 2 mm isotropic past few years, either in perpetual data repositories (Fan et al., 2018b , 2016 ; Koller et al., 2021 ; Tian et al., 2022 ; Wang et al., 2021) or through organizing computational competitions such as the International Sym-posium on Biomedical Imaging (ISBI) challenge on white matter mod-eling (Ferizi et al., 2017) and Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges on cross-scanner and cross-protocol diﬀusion data harmonization (Ning et al., 2020a ; Tax et al., 2019).",0,1,0
10.1016/j.neuroimage.2022.118958,istbi.fudan.edu.cn/info/1084/1677.htm,"To date, these sites include: the Cardiﬀ Univer-sity Brain Research Imaging Center (CUBRIC) at Cardiﬀ University in the United Kingdom; the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany; and the Zhangjiang International Brain Imaging Center (ZIC) supported by the Shanghai Municipal Gov-ernment and Fudan University in Shanghai, China. 2 https://www.cardiﬀ.ac.uk/news/view/188635-europes-most-powerful-brain-scanner-arrives. 3 https://www.cbs.mpg.de/press-releases/super-brain-scanner-connectom?c = 7533. 4 https://istbi.fudan.edu.cn/info/1084/1677.htm. 3 Q.",0,0,1
10.1016/j.neuroimage.2022.118958,sciencedirect.com/journal/neuroimage/vol/80,"Descriptions of the system de-sign (Setsompop et al., 2013) and initial demonstrations of its unprece-dented capabilities (McNab et al., 2013a) were reported in the ﬁrst spe-cial issue of the journal NeuroImage devoted to human connectomics (https://www.sciencedirect.com/journal/neuroimage/vol/80).",0,0,1
10.1016/j.neuroimage.2022.118958,dicardiology.com/content/siemens-announces-ﬁrst-us-installation-magnetom-prisma-3t-mri-system,"For exam-ple, the Siemens MAGNETOM Prisma 3T scanner and the GE Premier 3T scanner provide up to 80 mT/m and have been installed worldwide for clinical and research use. 5 Philips developed the high-performance Quasar Dual gradient system (maximum gradient strength of 80 mT/m), which has been installed on diﬀerent models, including the Achieva 3T 5 https://www.dicardiology.com/content/siemens-announces-ﬁrst-us-installation-magnetom-prisma-3t-mri-system.",1,0,0
10.1016/j.neuroimage.2022.118958,demo.united-imaging.com/cn/news/2021/%e8%81%94%e5%bd,The PNS thresholds of an MRI gradient are typically given in terms of minimum stimulating gradient amplitude ΔG as a function of gradient 6 Philips Achieva 3.0T MRI -medical equipment distribution (htig.com). 7 https://www.philips.nl/healthcare/product/HC781271/ingenia-30t-cx-mr-system. 8 https://jp.medical.canon/News/PressRelease/Detail/45069-834. 9 http://demo.united-imaging.com/cn/news/2021/%E8%81%94%E5%BD %B1-cmef-%E5%8D%81%E5%B9%B4%E5%8E%9A%E7%A7%AF-%E8%87%AA%E4%B8%BB%E5%8F%AF%E6%8E%A7%E5%9C%A8%E6 %AD%A4-%E6%B2%B8%E8%85%BE/. 4 Q.,0,0,1
10.1016/j.neuroimage.2022.118958,ida.loni.usc.edu,"Datasets (References) Site of Acquisition Data Repository Data Description in Brief MGH HCP Adult Diﬀusion (Fan et al., 2016) MGH ConnectomeDB https://db.humanconnectome.org , or Laboratory of Neuro Imaging Image Data Archive (LONI IDA) https://ida.loni.usc.edu • 35 healthy adults • b -values = 1000, 3000, 5000, 10,000 s/mm 2 • voxel size = 1.5 mm isotropic ISBI 2015 Challenge on White Matter Modeling (Ferizi et al., 2017) MGH The Challenge Website: http://cmic.cs.ucl.ac.uk/wmmchallenge • 1 healthy adult • 48 shells acquired (36 for training, 12 for testing) • b -value up to 45,900 s/mm 2 • voxel size = 2 ×2 ×4 mm MICCAI 2017 & 2018 Challenge on Data Harmonization (Tax et al., 2019) (Ning et al., 2019) Cardiﬀ CUBRIC Center https://www.cardiﬀ.ac.uk/cardiﬀ-university-brain-research-imaging-center/research/projects/cross-scanner-and-cross-protocol-diﬀusion-MRI-data-harmonization • 15 healthy adults, • across 3 scanners and 5 acquisition protocols • b -value up to 5000 s/mm 2 • voxel size down to 1.2 mm isotropic Taxon phantom (Fan et al., 2018b) MGH XNAT Central database https://central.xnat.org (ProjectID: dMRI_Phant_MGH) • Biomimetic phantom, • b -values up to 18,250 s/mm 2 • Δ= 20, 30, 40, 50 ms • voxel size = 2 mm isotropic gSlider diﬀusion data (Wang et al., 2021) MGH Dryad https://doi.org/10.5061/dryad.nzs7h44q2 (Part I) https://doi.org/10.5061/dryad.rjdfn2z8g (Part II) • 1 healthy adult, 9 two-hour sessions • b -values = 1000, 2500s/mm 2 • voxel size = 760 μm MICRA (Koller et al., 2021) CardiﬀOpen Science Framework https://osf.io/z3mkn/• 6 healthy adults, each with 5 repetitions • b -value up to 6000 s/mm 2 • voxel size = 2 mm isotropic • Relaxometry and Quantitative Magnetization Transfer (QMT) data also available MGH Connectome Diﬀusion Microstructure Dataset (CDMD) (Tian et al., 2022) MGH FigShare https://doi.org/10.6084/m9.ﬁgshare.c.5315474.v1 • 26 healthy adults • b -value up to 17,800 s/mm 2 • Δ= 19, 49 ms; 8 b -values per Δ• voxel size = 2 mm isotropic past few years, either in perpetual data repositories (Fan et al., 2018b , 2016 ; Koller et al., 2021 ; Tian et al., 2022 ; Wang et al., 2021) or through organizing computational competitions such as the International Sym-posium on Biomedical Imaging (ISBI) challenge on white matter mod-eling (Ferizi et al., 2017) and Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges on cross-scanner and cross-protocol diﬀusion data harmonization (Ning et al., 2020a ; Tax et al., 2019).",0,1,0
10.1016/j.neuroimage.2022.118958,central.xnat.org,"Datasets (References) Site of Acquisition Data Repository Data Description in Brief MGH HCP Adult Diﬀusion (Fan et al., 2016) MGH ConnectomeDB https://db.humanconnectome.org , or Laboratory of Neuro Imaging Image Data Archive (LONI IDA) https://ida.loni.usc.edu • 35 healthy adults • b -values = 1000, 3000, 5000, 10,000 s/mm 2 • voxel size = 1.5 mm isotropic ISBI 2015 Challenge on White Matter Modeling (Ferizi et al., 2017) MGH The Challenge Website: http://cmic.cs.ucl.ac.uk/wmmchallenge • 1 healthy adult • 48 shells acquired (36 for training, 12 for testing) • b -value up to 45,900 s/mm 2 • voxel size = 2 ×2 ×4 mm MICCAI 2017 & 2018 Challenge on Data Harmonization (Tax et al., 2019) (Ning et al., 2019) Cardiﬀ CUBRIC Center https://www.cardiﬀ.ac.uk/cardiﬀ-university-brain-research-imaging-center/research/projects/cross-scanner-and-cross-protocol-diﬀusion-MRI-data-harmonization • 15 healthy adults, • across 3 scanners and 5 acquisition protocols • b -value up to 5000 s/mm 2 • voxel size down to 1.2 mm isotropic Taxon phantom (Fan et al., 2018b) MGH XNAT Central database https://central.xnat.org (ProjectID: dMRI_Phant_MGH) • Biomimetic phantom, • b -values up to 18,250 s/mm 2 • Δ= 20, 30, 40, 50 ms • voxel size = 2 mm isotropic gSlider diﬀusion data (Wang et al., 2021) MGH Dryad https://doi.org/10.5061/dryad.nzs7h44q2 (Part I) https://doi.org/10.5061/dryad.rjdfn2z8g (Part II) • 1 healthy adult, 9 two-hour sessions • b -values = 1000, 2500s/mm 2 • voxel size = 760 μm MICRA (Koller et al., 2021) CardiﬀOpen Science Framework https://osf.io/z3mkn/• 6 healthy adults, each with 5 repetitions • b -value up to 6000 s/mm 2 • voxel size = 2 mm isotropic • Relaxometry and Quantitative Magnetization Transfer (QMT) data also available MGH Connectome Diﬀusion Microstructure Dataset (CDMD) (Tian et al., 2022) MGH FigShare https://doi.org/10.6084/m9.ﬁgshare.c.5315474.v1 • 26 healthy adults • b -value up to 17,800 s/mm 2 • Δ= 19, 49 ms; 8 b -values per Δ• voxel size = 2 mm isotropic past few years, either in perpetual data repositories (Fan et al., 2018b , 2016 ; Koller et al., 2021 ; Tian et al., 2022 ; Wang et al., 2021) or through organizing computational competitions such as the International Sym-posium on Biomedical Imaging (ISBI) challenge on white matter mod-eling (Ferizi et al., 2017) and Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges on cross-scanner and cross-protocol diﬀusion data harmonization (Ning et al., 2020a ; Tax et al., 2019).",0,1,0
10.1016/j.neuroimage.2022.118958,htig.com,The PNS thresholds of an MRI gradient are typically given in terms of minimum stimulating gradient amplitude ΔG as a function of gradient 6 Philips Achieva 3.0T MRI -medical equipment distribution (htig.com). 7 https://www.philips.nl/healthcare/product/HC781271/ingenia-30t-cx-mr-system. 8 https://jp.medical.canon/News/PressRelease/Detail/45069-834. 9 http://demo.united-imaging.com/cn/news/2021/%E8%81%94%E5%BD %B1-cmef-%E5%8D%81%E5%B9%B4%E5%8E%9A%E7%A7%AF-%E8%87%AA%E4%B8%BB%E5%8F%AF%E6%8E%A7%E5%9C%A8%E6 %AD%A4-%E6%B2%B8%E8%85%BE/. 4 Q.,0,0,1
10.1016/j.neuroimage.2022.118958,cardiﬀ.ac.uk/cardiﬀ-university-brain-research-imaging-center/research/projects/cross-scanner-and-cross-protocol-diﬀusion-mri-data-harmonization,"Datasets (References) Site of Acquisition Data Repository Data Description in Brief MGH HCP Adult Diﬀusion (Fan et al., 2016) MGH ConnectomeDB https://db.humanconnectome.org , or Laboratory of Neuro Imaging Image Data Archive (LONI IDA) https://ida.loni.usc.edu • 35 healthy adults • b -values = 1000, 3000, 5000, 10,000 s/mm 2 • voxel size = 1.5 mm isotropic ISBI 2015 Challenge on White Matter Modeling (Ferizi et al., 2017) MGH The Challenge Website: http://cmic.cs.ucl.ac.uk/wmmchallenge • 1 healthy adult • 48 shells acquired (36 for training, 12 for testing) • b -value up to 45,900 s/mm 2 • voxel size = 2 ×2 ×4 mm MICCAI 2017 & 2018 Challenge on Data Harmonization (Tax et al., 2019) (Ning et al., 2019) Cardiﬀ CUBRIC Center https://www.cardiﬀ.ac.uk/cardiﬀ-university-brain-research-imaging-center/research/projects/cross-scanner-and-cross-protocol-diﬀusion-MRI-data-harmonization • 15 healthy adults, • across 3 scanners and 5 acquisition protocols • b -value up to 5000 s/mm 2 • voxel size down to 1.2 mm isotropic Taxon phantom (Fan et al., 2018b) MGH XNAT Central database https://central.xnat.org (ProjectID: dMRI_Phant_MGH) • Biomimetic phantom, • b -values up to 18,250 s/mm 2 • Δ= 20, 30, 40, 50 ms • voxel size = 2 mm isotropic gSlider diﬀusion data (Wang et al., 2021) MGH Dryad https://doi.org/10.5061/dryad.nzs7h44q2 (Part I) https://doi.org/10.5061/dryad.rjdfn2z8g (Part II) • 1 healthy adult, 9 two-hour sessions • b -values = 1000, 2500s/mm 2 • voxel size = 760 μm MICRA (Koller et al., 2021) CardiﬀOpen Science Framework https://osf.io/z3mkn/• 6 healthy adults, each with 5 repetitions • b -value up to 6000 s/mm 2 • voxel size = 2 mm isotropic • Relaxometry and Quantitative Magnetization Transfer (QMT) data also available MGH Connectome Diﬀusion Microstructure Dataset (CDMD) (Tian et al., 2022) MGH FigShare https://doi.org/10.6084/m9.ﬁgshare.c.5315474.v1 • 26 healthy adults • b -value up to 17,800 s/mm 2 • Δ= 19, 49 ms; 8 b -values per Δ• voxel size = 2 mm isotropic past few years, either in perpetual data repositories (Fan et al., 2018b , 2016 ; Koller et al., 2021 ; Tian et al., 2022 ; Wang et al., 2021) or through organizing computational competitions such as the International Sym-posium on Biomedical Imaging (ISBI) challenge on white matter mod-eling (Ferizi et al., 2017) and Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges on cross-scanner and cross-protocol diﬀusion data harmonization (Ning et al., 2020a ; Tax et al., 2019).",0,1,0
10.1016/j.neuroimage.2022.118958,jp.medical.canon/news/pressrelease/detail/45069-834,The PNS thresholds of an MRI gradient are typically given in terms of minimum stimulating gradient amplitude ΔG as a function of gradient 6 Philips Achieva 3.0T MRI -medical equipment distribution (htig.com). 7 https://www.philips.nl/healthcare/product/HC781271/ingenia-30t-cx-mr-system. 8 https://jp.medical.canon/News/PressRelease/Detail/45069-834. 9 http://demo.united-imaging.com/cn/news/2021/%E8%81%94%E5%BD %B1-cmef-%E5%8D%81%E5%B9%B4%E5%8E%9A%E7%A7%AF-%E8%87%AA%E4%B8%BB%E5%8F%AF%E6%8E%A7%E5%9C%A8%E6 %AD%A4-%E6%B2%B8%E8%85%BE/. 4 Q.,0,0,1
10.1016/j.neuroimage.2022.118958,cmic.cs.ucl.ac.uk/wmmchallenge,"Datasets (References) Site of Acquisition Data Repository Data Description in Brief MGH HCP Adult Diﬀusion (Fan et al., 2016) MGH ConnectomeDB https://db.humanconnectome.org , or Laboratory of Neuro Imaging Image Data Archive (LONI IDA) https://ida.loni.usc.edu • 35 healthy adults • b -values = 1000, 3000, 5000, 10,000 s/mm 2 • voxel size = 1.5 mm isotropic ISBI 2015 Challenge on White Matter Modeling (Ferizi et al., 2017) MGH The Challenge Website: http://cmic.cs.ucl.ac.uk/wmmchallenge • 1 healthy adult • 48 shells acquired (36 for training, 12 for testing) • b -value up to 45,900 s/mm 2 • voxel size = 2 ×2 ×4 mm MICCAI 2017 & 2018 Challenge on Data Harmonization (Tax et al., 2019) (Ning et al., 2019) Cardiﬀ CUBRIC Center https://www.cardiﬀ.ac.uk/cardiﬀ-university-brain-research-imaging-center/research/projects/cross-scanner-and-cross-protocol-diﬀusion-MRI-data-harmonization • 15 healthy adults, • across 3 scanners and 5 acquisition protocols • b -value up to 5000 s/mm 2 • voxel size down to 1.2 mm isotropic Taxon phantom (Fan et al., 2018b) MGH XNAT Central database https://central.xnat.org (ProjectID: dMRI_Phant_MGH) • Biomimetic phantom, • b -values up to 18,250 s/mm 2 • Δ= 20, 30, 40, 50 ms • voxel size = 2 mm isotropic gSlider diﬀusion data (Wang et al., 2021) MGH Dryad https://doi.org/10.5061/dryad.nzs7h44q2 (Part I) https://doi.org/10.5061/dryad.rjdfn2z8g (Part II) • 1 healthy adult, 9 two-hour sessions • b -values = 1000, 2500s/mm 2 • voxel size = 760 μm MICRA (Koller et al., 2021) CardiﬀOpen Science Framework https://osf.io/z3mkn/• 6 healthy adults, each with 5 repetitions • b -value up to 6000 s/mm 2 • voxel size = 2 mm isotropic • Relaxometry and Quantitative Magnetization Transfer (QMT) data also available MGH Connectome Diﬀusion Microstructure Dataset (CDMD) (Tian et al., 2022) MGH FigShare https://doi.org/10.6084/m9.ﬁgshare.c.5315474.v1 • 26 healthy adults • b -value up to 17,800 s/mm 2 • Δ= 19, 49 ms; 8 b -values per Δ• voxel size = 2 mm isotropic past few years, either in perpetual data repositories (Fan et al., 2018b , 2016 ; Koller et al., 2021 ; Tian et al., 2022 ; Wang et al., 2021) or through organizing computational competitions such as the International Sym-posium on Biomedical Imaging (ISBI) challenge on white matter mod-eling (Ferizi et al., 2017) and Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges on cross-scanner and cross-protocol diﬀusion data harmonization (Ning et al., 2020a ; Tax et al., 2019).",0,1,0
10.1016/j.neuroimage.2022.118958,cbs.mpg.de/press-releases/super-brain-scanner-connectom?c,"To date, these sites include: the Cardiﬀ Univer-sity Brain Research Imaging Center (CUBRIC) at Cardiﬀ University in the United Kingdom; the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany; and the Zhangjiang International Brain Imaging Center (ZIC) supported by the Shanghai Municipal Gov-ernment and Fudan University in Shanghai, China. 2 https://www.cardiﬀ.ac.uk/news/view/188635-europes-most-powerful-brain-scanner-arrives. 3 https://www.cbs.mpg.de/press-releases/super-brain-scanner-connectom?c = 7533. 4 https://istbi.fudan.edu.cn/info/1084/1677.htm. 3 Q.",0,0,1
10.1016/j.neuroimage.2022.118958,cardiﬀ.ac.uk/news/view/188635-europes-most-powerful-brain-scanner-arrives,"To date, these sites include: the Cardiﬀ Univer-sity Brain Research Imaging Center (CUBRIC) at Cardiﬀ University in the United Kingdom; the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany; and the Zhangjiang International Brain Imaging Center (ZIC) supported by the Shanghai Municipal Gov-ernment and Fudan University in Shanghai, China. 2 https://www.cardiﬀ.ac.uk/news/view/188635-europes-most-powerful-brain-scanner-arrives. 3 https://www.cbs.mpg.de/press-releases/super-brain-scanner-connectom?c = 7533. 4 https://istbi.fudan.edu.cn/info/1084/1677.htm. 3 Q.",0,0,1
10.1016/j.neuroimage.2022.118958,philips.nl/healthcare/product/hc781271/ingenia-30t-cx-mr-system,The PNS thresholds of an MRI gradient are typically given in terms of minimum stimulating gradient amplitude ΔG as a function of gradient 6 Philips Achieva 3.0T MRI -medical equipment distribution (htig.com). 7 https://www.philips.nl/healthcare/product/HC781271/ingenia-30t-cx-mr-system. 8 https://jp.medical.canon/News/PressRelease/Detail/45069-834. 9 http://demo.united-imaging.com/cn/news/2021/%E8%81%94%E5%BD %B1-cmef-%E5%8D%81%E5%B9%B4%E5%8E%9A%E7%A7%AF-%E8%87%AA%E4%B8%BB%E5%8F%AF%E6%8E%A7%E5%9C%A8%E6 %AD%A4-%E6%B2%B8%E8%85%BE/. 4 Q.,0,0,1
10.1016/j.neuroimage.2022.118958,osf.io/z3mkn/•,"Datasets (References) Site of Acquisition Data Repository Data Description in Brief MGH HCP Adult Diﬀusion (Fan et al., 2016) MGH ConnectomeDB https://db.humanconnectome.org , or Laboratory of Neuro Imaging Image Data Archive (LONI IDA) https://ida.loni.usc.edu • 35 healthy adults • b -values = 1000, 3000, 5000, 10,000 s/mm 2 • voxel size = 1.5 mm isotropic ISBI 2015 Challenge on White Matter Modeling (Ferizi et al., 2017) MGH The Challenge Website: http://cmic.cs.ucl.ac.uk/wmmchallenge • 1 healthy adult • 48 shells acquired (36 for training, 12 for testing) • b -value up to 45,900 s/mm 2 • voxel size = 2 ×2 ×4 mm MICCAI 2017 & 2018 Challenge on Data Harmonization (Tax et al., 2019) (Ning et al., 2019) Cardiﬀ CUBRIC Center https://www.cardiﬀ.ac.uk/cardiﬀ-university-brain-research-imaging-center/research/projects/cross-scanner-and-cross-protocol-diﬀusion-MRI-data-harmonization • 15 healthy adults, • across 3 scanners and 5 acquisition protocols • b -value up to 5000 s/mm 2 • voxel size down to 1.2 mm isotropic Taxon phantom (Fan et al., 2018b) MGH XNAT Central database https://central.xnat.org (ProjectID: dMRI_Phant_MGH) • Biomimetic phantom, • b -values up to 18,250 s/mm 2 • Δ= 20, 30, 40, 50 ms • voxel size = 2 mm isotropic gSlider diﬀusion data (Wang et al., 2021) MGH Dryad https://doi.org/10.5061/dryad.nzs7h44q2 (Part I) https://doi.org/10.5061/dryad.rjdfn2z8g (Part II) • 1 healthy adult, 9 two-hour sessions • b -values = 1000, 2500s/mm 2 • voxel size = 760 μm MICRA (Koller et al., 2021) CardiﬀOpen Science Framework https://osf.io/z3mkn/• 6 healthy adults, each with 5 repetitions • b -value up to 6000 s/mm 2 • voxel size = 2 mm isotropic • Relaxometry and Quantitative Magnetization Transfer (QMT) data also available MGH Connectome Diﬀusion Microstructure Dataset (CDMD) (Tian et al., 2022) MGH FigShare https://doi.org/10.6084/m9.ﬁgshare.c.5315474.v1 • 26 healthy adults • b -value up to 17,800 s/mm 2 • Δ= 19, 49 ms; 8 b -values per Δ• voxel size = 2 mm isotropic past few years, either in perpetual data repositories (Fan et al., 2018b , 2016 ; Koller et al., 2021 ; Tian et al., 2022 ; Wang et al., 2021) or through organizing computational competitions such as the International Sym-posium on Biomedical Imaging (ISBI) challenge on white matter mod-eling (Ferizi et al., 2017) and Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges on cross-scanner and cross-protocol diﬀusion data harmonization (Ning et al., 2020a ; Tax et al., 2019).",0,1,0
10.1016/j.neuroimage.2022.119488,nitric.org,"Diﬀusion mapping Diﬀusion-weighted (DWI) spin-echo echo-planar images were con-verted from DICOM to NIfTI format (www.nitric.org ; (Li et al., 2016)), and then processed using the FMRIB Diﬀusion Toolbox DTIFIT (Behrens et al., 2003).",1,0,0
10.1016/j.neuroimage.2022.119488,psmd-marker.com,"The peak width of skeletonized mean diﬀusivity, PSMD, (Baykara et al., 2016) was calculated using FSL and a publicly available script (http://www.psmd-marker.com , version 1.0).",1,0,0
10.1016/j.neuroimage.2022.119488,fmrib.ox.ac.uk,"Fractional anisotropy (FA), mean (MD), radial (RD) and axial (AxD) diﬀusivity (mm 2/s) were de-termined for selected white matter tracts using the Johns Hopkins University white matter atlas (Mori et al., 2005) available in FSL (http://www.fmrib.ox.ac.uk).",1,0,0
10.1016/j.neuroimage.2022.119294,github.com/vicco-group/frrsa,"To facilitate future use of this method, we provide a toolbox to run FR-RSA in Python (https://github.com/ViCCo-Group/frrsa), with recommenda-tions regarding implementational choices.",1,0,0
10.1016/j.neuroimage.2022.119205,strimmerlab.org/software/sda,"Whole-brain classiﬁcation models were generated using shrinkage discriminant analysis (SDA; Strimmer Lab, Imperial College London; strimmerlab.org/software/sda).",1,0,0
10.1016/j.neuroimage.2022.119205,github.com/poldracklab/fmriprep,"fMRI preprocessing Volumes were converted to NIFTI format and preprocessed us-ing the Brain Imaging Data Structure (BIDS; Gorgolewski et al., 2016) fmriprep pipeline version 1.18 (Esteban et al., 2019 ; https://github.com/poldracklab/fmriprep).",1,0,0
10.1016/j.neuroimage.2022.118878,cinetic.arts.ro/en/met-2,"Participants and data collection > Participants in the study were recruited through advertisement within the University of Bucharest, University of theater and Film and the CINETic’s Research center website https://cinetic.arts.ro/en/met-2/.",0,0,1
10.1016/j.neuroimage.2022.118878,car-toolcommunity.unige.ch,"In order to determine the optimal number of clusters at the individual and the group level, we used the criteria implemented in Cartool (a free academic software developed by Denis Brunet; car-toolcommunity.unige.ch), based on seven maximally independent cri-teria: Davies and Bouldin, Gamma, Silhouette, Dunn Robust, Point-Biserial, Krzanowski-Lai Index, and Cross-Validation (Custo et al., 2017 ; Murray et al., 2008 ; Brunet et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.118878,ant-neuro.com,EEG data were acquired in a dimly light room using a 128-channel ANT Neuro Waveguard System (https://www.ant-neuro.com/) sampled online at 1kHZ with a Cz reference.,1,0,0
10.1016/j.neuroimage.2022.118878,cartoolcommunity.unige.ch,"The free academic software Cartool (cartoolcommunity.unige.ch) was used for the EEG data processing and microstate analysis (Brunet et al., 2011)., The Cartool software (cartoolcommunity.unige.ch) has been pro-grammed by Denis Brunet, from the Functional Brain Mapping Labora-tory (FBMLab), Geneva, Switzerland, and is supported by the Center for Biomedical Imaging (CIBM) of Geneva and Lausanne.",1,0,0
10.1016/j.neuroimage.2022.119637,brain-development.org/ixi-dataset,We conducted a semi-automatic quality control (QC) guided with quality metrics leading to a selection of images that meet the quality criteria for all three pre-processing pipelines (see Fig. 5 and the detailed QC per pre-processing below). 1 https://brain-development.org/ixi-dataset. 2 http://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html. 3 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html. 2 B.,0,1,0
10.1016/j.neuroimage.2022.119637,fcon_1000.projects.nitrc.org/indi/abide/abide_ii.html,We conducted a semi-automatic quality control (QC) guided with quality metrics leading to a selection of images that meet the quality criteria for all three pre-processing pipelines (see Fig. 5 and the detailed QC per pre-processing below). 1 https://brain-development.org/ixi-dataset. 2 http://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html. 3 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html. 2 B.,0,1,0
10.1016/j.neuroimage.2022.119637,baobablab.github.io/bhb/challenges/age_prediction_with_site_removal,"is described on the web page: https://baobablab.github.io/bhb/challenges/age_prediction_with_site_removal., Their implementation is available on challenge web page : https://baobablab.github.io/bhb/challenges/age_prediction_with_site_removal.",1,0,0
10.1016/j.neuroimage.2022.119637,neuro.uni-jena.de/cat,CAT12 VBM Steps: Voxel-Based Morphometry (VBM) was performed with CAT12 Gaser and Dahnke (2016) (http://www.neuro.uni-jena.de/cat).,1,0,0
10.1016/j.neuroimage.2022.119637,ramp.studio,"Acknowledgments We are exceedingly grateful to Alexandre Gramfort, Francois Caud, Guillaume Lemaître, Thomas Moreau (Université Paris-Saclay, Inria, CEA) and Roman Yurchak (Symerio SME, Paris area) to provide access and active support to the challenge platform: https://ramp.studio.",0,0,1
10.1016/j.neuroimage.2022.119637,fcon_1000.projects.nitrc.org/indi/abide/abide_i.html,We conducted a semi-automatic quality control (QC) guided with quality metrics leading to a selection of images that meet the quality criteria for all three pre-processing pipelines (see Fig. 5 and the detailed QC per pre-processing below). 1 https://brain-development.org/ixi-dataset. 2 http://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html. 3 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html. 2 B.,0,1,0
10.1016/j.neuroimage.2022.119637,baobablab.github.io/bhb,"All up-to-date information are centralized at this lo-cation https://baobablab.github.io/bhb., is described on the web page: https://baobablab.github.io/bhb/challenges/age_prediction_with_site_removal., Their implementation is available on challenge web page : https://baobablab.github.io/bhb/challenges/age_prediction_with_site_removal.",0,0,1
10.1016/j.neuroimage.2022.119637,github.com/jfortin1/neurocombat_sklearn,"This is especially true for DenseNet, the best performing net-work on both VBM and Quasi-Raw on the internal test, one of the mod-els that retains the most site information (8.0% Bacc and 15.2% Bacc 4 https://github.com/Jfortin1/neurocombat_sklearn. 11 B.",1,0,0
10.1016/j.neuroimage.2022.119637,brainprep.readthedocs.io,The project hosting the codes is freely accessible at https://brainprep.readthedocs.io.,1,0,0
10.1016/j.neuroimage.2022.119637,surfer.nmr.mgh.harvard.edu,FreeSurfer Steps: Cortical analysis was performed with FreeSurfer “recon-all ”(https://surfer.nmr.mgh.harvard.edu).,1,0,0
10.1016/j.neuroimage.2022.119461,eeglab.org,EEG preprocessing was performed using EEGLAB toolbox (https://eeglab.org) and FASTER plugin (https://sourceforge.net/projects/faster/).,1,0,0
10.1016/j.neuroimage.2022.119461,sourceforge.net/projects/faster,EEG preprocessing was performed using EEGLAB toolbox (https://eeglab.org) and FASTER plugin (https://sourceforge.net/projects/faster/).,1,0,0
10.1016/j.neuroimage.2022.119461,mgh.harvard.edu,mgh.harvard.edu).,0,0,1
10.1016/j.neuroimage.2022.119461,mathworks.com/help/stats/kmeans.html,Clustering analysis was conducted using the MATLAB kmeans function (https://www.mathworks.com/help/stats/kmeans.html).,1,0,0
10.1016/j.neuroimage.2022.119485,abcdstudy.org/consortium_members,A listing of participat-ing sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.,0,0,1
10.1016/j.neuroimage.2022.119485,abcdstudy.org/federal-partners.html,A full list of supporters is available at https://abcdstudy.org/federal-partners.html.,0,0,1
10.1016/j.neuroimage.2022.119485,abcdstudy.org,"Data and code availability All data used in this study are openly available and can be ac-cessed directly from the HCP (https://www.humanconnectome.org/study/hcp-young-adult) and ABCD (https://abcdstudy.org/) websites., A full list of supporters is available at https://abcdstudy.org/federal-partners.html., A listing of participat-ing sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.",0,1,0
10.1016/j.neuroimage.2022.119485,humanconnectome.org/study/hcp-young-adult,Data and code availability All data used in this study are openly available and can be ac-cessed directly from the HCP (https://www.humanconnectome.org/study/hcp-young-adult) and ABCD (https://abcdstudy.org/) websites.,0,1,0
10.1016/j.neuroimage.2022.119485,github.com/elvisha/neuroanatomical-predictions-of-behaviour,Code used to generate the results presented here are available on GitHub (https://github.com/elvisha/neuroanatomical-predictions-of-behaviour).,1,0,0
10.1016/j.neuroimage.2022.119485,abcdstudy.org,"Data and code availability All data used in this study are openly available and can be ac-cessed directly from the HCP (https://www.humanconnectome.org/study/hcp-young-adult) and ABCD (https://abcdstudy.org/) websites., Data used in the preparation of this article were also obtained from the Adolescent Brain Cognitive Development SM (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA)., A full list of supporters is available at https://abcdstudy.org/federal-partners.html., A listing of participat-ing sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.",0,1,0
10.1016/j.neuroimage.2022.119620,github.com/gjertrud/lea,"LEA Matlab code, and instructions on how to use it, are available at https://github.com/Gjertrud/LEA., Matlab code for Likelihood Es-timation of Aﬃnity (LEA) is available at https://github.com/Gjertrud/LEA.",1,0,0
10.1016/j.neuroimage.2022.119620,github.com/martinschain/leo,"LEO Matlab code, and instructions on how to use it, are available at https://github.com/martinschain/LEO., Matlab code for Likelihood Estimation of Occupancy (LEO) is available at https://github.com/martinschain/LEO.",1,0,0
10.1016/j.neuroimage.2022.119208,zenodo.org/record/3773316,Data/code availability The Matlab code of the simulation framework is publicly avail-able at https://zenodo.org/record/3773316 and https://gitlab.lrz.de/nmrm_lab/public_projects/bold-simulation.,1,0,0
10.1016/j.neuroimage.2022.119208,gitlab.lrz.de/nmrm_lab/public_projects/bold-simulation,Data/code availability The Matlab code of the simulation framework is publicly avail-able at https://zenodo.org/record/3773316 and https://gitlab.lrz.de/nmrm_lab/public_projects/bold-simulation.,1,0,0
10.1016/j.neuroimage.2022.119621,adni.loni.usc.edu/data-samples/access-data,Data access requires acceptance of the ADNI Data Use Agreement (https://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Data_Use_Agreement.pdf) and online application form (https://adni.loni.usc.edu/data-samples/access-data/).,0,0,1
10.1016/j.neuroimage.2022.119621,adni.loni.usc.edu,"To study the tem-poral pattern, we utilized longitudinal data from the Alzheimer’s Dis-ease Neuroimaging Initiative (ADNI) (http://adni.loni.usc.edu/) from individuals with up to 8 years of follow-up., Participants Data were downloaded from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database that included ADNI1, ADNI-GO, ADNI2, and ADNI3 (adni.loni.usc.edu/)., DATA availability statement Data used in this study are publicly available through the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., Data access requires acceptance of the ADNI Data Use Agreement (https://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Data_Use_Agreement.pdf) and online application form (https://adni.loni.usc.edu/data-samples/access-data/).",0,1,0
10.1016/j.neuroimage.2022.119621,adni.loni.usc.edu,"To study the tem-poral pattern, we utilized longitudinal data from the Alzheimer’s Dis-ease Neuroimaging Initiative (ADNI) (http://adni.loni.usc.edu/) from individuals with up to 8 years of follow-up., Participants Data were downloaded from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database that included ADNI1, ADNI-GO, ADNI2, and ADNI3 (adni.loni.usc.edu/)., Data access requires acceptance of the ADNI Data Use Agreement (https://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Data_Use_Agreement.pdf) and online application form (https://adni.loni.usc.edu/data-samples/access-data/).",0,1,0
10.1016/j.neuroimage.2022.119621,adni.loni.usc.edu,"To study the tem-poral pattern, we utilized longitudinal data from the Alzheimer’s Dis-ease Neuroimaging Initiative (ADNI) (http://adni.loni.usc.edu/) from individuals with up to 8 years of follow-up.",0,1,0
10.1016/j.neuroimage.2022.119621,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119621,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_data_use_agreement.pdf,Data access requires acceptance of the ADNI Data Use Agreement (https://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Data_Use_Agreement.pdf) and online application form (https://adni.loni.usc.edu/data-samples/access-data/).,0,0,1
10.1016/j.neuroimage.2022.119355,nitrc.org/projects/conn,Data and code availability statement The CONN toolbox is freely available online (http://www.nitrc.org/projects/conn).,1,0,0
10.1016/j.neuroimage.2022.119355,nitrc.org/projects/conn,"NeuroImage 258 (2022) 119355 (www.nitrc.org/projects/conn , RRID:SCR_009550; Whitﬁeld-Gabrieli and Nieto-Castanon, 2012)., Data and code availability statement The CONN toolbox is freely available online (http://www.nitrc.org/projects/conn).",1,0,0
10.1016/j.neuroimage.2022.119355,github.com/kkim-codearchive/aﬀectneurostudy,The code for inter-subject similarity analysis is available https://github.com/kkim-codeArchive/aﬀectNeuroStudy.,1,0,0
10.1016/j.neuroimage.2022.119355,github.com/pzeidman/dcm-peb-example,DCM/PEB: The code for the DCM/PEB analysis is freely available online (https://github.com/pzeidman/dcm-peb-example).,1,0,0
10.1016/j.neuroimage.2022.119355,neurosynth.org,"We used the Neurosynth (www.neurosynth.org) to select 2200 studies contained the keywords of ‘theory of mind’, ‘valence’, and ‘emotion’ and subﬁelds of ‘emotion’ including ‘emotional responses’ that showed the rate of keyword appearance above 5% in the main text., The customized meta-analysis mask was created from the python package NeuroSynth (www.neurosynth.org).",1,0,0
10.1016/j.neuroimage.2022.118957,surfer.nmr.mgh.harvard.edu,"The hippocampal subﬁelds, in-cluding the parasubiculum, presubiculum, subiculum, cornu ammonis areas (CA1, CA2/3 composite, CA4), granule cell layer of the den-tate gyrus, ﬁmbria, molecular layer of hippocampus (MolLayer), hip-pocampal ﬁssure and hippocampal tail, were generated from the reg-istered pre-contrast T1W data using the hippocampus segmentation tool in Freesurfer (version 6.0.0, http://surfer.nmr.mgh.harvard.edu/) (Collins et al., 1994 , Fischl et al., 2004 , Fischl et al., 2002 , Iglesias et al., 2015)., Similarly, the hippocampus subﬁeld segmentation was performed using Freesurfer (version 6.0.0, http://surfer.nmr.mgh.harvard.edu/).",1,0,0
10.1016/j.neuroimage.2022.118957,nitrc.org/projects/mricron,"MRIcron (Rorden and Brett, 2000) (v1.0.2, https://www.nitrc.org/projects/mricron/) was used for visualizing the SWI PGAC images with overlays of hippocampal subﬁelds, MVM, MRV avg and MRA nl.",1,0,0
10.1016/j.neuroimage.2022.118957,mathworks.com/matlabcentral/ﬁleexchange/24409-hessian-based-frangi-vesselness-ﬁlter,The process of image registration can be reproduced using the SPM12 tool (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and the MATLAB function of the 3D vesselness ﬁlter can be downloaded from (https://www.mathworks.com/matlabcentral/ﬁleexchange/24409-hessian-based-frangi-vesselness-ﬁlter).,1,0,0
10.1016/j.neuroimage.2022.118957,mathworks.com/matlabcentral/ﬁleexchange/24409-hessian-based,The process of image registration can be reproduced using the SPM12 tool (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and the MATLAB function of the 3D vesselness ﬁlter can be downloaded from (https://www.mathworks.com/matlabcentral/ﬁleexchange/24409-hessian-based-frangi-vesselness-ﬁlter).,1,0,0
10.1016/j.neuroimage.2022.118957,ﬁl.ion.ucl.ac.uk/spm/software/spm12,The process of image registration can be reproduced using the SPM12 tool (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and the MATLAB function of the 3D vesselness ﬁlter can be downloaded from (https://www.mathworks.com/matlabcentral/ﬁleexchange/24409-hessian-based-frangi-vesselness-ﬁlter).,1,0,0
10.1016/j.neuroimage.2022.118957,spintechimaging.com/products/research-software,"Manual contouring, data sorting and simple image ma-nipulations were performed using SPIN-Research software (https://spintechimaging.com/products/research-software/)., Manual contouring, data sorting and simple image manipulations were performed using SPIN-Research software (https://spintechimaging.com/products/research-software/).",1,0,0
10.1016/j.neuroimage.2022.119741,sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmark,"The dataset, the baseline model, and all necessary codes to reproduce the experiments are available at https://github.com/MICLab-Unicamp/HypAST and https://sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmarking., We also used a post-processing step to remove 1 https://sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmark. 2 https://codalab.lisn.upsaclay.fr/competitions/7583. 3 https://github.com/MICLab-Unicamp/HypAST.",0,1,0
10.1016/j.neuroimage.2022.119741,codalab.lisn.upsaclay.fr/competitions/7583,We also used a post-processing step to remove 1 https://sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmark. 2 https://codalab.lisn.upsaclay.fr/competitions/7583. 3 https://github.com/MICLab-Unicamp/HypAST.,0,1,0
10.1016/j.neuroimage.2022.119741,2.5.3.1,2.5.3.1.,0,0,1
10.1016/j.neuroimage.2022.119741,github.com/miclab-unicamp/hypast,"The dataset, the baseline model, and all necessary codes to reproduce the experiments are available at https://github.com/MICLab-Unicamp/HypAST and https://sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmarking., We also used a post-processing step to remove 1 https://sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmark. 2 https://codalab.lisn.upsaclay.fr/competitions/7583. 3 https://github.com/MICLab-Unicamp/HypAST.",0,1,0
10.1016/j.neuroimage.2022.119741,sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmarking,"The dataset, the baseline model, and all necessary codes to reproduce the experiments are available at https://github.com/MICLab-Unicamp/HypAST and https://sites.google.com/view/calgary-campinas-dataset/hypothalamus-benchmarking.",0,1,0
10.1016/j.neuroimage.2022.119039,github.com/luistrujillo11/aging.git,Data and code availability statement https://doi.org/10.5281/zenodo.5329351 This doi link contains the ﬁles of the preprocessed structural T1 res-onances and the relative volumes of each subject https://github.com/LuisTrujillo11/Aging.git This link contains the code in created in Rstudio used for the general lineal model of Deformation Based Morphometry.,1,0,0
10.1016/j.neuroimage.2022.119039,cobralab.ca,"Devenyi at the Computational Brain Anatomy Lab (CoBrA Lab) (http://cobralab.ca), CIC, Douglas Research Centre, Montreal and Compute Canada (www.computecanada.ca).",0,0,1
10.1016/j.neuroimage.2022.119039,computecanada.ca,"Devenyi at the Computational Brain Anatomy Lab (CoBrA Lab) (http://cobralab.ca), CIC, Douglas Research Centre, Montreal and Compute Canada (www.computecanada.ca).",0,0,1
10.1016/j.neuroimage.2022.119039,m.sc,We also would like to thank M.Sc.,0,0,1
10.1016/j.neuroimage.2022.119039,github.com/mouse-imageing-centre/pydpiper,"Morpho-logical analysis was performed by converting DICOM to MINC for-mat, and then preprocessed using an in-house pipeline based on MINC-Tools (https://github.com/CoBrALab/minc-toolkit-extras/blob/master/mouse-preprocessing-v5.sh) and the pydpiper pipeline (https://github.com/Mouse-Imageing-Centre/pydpiper).",1,0,0
10.1016/j.neuroimage.2022.119039,biorender.com,Created with BioRender.com 3 M.,1,0,0
10.1016/j.neuroimage.2022.119039,github.com/cobralab/minc-toolkit-extras/blob/master/mouse-preprocessing-v5.sh,"Morpho-logical analysis was performed by converting DICOM to MINC for-mat, and then preprocessed using an in-house pipeline based on MINC-Tools (https://github.com/CoBrALab/minc-toolkit-extras/blob/master/mouse-preprocessing-v5.sh) and the pydpiper pipeline (https://github.com/Mouse-Imageing-Centre/pydpiper).",1,0,0
10.1016/j.neuroimage.2022.119039,1.5.2.2,"All analyses were per-formed using pydpiper version 1.8 (Friedel et al., 2014), R studio version 3.6.3 (Rstudio, 2020), and the RMINC version 1.5.2.2 (Lerch, J., 2017) and tidyverse version 1.3.1 (Wickham, H., 2017) packages.",1,0,0
10.1016/j.neuroimage.2022.119681,ﬁl.ion.ucl.ac.uk/spm/data/mmfaces,"Thus, by taking the leverage of the beamformer’s high spa-tial resolution while simultaneously localizing and segregating the mul-tiple simultaneous sources with ICA There are two pilot studies which attempted to demonstrate source ICA on a single-subject face processing data in EEG (Jonmohamadi et al., 2014) and in MEG (Jonmohamadi and Jones, 2015), on the same single healthy subject, which is openly avail-able (www.ﬁl.ion.ucl.ac.uk/spm/data/mmfaces/).",0,1,0
10.1016/j.neuroimage.2022.119681,surfer.nmr.mgh.harvard.edu,FreeSurfer (http://surfer.nmr.mgh.harvard.edu/) was used to generate a brain mesh from the individual MRI.,1,0,0
10.1016/j.neuroimage.2022.119681,ins-amu.fr/software,"The toolboxes used in this work are available at https://meg.univ-amu.fr/wiki/Main_Page and https://ins-amu.fr/software., The toolboxes used in this work are available at https://meg.univ-amu.fr/wiki/Main_Page and https://ins-amu.fr/software.",1,0,0
10.1016/j.neuroimage.2022.119681,meg.univ-amu.fr/wiki/main_page,"The toolboxes used in this work are available at https://meg.univ-amu.fr/wiki/Main_Page and https://ins-amu.fr/software., The toolboxes used in this work are available at https://meg.univ-amu.fr/wiki/Main_Page and https://ins-amu.fr/software.",1,0,0
10.1016/j.neuroimage.2022.119509,github.com/rpomponio/neuroharmonize,3.8.5) scripts (https://github.com/rpomponio/neuroHarmonize).,1,0,0
10.1016/j.neuroimage.2022.119509,enigma.ini.usc.edu/protocols/imaging-protocols,"Imaging data preprocessing Anatomical brain images were preprocessed at Duke University through a standardized neuroimaging and QC pipeline developed by the ENIGMA Consortium (http://enigma.ini.usc.edu/protocols/imaging-protocols/) (Logue et al., 2018).",0,1,0
10.1016/j.neuroimage.2022.119509,4n8ygg-delin-sun.shinyapps.io/sdl_shiny,"Results As shown in Fig. 1 and the interactive plot at https://4n8ygg-delin-sun.shinyapps.io/SDL_Shiny/, data distribution and age-related slops are largely modulated by site.",0,0,1
10.1016/j.neuroimage.2022.119509,github.com/jfortin1/combatharmonization,The ComBat approach was implemented using R scripts (https://github.com/Jfortin1/ComBatHarmonization) running on RStu-dio (ver.,1,0,0
10.1016/j.neuroimage.2022.119509,surfer.nmr.mgh.harvard.edu,"Cortical thickness mea-surements were generated using the FreeSurfer software (https://surfer.nmr.mgh.harvard.edu) based on the Destrieux atlas (Destrieux et al., 2010) that contains 74 regions per hemisphere.",1,0,0
10.1016/j.neuroimage.2021.118718,drcmr.dk/powder-averaging-dataset,Data availability and ethics The data that support the ﬁndings of this study are publicly avail-able on the download center of the Danish Research Centre for Magnetic Resonance (https://www.drcmr.dk/powder-averaging-dataset).,0,1,0
10.1016/j.neuroimage.2022.119589,nda.nih.gov/abcd,(2019) 1 ABCD 2188 (1144F) Healthy 1-2 -20 https://nda.nih.gov/abcd Lake et al.,0,0,1
10.1016/j.neuroimage.2022.119589,myconnectome.org/wp/3,(2019) 1 MSC 10 (5F) Healthy 10 2 30 https://openneuro.org/datasets/ds000224/versions/00001. 2 MyConnectome 1 (0F) Healthy 84 -10 http://myconnectome.org/wp/3 HCP 384 (174F) Healthy 2 -30 https://www.humanconnectome.org 4 WashU 120 120 (60F) Healthy 1 -14 https://legacy.openfmri.org/dataset/ds000243/Shah et al.,0,0,1
10.1016/j.neuroimage.2022.119589,imaginggenomics.net.au/projects/qtim/gordon,(2016) 1 HNU 30 (15F) Healthy 10 -10 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/hnu_1.html 2 eNKI-TRT 20 (4F) Representative Sample 2 -10 http://fcon_1000.projects.nitrc.org/indi/pro/eNKI_RS_TRT/FrontPage.html 3 QTIM 272 (204F) Twins 1 -5 https://imaginggenomics.net.au/projects/qtim/Gordon et al.,0,0,1
10.1016/j.neuroimage.2022.119589,dataverse.harvard.edu/dataset.xhtml?persistentid,"(2015) 1 -25 (9F) Healthy 5 -12 -2 HCP 100 (54F) Healthy 2 7 30 https://www.humanconnectome.org 3 GSP 104 (56F) Healthy 1 -6 https://dataverse.harvard.edu/dataset.xhtml?persistentId = doi:10., (2018) 1 HNU 30 (15F) Healthy 10 -10 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/hnu_1.html 2 GSP 40 (15F) Healthy 2 -6 https://dataverse.harvard.edu/dataset.xhtml?persistentId = doi:10.",0,0,1
10.1016/j.neuroimage.2022.119589,cam-can.org/index.php?content,(2015) 1 Cam-CAN 632 (320F) Healthy 1 2 8.5 https://www.cam-can.org/index.php?content = dataset Anderson et al.,0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/adhd200/horien,"(2019) 1 ABIDE 1044 (NA) Healthy, Autism Spectrum Disorder 2-3 1 8 ± 2 (mean ± SD) https://fcon_1000.projects.nitrc.org/indi/abide/2 ADHD-200 776 (NA) Healthy, ADHD 1 1 24 https://fcon_1000.projects.nitrc.org/indi/adhd200/Horien et al.",0,0,1
10.1016/j.neuroimage.2022.119589,openneuro.org/datasets/ds000224/versions/00001,"(2020) 1 -30 (9F) Healthy pediatric 2 -6-12 -2 MSC 20 (5F) Healthy 2 -30 https://openneuro.org/datasets/ds000224/versions/00001. 3 -30 (24F) Healthy pediatric 1 -6-12 -4 HCP 50 (26F) Healthy 1 -15-30 https://www.humanconnectome.org 5 -38 (16F) Healthy pediatric 1 -6-12 -6 HCP 50 (26F) Healthy 1 -15-30 https://www.humanconnectome.org 7 -34 (12F) Healthy pediatric 1 -15-30 -Finn et al., (2017c) 1 MSC 10 (5F) Healthy 10 3 30 https://openneuro.org/datasets/ds000224/versions/00001 Gordon et al., (2020) 1 MSC 10 (5F) Healthy 1-10 3 30 https://openneuro.org/datasets/ds000224/versions/00001., (2018) 1 MSC 10 (5F) Healthy 10 3 30 https://openneuro.org/datasets/ds000224/versions/00001 Greene et al., (2020) 1 MSC 10 (5F) Healthy 10 -30 https://openneuro.org/datasets/ds000224/versions/00001., (2020) 1 -4 (0F) Healthy 12-24 -174-348 -2 MSC 10 (5F) Healthy 10 -30 https://openneuro.org/datasets/ds000224/versions/00001. 3 CAST 3 (1F) Healthy 42-64 -30 https://openneuro.org:443/datasets/ds002766 4 MyConnectome 1 (0F) Healthy 104 6 10 http://openfmri.org/dataset/ds000031 Sylvester et al., (2020) 1 MSC 10 (5F) Healthy 10 -30 https://openneuro.org/datasets/ds000224/versions/00001., (2020) 1 -1 (0F) Healthy 33 6 14 -2 MSC 10 (5F) Healthy 10 3 30 https://openneuro.org/datasets/ds000224/versions/00001. 3 HCP 514 (NA) Healthy 2 7 30 https://www.humanconnectome.org Seitzman et al., (2019) 1 MSC 10 (5F) Healthy 10 2 30 https://openneuro.org/datasets/ds000224/versions/00001. 2 MyConnectome 1 (0F) Healthy 84 -10 http://myconnectome.org/wp/3 HCP 384 (174F) Healthy 2 -30 https://www.humanconnectome.org 4 WashU 120 120 (60F) Healthy 1 -14 https://legacy.openfmri.org/dataset/ds000243/Shah et al., (2021) 1 MSC DS1: 10 (5F) Healthy 10 3 30 https://openneuro.org/datasets/ds000224/versions/00001., (2018) 1 MSC 10 (5F) Healthy 10 3 30 https://openneuro.org/datasets/ds000224/versions/00001.",0,0,1
10.1016/j.neuroimage.2022.119589,legacy.openfmri.org/dataset/ds000243/shah,(2019) 1 MSC 10 (5F) Healthy 10 2 30 https://openneuro.org/datasets/ds000224/versions/00001. 2 MyConnectome 1 (0F) Healthy 84 -10 http://myconnectome.org/wp/3 HCP 384 (174F) Healthy 2 -30 https://www.humanconnectome.org 4 WashU 120 120 (60F) Healthy 1 -14 https://legacy.openfmri.org/dataset/ds000243/Shah et al.,0,0,1
10.1016/j.neuroimage.2022.119589,nitrc.org/projects/pnc,"(2019) 1 HCP 515 (274F) Healthy 1 7 30 https://www.humanconnectome.org 2 PNC 571 (320F) Healthy 1 2 6 https://www.nitrc.org/projects/pnc Greene et al., (2018) 1 HCP 515 (274F) Healthy 1 7 30 https://www.humanconnectome.org 2 PNC 571 (320F) Healthy 1 2 6 https://www.nitrc.org/projects/pnc Jiang et al.",0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/corr/html/hnu_1.html,"(2018) 1 HNU 30 (15F) Healthy 10 -10 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/hnu_1.html 2 GSP 40 (15F) Healthy 2 -6 https://dataverse.harvard.edu/dataset.xhtml?persistentId = doi:10., (2016) 1 HNU 30 (15F) Healthy 10 -10 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/hnu_1.html 2 eNKI-TRT 20 (4F) Representative Sample 2 -10 http://fcon_1000.projects.nitrc.org/indi/pro/eNKI_RS_TRT/FrontPage.html 3 QTIM 272 (204F) Twins 1 -5 https://imaginggenomics.net.au/projects/qtim/Gordon et al.",0,0,1
10.1016/j.neuroimage.2022.119589,nitrc.org/projects/multimodal,(2015) 1 Kirby 1 (0F) Healthy 158 -7 http://www.nitrc.org/projects/kirbyweekly 2 Kirby 21 (10F) Healthy 1 -7 http://www.nitrc.org/projects/multimodal Airan et al.,0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/abide/2,"(2019) 1 ABIDE 1044 (NA) Healthy, Autism Spectrum Disorder 2-3 1 8 ± 2 (mean ± SD) https://fcon_1000.projects.nitrc.org/indi/abide/2 ADHD-200 776 (NA) Healthy, ADHD 1 1 24 https://fcon_1000.projects.nitrc.org/indi/adhd200/Horien et al.",0,0,1
10.1016/j.neuroimage.2022.119589,zenodo.org/record/3350885#.yuiusj1kiuk,(2020) † 1 -1 (0F) Healthy 25 -9-10 https://zenodo.org/record/3350885#.YUIUSJ1KiUk 2 HNU 30 (15F) Healthy 10 -10 https://ﬁgshare.com/s/7dac285e153e176d90e8 Chen et al.,0,0,1
10.1016/j.neuroimage.2022.119589,openfmri.org/dataset/ds000031,"(2020) 1 -4 (0F) Healthy 12-24 -174-348 -2 MSC 10 (5F) Healthy 10 -30 https://openneuro.org/datasets/ds000224/versions/00001. 3 CAST 3 (1F) Healthy 42-64 -30 https://openneuro.org:443/datasets/ds002766 4 MyConnectome 1 (0F) Healthy 104 6 10 http://openfmri.org/dataset/ds000031 Sylvester et al., (2015) 1 MyConnectome 1 (0F) Healthy 104 6 10 http://openfmri.org/dataset/ds000031 Laumann et al., (2015) 1 MyConnectome 1 (0F) Healthy 84 6 10 http://openfmri.org/dataset/ds000031 2 -1 (0F) Healthy 10 -30 -3 WashU 120 120 (60F) Healthy 1 -14 https://legacy.openfmri.org/dataset/ds000243/(continued on next page) 3 K.J.",0,0,1
10.1016/j.neuroimage.2022.119589,preprocessed-connectomes-project.org/adhd200/geerligs,"(2016) 1 -25 (13F) Healthy 1 1 12 -2 ADHD-200 113 (35F) Healthy, ADHD 1 1 24 http://preprocessed-connectomes-project.org/adhd200/Geerligs et al.",0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/retro/simon.html,(2019) † 1 SIMON 1 (0F) Healthy 73 -9-10 http://fcon_1000.projects.nitrc.org/indi/retro/SIMON.html Braga and Buckner (2017) 1 -4 (4F) Healthy 24 -7 -Brennan et al.,0,0,1
10.1016/j.neuroimage.2022.119589,legacy.openfmri.org/dataset/ds000243/marek,(2017a) 1 -1 (NA) Healthy 10 -30 2 WashU 120 120 (60F) Healthy 1 -14 https://legacy.openfmri.org/dataset/ds000243/Marek et al.,0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/2,"(2019) 1 SLIM 105 (49F) Healthy 3 -8 http://fcon_1000.projects.nitrc.org/2 CoRR 93 (45F) Healthy 2-3 -5 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/samples.html 3 CoRR 79 (58F) Healthy 2 -12 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/samples.html 4 CoRR 26 (0F) Healthy 2 -24 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/samples.html Filevich et al., (2020) 1 FCON 1000 140 (73F) Healthy 1-2 1 16 http://fcon_1000.projects.nitrc.org/2 FCON 1000 208 (104F) Healthy 1-3 -5 Choe et al.",0,0,1
10.1016/j.neuroimage.2022.119589,openneuro.org:443/datasets/ds002766,"(2020) 1 -4 (0F) Healthy 12-24 -174-348 -2 MSC 10 (5F) Healthy 10 -30 https://openneuro.org/datasets/ds000224/versions/00001. 3 CAST 3 (1F) Healthy 42-64 -30 https://openneuro.org:443/datasets/ds002766 4 MyConnectome 1 (0F) Healthy 104 6 10 http://openfmri.org/dataset/ds000031 Sylvester et al., (2020) 1 CAST 3 (1F) Healthy 42-64 -30 https://openneuro.org:443/datasets/ds002766 Ousdal et al.",0,0,1
10.1016/j.neuroimage.2022.119589,openneuro.org/datasets/ds002685/versions/1.3.1,"(2018) 1 IBC 12 (2F) Healthy 9-30 12 -https://openneuro.org/datasets/ds002685/versions/1.3.1 Pinho et al., (2021) 1 IBC 13 (2F) Healthy 9-30 12 -https://openneuro.org/datasets/ds002685/versions/1.3.1 Rosenberg et al.",0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/abide/abide_ii.html,"Citation Dataset Number Dataset Name Sample Population Sessions Tasks RS-fMRI (min./session) Dataset Availability Resting-state fMRI studies Byrge and Kennedy (2019) 1 HCP 835 (NA) Healthy 2 -30 https://www.humanconnectome.org 2 ABIDE 54 (11F) Healthy, Autism Spectrum Disorder 2-3 1 32 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html Chen and Hu (2018) 1 HCP 100 (54F) Healthy 2 -30 https://www.humanconnectome.org Demeter et al.",0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/pro/enki_rs_trt/frontpage.html,(2016) 1 HNU 30 (15F) Healthy 10 -10 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/hnu_1.html 2 eNKI-TRT 20 (4F) Representative Sample 2 -10 http://fcon_1000.projects.nitrc.org/indi/pro/eNKI_RS_TRT/FrontPage.html 3 QTIM 272 (204F) Twins 1 -5 https://imaginggenomics.net.au/projects/qtim/Gordon et al.,0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/corr/html/samples.html,(2019) 1 SLIM 105 (49F) Healthy 3 -8 http://fcon_1000.projects.nitrc.org/2 CoRR 93 (45F) Healthy 2-3 -5 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/samples.html 3 CoRR 79 (58F) Healthy 2 -12 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/samples.html 4 CoRR 26 (0F) Healthy 2 -24 http://fcon_1000.projects.nitrc.org/indi/CoRR/html/samples.html Filevich et al.,0,0,1
10.1016/j.neuroimage.2022.119589,naturalscenesdataset.org/duchesne,(2022) 1 NSD 8 (6F) Unreported 30-40 1 100-180 http://naturalscenesdataset.org/Duchesne et al.,0,0,1
10.1016/j.neuroimage.2022.119589,nitrc.org/projects/kirbyweekly,(2015) 1 Kirby 1 (0F) Healthy 158 -7 http://www.nitrc.org/projects/kirbyweekly 2 Kirby 21 (10F) Healthy 1 -7 http://www.nitrc.org/projects/multimodal Airan et al.,0,0,1
10.1016/j.neuroimage.2022.119589,fcon_1000.projects.nitrc.org/indi/pro/nki.html,(2016) 1 KKI 21 (10F) Healthy 2 -30 http://fcon_1000.projects.nitrc.org/indi/pro/nki.html 2 NKI 23 (6F) Healthy 2 -5 3 NKI 23 (6F) Healthy 2 -10 4 NKI 23 (6F) Healthy 2 -10 Allen et al.,0,0,1
10.1016/j.neuroimage.2022.119589,humanconnectome.org,"Citation Dataset Number Dataset Name Sample Population Sessions Tasks RS-fMRI (min./session) Dataset Availability Resting-state fMRI studies Byrge and Kennedy (2019) 1 HCP 835 (NA) Healthy 2 -30 https://www.humanconnectome.org 2 ABIDE 54 (11F) Healthy, Autism Spectrum Disorder 2-3 1 32 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html Chen and Hu (2018) 1 HCP 100 (54F) Healthy 2 -30 https://www.humanconnectome.org Demeter et al., (2020) 1 -30 (9F) Healthy pediatric 2 -6-12 -2 MSC 20 (5F) Healthy 2 -30 https://openneuro.org/datasets/ds000224/versions/00001. 3 -30 (24F) Healthy pediatric 1 -6-12 -4 HCP 50 (26F) Healthy 1 -15-30 https://www.humanconnectome.org 5 -38 (16F) Healthy pediatric 1 -6-12 -6 HCP 50 (26F) Healthy 1 -15-30 https://www.humanconnectome.org 7 -34 (12F) Healthy pediatric 1 -15-30 -Finn et al., (2015) 1 HCP 126 (86F) Healthy 2 4 30 https://www.humanconnectome.org 2 Yale 45 (17F) Healthy 1 -45 Kashyap et al., (2019) 1 HCP 803 (NA) Healthy 2 -30 https://www.humanconnectome.org Liu et al., (2018) 1 HCP 105 (68F) Healthy 2 -30 https://www.humanconnectome.org Liu et al., (2019) 1 HCP 801 (443F) Healthy 2 -30 https://www.humanconnectome.org 2 HCP 183 (81F) Healthy 2 -30 https://www.humanconnectome.org Miranda-Dominguez et al., (2018) 1 -159 (64F) Healthy 1-3 -5 2 HCP 198 (109F) Healthy 2 -30 https://www.humanconnectome.org Noble et al., (2017) 1 -12 (6F) Healthy 4 -36 -2 HCP 606 (NA) Healthy 2 -30 https://www.humanconnectome.org Smith et al., (2015) 1 HCP 461 (271F) Healthy 2 -30 https://www.humanconnectome.org Wang et al., (2021) 1 HCP 886 (NA) Healthy 2 -30 https://www.humanconnectome.org Wang et al., (2015) 1 -25 (9F) Healthy 5 -12 -2 HCP 100 (54F) Healthy 2 7 30 https://www.humanconnectome.org 3 GSP 104 (56F) Healthy 1 -6 https://dataverse.harvard.edu/dataset.xhtml?persistentId = doi:10., (2020) 1 HCP 502 (274F) Healthy 2 1 30 https://www.humanconnectome.org 2 -157 (105F) Healthy, Amnestic Mild Cognitive Impairment, Alzheimer’s Disease 1 ---Cole et al., (2014) 1 -15 (7F) Healthy 1 64 10 -2 HCP 118 (NA) Healthy 2 7 30 https://www.humanconnectome.org Cole et al., (2016) 1 HCP 100 (54F) Healthy 2 7 30 https://www.humanconnectome.org (continued on next page) 4 K.J., (2019) 1 HCP 75 (NA) Healthy 2 7 30 https://www.humanconnectome.org Finn et al., (2017) 1 HCP 716 (392F) Healthy 2 7 30 https://www.humanconnectome.org Gao et al., (2019) 1 HCP 515 (274F) Healthy 1 7 30 https://www.humanconnectome.org 2 PNC 571 (320F) Healthy 1 2 6 https://www.nitrc.org/projects/pnc Greene et al., (2018) 1 HCP 515 (274F) Healthy 1 7 30 https://www.humanconnectome.org 2 PNC 571 (320F) Healthy 1 2 6 https://www.nitrc.org/projects/pnc Jiang et al., (2020) 1 HCP 463 (269F) Healthy 1 7 15 https://www.humanconnectome.org Salehi et al., (2020) 1 -1 (0F) Healthy 33 6 14 -2 MSC 10 (5F) Healthy 10 3 30 https://openneuro.org/datasets/ds000224/versions/00001. 3 HCP 514 (NA) Healthy 2 7 30 https://www.humanconnectome.org Seitzman et al., (2019) 1 MSC 10 (5F) Healthy 10 2 30 https://openneuro.org/datasets/ds000224/versions/00001. 2 MyConnectome 1 (0F) Healthy 84 -10 http://myconnectome.org/wp/3 HCP 384 (174F) Healthy 2 -30 https://www.humanconnectome.org 4 WashU 120 120 (60F) Healthy 1 -14 https://legacy.openfmri.org/dataset/ds000243/Shah et al., (2016) 1 HCP 476 (280F) Healthy 2 7 30 https://www.humanconnectome.org Tavor et al., (2016) 1 HCP 98 (NA) Healthy 2 7 30 https://www.humanconnectome.org Wu et al., (2020) 1 HCP 922 (NA) Healthy 1 7 -https://www.humanconnectome.org Kraus et al.",0,0,1
10.1016/j.neuroimage.2022.119589,legacy.openfmri.org/dataset/ds000243/(continued,(2015) 1 MyConnectome 1 (0F) Healthy 84 6 10 http://openfmri.org/dataset/ds000031 2 -1 (0F) Healthy 10 -30 -3 WashU 120 120 (60F) Healthy 1 -14 https://legacy.openfmri.org/dataset/ds000243/(continued on next page) 3 K.J.,0,0,1
10.1016/j.neuroimage.2022.119204,sdmproject.com/utilities/?show,"In cases where only z -or p -values were reported, t -values were obtained with an in-house converter (https://www.sdmproject.com/utilities/?show = Statistics).",1,0,0
10.1016/j.neuroimage.2022.119204,brainmap.org/ale,"Activation likelihood estimation A complementary meta-analysis was conducted for the whole dataset using the ALE approach, with the aim of corroborating ﬁnd-ings of the SDM-PSI meta-analysis with an alternative and commonly used algorithm (v3.0.2 GingerALE software; Eickhoﬀ et al., 2009 ; Turkeltaub et al., 2002 , 2012 ; http://brainmap.org/ale/).",1,0,0
10.1016/j.neuroimage.2022.119204,brainmap.org,"Additional search was con-ducted in the BrainMap database (Sleuth 3.0.4; Fox and Lancaster, 2002 ; Laird, Lancaster and Fox, 2005; http://www.brainmap.org/) with pa-rameters as follows: Subject Age = “18 to 65 ″ , Stimulus Modality = “Vi-sual ”, Paradigm Class = “Delayed Match to Sample ”, Behavioural Do-main = “Working Memory ”, Imaging Modality = “fMRI ”.",0,1,0
10.1016/j.neuroimage.2022.119204,sdmproject.com,"Seed-based eﬀect size mapping Coordinate-based meta-analyses were conducted using the latest SDM with permutation of subject images (v6.21 SDM-PSI; Albajes-Eizagirre et al., 2019 ; Radua and Mataix-Cols, 2009 ; Radua et al., 2012 , 2014 ; https://www.sdmproject.com/)., In cases where only z -or p -values were reported, t -values were obtained with an in-house converter (https://www.sdmproject.com/utilities/?show = Statistics).",1,0,0
10.1016/j.neuroimage.2022.119414,fmriprep.readthedocs.io/en/stable/workﬂows.html,"For more details of the pipeline, see https://fmriprep.readthedocs.io/en/stable/workﬂows.html 2 B.",0,0,1
10.1016/j.neuroimage.2022.119414,cran.r-project.org/web/packages/nbr,The code used in the analysis is part of the Network-Based R-Statistics pack-age available at https://cran.r-project.org/web/packages/NBR/.,1,0,0
10.1016/j.neuroimage.2022.119720,icds.psu.edu,Data analyses were conducted using the computing re-sources provided by the Institute for Computational and Data Sciences at the Pennsylvania State University (https://icds.psu.edu).,1,0,0
10.1016/j.neuroimage.2022.119720,ﬁl.ion.ucl.ac.uk/spm,"The binary time se-ries was then convolved with the hemodynamic response function from SPM (https://www.ﬁl.ion.ucl.ac.uk/spm/) to generate a new time se-ries, which was then binarized by using the threshold of 0.",1,0,0
10.1016/j.neuroimage.2022.119720,sleephealthjournal.org,"Buxton dis-closes that he received subcontract grants to Penn State from Proac-tive Life LLC (formerly Mobile Sleep Technologies) doing business as SleepScape (NSF/STTR #1622766, NIH/NIA SBIR R43-AG056250, R44-AG056250), received honoraria/travel support for lectures from Boston University, Boston College, Tufts School of Dental Medicine, New York University, the University of Miami, and Allstate, consulting fees from SleepNumber, and receives an honorarium for his role as the Editor in Chief of Sleep Health (sleephealthjournal.org).",0,0,1
10.1016/j.neuroimage.2022.119299,humanconnectome.org/and,The HCP and MICs dMRI and rs-fMRI data are publicly available at https://www.humanconnectome.org/and https://portal.conp.ca/dataset?id = projects/mica-mics.,0,1,0
10.1016/j.neuroimage.2022.119299,micapipe.readthedocs.io,The preprocessing of the MICs dataset was performed with micapipe (https://micapipe.readthedocs.io).,1,0,0
10.1016/j.neuroimage.2022.119299,github.com/govindasurampudi/mkl,We used code provided by the au-thors for both MKL (https://github.com/govindasurampudi/MKL) and Spectral (https://brainopt.github.io/spectral-mapping).,1,0,0
10.1016/j.neuroimage.2022.119299,portal.conp.ca/dataset?id,The HCP and MICs dMRI and rs-fMRI data are publicly available at https://www.humanconnectome.org/and https://portal.conp.ca/dataset?id = projects/mica-mics.,0,1,0
10.1016/j.neuroimage.2022.119299,humanconnectome.org/software/hcp-mr-pipelines,The code used for preprocessing is available at https://www.humanconnectome.org/software/hcp-mr-pipelines.,1,0,0
10.1016/j.neuroimage.2022.119299,github.com/antsx/ants,"T1w scans were then linearly co-registered and averaged, automatically corrected for intensity nonuniformity using the N4ITK approach (Tustison et al., 2010), which is available in the Advanced Neuroimaging Tools (ANTs) (https://github.com/ANTsX/ANTs), and intensity normalized.",1,0,0
10.1016/j.neuroimage.2022.119299,github.com/mica-mni/micaopen/tree/master/sf_prediction,"The proposed approach is implemented in Python and the code is publicly available at (https://github.com/MICA-MNI/micaopen/tree/master/sf_prediction)., Code and data availability The code for the proposed framework is publicly available at https://github.com/MICA-MNI/micaopen/tree/master/sf_prediction.",1,0,0
10.1016/j.neuroimage.2022.119299,brainopt.github.io/spectral-mapping,We used code provided by the au-thors for both MKL (https://github.com/govindasurampudi/MKL) and Spectral (https://brainopt.github.io/spectral-mapping).,1,0,0
10.1016/j.neuroimage.2022.119054,portal.dementiasplatform.uk/analysedata/analysisenvironment,The max-ﬁltered (and raw) data are available here: https://portal.dementiasplatform.uk/AnalyseData/AnalysisEnvironment.,0,1,0
10.1016/j.neuroimage.2022.119054,github.com/delshadv/mri_meg_combination,"The imputed data are available in the tab-separated value ﬁle “participants-imputed.tsv ”on the GitHub repository (https://github.com/delshadv/MRI_MEG_Combination)., Code availability The custom written codes to implement all validation analy-ses is available on GitHub (https://github.com/delshadv/MRI_MEG_Combination).",0,1,0
10.1016/j.neuroimage.2022.119054,cam-can.org,"The CBU controls were recruited from the CamCAN sample (www.cam-can.org) who are screened to be healthy, i.e., have MMSE (and indeed ACE-R) scores above conventional cut-oﬀs, as well as other screening described in CamCAN paper (Shafto et al., 2014).",0,1,0
10.1016/j.neuroimage.2022.119054,github.com/delshadv,"The imputed data are available in the tab-separated value ﬁle “participants-imputed.tsv ”on the GitHub repository (https://github.com/delshadv/MRI_MEG_Combination)., Code availability The custom written codes to implement all validation analy-ses is available on GitHub (https://github.com/delshadv/MRI_MEG_Combination)., https://github.com/delshadv for the kernel-based approach, as well as for the MRI and MEG pre-processing and feature extraction steps while the raw data are available on request from the Dementia Platform UK (DPUK).",0,1,0
10.1016/j.neuroimage.2022.119054,adni.loni.usc.edu/about,"This dearth of training data may also explain why higher classiﬁcation accuracies have been reported for other neu-roimaging markers (e.g., sMRI) for which larger databases exist, such as ADNI (http://adni.loni.usc.edu/about/).",0,1,0
10.1016/j.neuroimage.2022.119054,ohba-analysis.github.io/osl-docs,"The continuous data were then epoched into 2s segments, and bad epochs were marked using the OSL automatic artefact detection (https://ohba-analysis.github.io/osl-docs/).",1,0,0
10.1016/j.neuroimage.2022.119054,ﬁl.ion.ucl.ac.uk/spm,"The max-ﬁltered data were read into using the SPM12 toolbox (http://www.ﬁl.ion.ucl.ac.uk/spm/; Penny et al., 2006).",1,0,0
10.1016/j.neuroimage.2022.119054,portal.dementiasplatform.uk/apply,The raw data are available on the DPUK website (https://portal.dementiasplatform.uk/Apply) cited in the main paper.,0,1,0
10.1016/j.neuroimage.2022.119504,github.com/shescher/xdlreg,"For the image details, please see the openly available code: https://github.com/SHEscher/XDLreg., The code for the prediction and interpretation pipeline of the sim-ulation study can be found and downloaded at https://github.com/SHEscher/XDLreg.",1,0,0
10.1016/j.neuroimage.2022.119504,uniklinikum-leipzig.de/einrichtungen/life,"Data Availability Both the LIFE MRI data for model training and evaluation, and the LIFE biomarkers are health data, thus, according the GPDR of the EU, the access to the data can only be granted after application at the Leipzig Research centre for Civilization Diseases (LIFE ; https://www.uniklinikum-leipzig.de/einrichtungen/life).",0,1,0
10.1016/j.neuroimage.2022.119641,v.ly,V.LY.,0,0,1
10.1016/j.neuroimage.2022.119641,cran.r-project.org/package,"The linear mixed model analysis was per-formed using R software (https://www.R-project.org/), in conjunction with the nlme package (Version 3.1–153; https://CRAN.R-project.org/package = nlme).",1,0,0
10.1016/j.neuroimage.2022.119641,ibeat.cloud,"For gray matter, the MPF templates were skull stripped, tissue segmented and parcellated (Wang et al., 2018) using an infant-dedicated processing pipeline, iBEAT V2.0 Cloud (http://www.ibeat.cloud).",1,0,0
10.1016/j.neuroimage.2022.119641,r-project.org,"The linear mixed model analysis was per-formed using R software (https://www.R-project.org/), in conjunction with the nlme package (Version 3.1–153; https://CRAN.R-project.org/package = nlme).",1,0,0
10.1016/j.neuroimage.2022.118970,github.com/agriﬀa/gsp_brain_decode_ﬁngerprint.git,The code to implement the analyses performed in this study is available at https://github.com/agriﬀa/GSP_brain_decode_ﬁngerprint.git.,1,0,0
10.1016/j.neuroimage.2022.118970,mrtrix.org,"The same DW-MRI processing pipeline detailed in (Preti and Van De Ville, 2019) was used to reconstruct whole brain tractograms in-cluding 2 million ﬁbers, using a spherical deconvolution approach and the Spherical-deconvolution Informed Filtering of Tractograms 2 (SIFT2 (Smith et al., 2015a), https://www.mrtrix.org/).",1,0,0
10.1016/j.neuroimage.2022.119277,spams-devel.gforge.inria.fr,"The el-ements of x are then estimated using non-negative least squares with Tikhonov regularization (Efron et al., 2004) (regularization parame-ter 𝜆2 = 0.005) using the Lasso function implemented in the SPAMS optimization toolbox (http://spams-devel.gforge.inria.fr).",1,0,0
10.1016/j.neuroimage.2022.119277,github.com/daducci/amico/wiki/fitting-the-sandi-model,"The SANDI ﬁt was performed using its implementation in the accel-erated microstructure imaging via convex optimization (AMICO) frame-work in Python 3.5 (Daducci et al., 2015), publicly available at : https://github.com/daducci/AMICO/wiki/Fitting-the-SANDI-model.",1,0,0
10.1016/j.neuroimage.2022.119277,cai2r.net,"is supported by NIH under NINDS award R01 NS088040 and by the Center of Advanced Imaging Innova-tion and Research (CAI 2 R, www.cai2r.net), a NIBIB Biomedical Tech-nology Resource Center: P41 EB017183.",0,0,1
10.1016/j.neuroimage.2022.119701,github.com/aib8/overeasy-topup,The scripts including the speciﬁc commands used in this study are available under a license at https://github.com/aib8/overeasy-topup.,1,0,0
10.1016/j.neuroimage.2022.119701,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fsl,The data analysis presented in this manuscript used stan-dard tools freely available in the FSL software package (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL).,1,0,0
10.1016/j.neuroimage.2022.119384,psychopy.org,Stimuli of the fMRI Experiment Fig. 1 c shows the experiment stimuli which were programed using PsychoPy 3.0 (http://psychopy.org).,0,0,1
10.1016/j.neuroimage.2022.119384,gitee.com/qunjunliang/subway-navigational-task-analysis-scripts,"Code Accessibility The codes used in each analysis are available freely on Gi-tee (https://gitee.com/qunjunliang/subway-navigational-task-analysis-scripts)., Data and code availability statement All codes used in the present study and the participant-level DCMs are available on Gitee repository (https://gitee.com/qunjunliang/subway-navigational-task-analysis-scripts).",1,0,0
10.1016/j.neuroimage.2022.119384,mriqc.readthedocs.io/en/latest,"Functional Data Preprocessing Because of the relatively high acceleration factor used in our MSM EPI sequence, we performed a quality control (QC) with MRIQC soft-ware (https://mriqc.readthedocs.io/en/latest/) before data preprocess-ing.",1,0,0
10.1016/j.neuroimage.2022.119384,github.com/compneuro-ncu/fmridenoise,"Afterwards, the pre-processed images were denoised with the signals of white matter, cerebrospinal ﬂuid, and framewise displacement through a fMRIdenoise pipeline (https://github.com/compneuro-ncu/fmridenoise).",1,0,0
10.1016/j.neuroimage.2022.119060,osf.io/yn6gb,"(2019) and is available for download at https://osf.io/yn6gb/., (2019) and is available for download at https://osf.io/yn6gb/.",0,0,1
10.1016/j.neuroimage.2022.119060,osf.io/bf6va,The analysis was per-formed using MATLAB codes available at https://osf.io/bf6va/. 7 W.,1,0,0
10.1016/j.neuroimage.2022.119634,github.com/nyu-diﬀusionmri/mppca_denoise,"(Veraart et al., 2016), on the following reposi-tory: https://github.com/NYU-DiﬀusionMRI/mppca_denoise.",1,0,0
10.1016/j.neuroimage.2022.119634,mrshub.org,The Mat-lab code used to generate the simulation data is available on the follow-ing repository: https://github.com/jessie-mosso/DWMRS-MPPCA and linked to the MRSHub (https://mrshub.org).,0,1,0
10.1016/j.neuroimage.2022.119634,github.com/jessie-mosso/dwmrs-mppca,The Mat-lab code used to generate the simulation data is available on the follow-ing repository: https://github.com/jessie-mosso/DWMRS-MPPCA and linked to the MRSHub (https://mrshub.org).,0,1,0
10.1016/j.neuroimage.2022.119273,ﬁl.ion.ucl.ac.uk/spm,"Tissue segmentation was performed using the Statistical Paramet-ric Mapping toolbox for MATLAB (SPM12, www.ﬁl.ion.ucl.ac.uk/spm ; Ashburner and Friston, 2005).",1,0,0
10.1016/j.neuroimage.2022.119273,jasp-stats.org,"Bayesian analy-ses were conducted in JASP (www.jasp-stats.org ; Wagenmakers et al., 2018).",1,0,0
10.1016/j.neuroimage.2022.119273,osf.io/s3kzy/?view_only,"Next, as described in our pre-registration (https://osf.io/s3kzy/?view_only = 519f07b8ada148fd830aeba5c9a4df08), to test whether the relationship between GABA + and Glu could be explained by confounding factors, we systematically regressed out the inﬂuence of variables that could potentially account for observed relationships using a linear mixed eﬀects model.",0,0,1
10.1016/j.neuroimage.2021.118777,jstor.org/stable/3841398,78–83 URL : http://www.jstor.org/stable/3841398 REFERENCES Linked references are available on JSTOR for this a 308.,0,0,1
10.1016/j.neuroimage.2021.118777,osf.io/a2cqh,"Data and code availability The Data needed to evaluate the conclusions in the paper are pre-sent in the paper, Supplementary Materials, and/or the OSF re-pository (https://osf.io/a2cqh/).",0,1,0
10.1016/j.neuroimage.2022.119179,ﬁl.ion.ucl.ac.uk/spm,"MRI preprocessing Image analyses were performed using Statistical Paramet-ric Mapping 12 (SPM 12, Welcome Trust Center, London, UK: www.ﬁl.ion.ucl.ac.uk/spm/), Advanced Normalization Tools (ANTs, (Avants et al., 2011)), and in-house Matlab scripts.",1,0,0
10.1016/j.neuroimage.2022.119179,ncfwp.org,"Study design and participants The data for this study are part of a larger community-based partic-ipatory research project, called PACE5 (Preventing Agricultural Chem-ical Exposure –5) that is being conducted in partnership between the North Carolina Farmworkers Project (Benson, NC; https://ncfwp.org/) and Wake Forest School of Medicine.",0,1,0
10.1016/j.neuroimage.2022.119179,lab-corp.com,"The samples were collected at the baseline visit using ﬁlter paper blood kits provided by Laboratory Corporation of America® Holdings (Lab-corp.com, Test 79,128/CPT 83,655).",0,0,1
10.1016/j.neuroimage.2022.119179,nitrc.org/projects/mricron,The preprocessing included: i) segmenting the T1-wieghted structural images based on standard 6 tissue priors using SPM12; ii) combining gray matter (GM) and white matter (WM) tissue segments to create a brain tissue mask and the mask was thresholded at > 50% and applied to the T1 image to create a masked T1; iii) manual cleaning of the masked T1 in MRIcron software (https://www.nitrc.org/projects/mricron) to correct any misclassiﬁed voxels; iv) normalizing the manually cleaned masked T1 to MNI standard space (www.mni.mcgill.ca) using ANTs.,1,0,0
10.1016/j.neuroimage.2022.119179,mni.mcgill.ca,The preprocessing included: i) segmenting the T1-wieghted structural images based on standard 6 tissue priors using SPM12; ii) combining gray matter (GM) and white matter (WM) tissue segments to create a brain tissue mask and the mask was thresholded at > 50% and applied to the T1 image to create a masked T1; iii) manual cleaning of the masked T1 in MRIcron software (https://www.nitrc.org/projects/mricron) to correct any misclassiﬁed voxels; iv) normalizing the manually cleaned masked T1 to MNI standard space (www.mni.mcgill.ca) using ANTs.,1,0,0
10.1016/j.neuroimage.2022.118975,magandmore.com/en/powermag-lab-100,Note that the TMS device is capable of delivering 100 Hz trains of stimuli at up to 70% stimulator output (https://magandmore.com/en/powermag-lab-100/).,0,0,1
10.1016/j.neuroimage.2022.118975,osf.io/bkn64,Data and code availability statement All EEG data and code for analyzing the data are available on the Open Science Framework website (https://osf.io/bkn64/).,0,1,0
10.1016/j.neuroimage.2022.119219,cai2r.net,"Research was partially performed as part of the Center of Advanced Imaging Innovation and Research (CAI2R, www.cai2r.net), an NIBIB Biomedical Technology Resource Center (NIH P41 EB017183) and was partially supported by the NINDS (R01 NS088040) and NIBIB (R01 EB027075) of the NIH.",0,0,1
10.1016/j.neuroimage.2022.119650,3.2.2.2,3.2.2.2.,0,0,1
10.1016/j.neuroimage.2022.119650,3.2.1.2,3.2.1.2.,0,0,1
10.1016/j.neuroimage.2022.119650,3.2.1.1,3.2.1.1.,0,0,1
10.1016/j.neuroimage.2022.119650,3.2.2.1,3.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119458,who.int/news/item/28-04-2022-draft-intersectoral-global-action-plan-on-epilepsy-and-other-neurological-disorders-2022-2031,https://www.who.int/news/item/28-04-2022-draft-intersectoral-global-action-plan-on-epilepsy-and-other-neurological-disorders-2022-2031 Contribution of the authors PAVS and MLBV share the ﬁrst co-authorship.,0,0,1
10.1016/j.neuroimage.2022.118941,osf.io/cma6p,"Data and code availability statement We make our data (speciﬁcally, the beta values from each ROI for each run, subject and experiment) freely available via the Open Science Framework at https://osf.io/cma6p/.",0,1,0
10.1016/j.neuroimage.2022.118941,github.com/nipy/nibabel/releases,"Dale, Fischl, and Sereno, 1999) for prepro-cessing the data and conducting GLMs, and various open source Python packages-speciﬁcally, Nilearn, Nibabel (https://github.com/nipy/nibabel/releases), and Scikit-Learn-for conducting all support vec-tor machine analyses (Buitinck et al., 2013 ; Abraham et al., 2014).",1,0,0
10.1016/j.neuroimage.2022.119677,osf.io/swun7/2.1,Materials and methods The entire study was preregistered: https://osf.io/swun7/2.1.,0,0,1
10.1016/j.neuroimage.2022.119677,github.com/christianbrodbeck/eelbrain,"The analysis was conducted using ""eelbrain"", an open source Python module for accessible statistical analysis of MEG and EEG data (v0.31.7, https://github.com/christianbrodbeck/eelbrain, DOI 10.5281/zenodo.598150).",1,0,0
10.1016/j.neuroimage.2022.118923,github.com/robince/gcmi,"Mutual information Mutual information (MI) and other information theoretic quantities were calculated with the GCMI toolbox in (https://github.com/robince/gcmi)., The code used in analyzing mutual information is available at a public repository, https://github.com/robince/gcmi.",1,0,0
10.1016/j.neuroimage.2022.118923,rfmri.org,"With short (12–16 min) acquisitions, analysis pipelines can be performed reliably (Birn et al., 2013) using semi-automated toolboxes such as BRANT (https://sphinx-doc-brant.readthedocs.io/), CONN (https://web.conn-toolbox.org/) and DPARSF (rfmri.org/)., RsfMRI data processing Calculation of ALFF, fALFF and ReHo was performed in Mat-lab (Mathworks Inc.) using DPARSF (V5.1, http://rfmri.org/DPARSF) (Yan and Zang, 2010)., The toolboxes used in preprocessing (FSL) and analyzing resting state metrics (DPARSF) are available to the community at a public repository, https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/and http://rfmri.org/DPARSF.",1,0,0
10.1016/j.neuroimage.2022.118923,web.conn-toolbox.org,"With short (12–16 min) acquisitions, analysis pipelines can be performed reliably (Birn et al., 2013) using semi-automated toolboxes such as BRANT (https://sphinx-doc-brant.readthedocs.io/), CONN (https://web.conn-toolbox.org/) and DPARSF (rfmri.org/).",1,0,0
10.1016/j.neuroimage.2022.118923,rfmri.org/dparsf,"RsfMRI data processing Calculation of ALFF, fALFF and ReHo was performed in Mat-lab (Mathworks Inc.) using DPARSF (V5.1, http://rfmri.org/DPARSF) (Yan and Zang, 2010)., The toolboxes used in preprocessing (FSL) and analyzing resting state metrics (DPARSF) are available to the community at a public repository, https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/and http://rfmri.org/DPARSF.",1,0,0
10.1016/j.neuroimage.2022.118923,fsl.fmrib.ox.ac.uk/fsl/fslwiki/and,"The toolboxes used in preprocessing (FSL) and analyzing resting state metrics (DPARSF) are available to the community at a public repository, https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/and http://rfmri.org/DPARSF.",1,0,0
10.1016/j.neuroimage.2022.118923,sphinx-doc-brant.readthedocs.io,"With short (12–16 min) acquisitions, analysis pipelines can be performed reliably (Birn et al., 2013) using semi-automated toolboxes such as BRANT (https://sphinx-doc-brant.readthedocs.io/), CONN (https://web.conn-toolbox.org/) and DPARSF (rfmri.org/).",1,0,0
10.1016/j.neuroimage.2022.119397,osf.io/amsk2/?view_only,"Data availability Raw data is available from the Open Science Framework project ‘Hypoxia alters posterior cingulate cortex metabolism during a memory task: a 1H fMRS study’: https://osf.io/amsk2/?view_only = 6a48f01fafcd496baf6f63c7f122602e Credit authorship contribution statement Matthew Rogan: Project administration, Visualization, Conceptual-ization, Formal analysis, Writing – original draft, Investigation, Writ-ing –r e v i e w & editing.",0,1,0
10.1016/j.neuroimage.2022.119397,jmrui.eu,"The fMRS spectra (see Fig. 1 for example) and static MRS spectrum (see Supplementary Fig. 2 for example) were processed and analysed us-ing the Java-based version of the magnetic resonance user interface (jM-RUI; (Naressi et al., 2001); software version 6.1 (http://www.jmrui.eu)).",1,0,0
10.1016/j.neuroimage.2021.118843,github.com/utooley/tooley_2020_child_functional_comms/tree/master/partitions,"Data availability We provide two freely available partitions (in fsaverage6, fsLR, and MNI volumetric spaces), at https://github.com/utooley/Tooley_2020_child_functional_comms/tree/master/partitions.",1,0,0
10.1016/j.neuroimage.2021.118843,github.com/thomasyeolab/cbig/tree/master/stable_projects,"We conducted all anal-yses in R and MATLAB using custom code, including that available at https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects., Other toolboxes used in this project are available at https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects and https://aaronclauset.github.io/wsbm.",1,0,0
10.1016/j.neuroimage.2021.118843,aaronclauset.github.io/wsbm,"For a given subject’s 𝑛 ×𝑛 functional brain network, we maximize the like-lihood of the weighted stochastic block model using a variational Bayes technique described by (Aicher et al., 2015) and implemented in MAT-LAB code freely available at https://aaronclauset.github.io/wsbm/., Other toolboxes used in this project are available at https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects and https://aaronclauset.github.io/wsbm.",1,0,0
10.1016/j.neuroimage.2021.118843,github.com/thomasyeolab/cbig,"The clustering algorithm was implemented using pub-licly available code from Yeo and colleagues (2011) , using v0.17.0 at https://github.com/ThomasYeoLab/CBIG., We conducted all anal-yses in R and MATLAB using custom code, including that available at https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects., Other toolboxes used in this project are available at https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects and https://aaronclauset.github.io/wsbm.",1,0,0
10.1016/j.neuroimage.2021.118843,abcdstudy.org/principal-investigators.html,A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/principal-investigators.html.,0,0,1
10.1016/j.neuroimage.2021.118843,abcdstudy.org/nih-collaborators,A full list of supporters is available at https://abcdstudy.org/nih-collaborators.,0,0,1
10.1016/j.neuroimage.2021.118843,aaronclauset.github.io/wsbm,"For a given subject’s 𝑛 ×𝑛 functional brain network, we maximize the like-lihood of the weighted stochastic block model using a variational Bayes technique described by (Aicher et al., 2015) and implemented in MAT-LAB code freely available at https://aaronclauset.github.io/wsbm/.",1,0,0
10.1016/j.neuroimage.2021.118843,nda.nih.gov/edit_collection.html?id,"The raw data are available at https://nda.nih.gov/edit_collection.html?id = 2573., The raw data are available at https://nda.nih.gov/edit_collection.html?id = 2573., The raw data are available at https://nda.nih.gov/edit_collection.html?id = 2573.",0,1,0
10.1016/j.neuroimage.2021.118843,antsbrainextraction.sh,"The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workﬂow (from ANTs), us-ing OASIS30ANTs as the target template.",1,0,0
10.1016/j.neuroimage.2021.118843,github.com/pennbbl/xcpengine,Further documentation is available at https://xcpengine.readthedocs.io and https://github.com/PennBBL/xcpEngine.,0,0,1
10.1016/j.neuroimage.2021.118843,abcdstudy.org,"The ABCD dataset (https://abcdstudy.org) is freely available from the NIMH Data Archive (NDA)., Data used in the preparation of this article were ob-tained from the Adolescent Brain Cognitive Development (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA)., A full list of supporters is available at https://abcdstudy.org/nih-collaborators., A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/principal-investigators.html.",0,1,0
10.1016/j.neuroimage.2021.118843,data-archive.nimh.nih.gov/training/modules/study.html,"Instructions on how to create a NDA study are available at https://data-archive.nimh.nih.gov/training/modules/study.html., Instructions on how to create a NDA study are available at https://data-archive.nimh.nih.gov/training/modules/study.html.",0,0,1
10.1016/j.neuroimage.2021.118843,github.com/utooley/tooley_2021_child_functional_comms/tree/master/partitions,"All other analysis code is available at https://github.com/utooley/Tooley_2021_child_functional_comms/tree/master/partitions , along with the two developmental partitions generated in this study.",1,0,0
10.1016/j.neuroimage.2021.118843,xcpengine.readthedocs.io,Further documentation is available at https://xcpengine.readthedocs.io and https://github.com/PennBBL/xcpEngine.,0,0,1
10.1016/j.neuroimage.2022.119526,brainsmash.readthedocs.io/en/latest,These surrogate gradient maps were estimated using BrainSMASH (https://brainsmash.readthedocs.io/en/latest/).,1,0,0
10.1016/j.neuroimage.2022.119526,diedrichsenlab.org/imaging/suit.htm,"Maps were averaged within 273 regions of interest by combining a parcella-tion of 210 cortical regions and 36 subcortical regions from the Brainnetome atlas (Fan et al., 2016) (http://www.brainnetome.org/) and 27 cerebellar regions from the SUIT atlas (Diedrichsen, 2006) (http://www.diedrichsenlab.org/imaging/suit.htm).",0,1,0
10.1016/j.neuroimage.2022.119526,fsl.fmrib.ox.ac.uk/fsl/fslwiki,We used FSL (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/) and AFNI (https://afni.nimh.nih.gov/) for additional fMRI preprocessing.,1,0,0
10.1016/j.neuroimage.2022.119526,ﬁl.ion.ucl.ac.uk/spm/software/spm12,Task condition block regressors were convolved with a hemo-dynamic response function using the ‘spm_get_bf’ function in SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).,1,0,0
10.1016/j.neuroimage.2022.119526,human.brain-map.org,"Data and code availability Original data was obtained from the Human Connectome Project (1U54MH091657, PIs Van Essen and Ugurbil) and the Allen Hu-man Brain Atlas (http://human.brain-map.org/).",0,1,0
10.1016/j.neuroimage.2022.119526,brainnetome.org,"Maps were averaged within 273 regions of interest by combining a parcella-tion of 210 cortical regions and 36 subcortical regions from the Brainnetome atlas (Fan et al., 2016) (http://www.brainnetome.org/) and 27 cerebellar regions from the SUIT atlas (Diedrichsen, 2006) (http://www.diedrichsenlab.org/imaging/suit.htm).",0,1,0
10.1016/j.neuroimage.2022.119526,github.com/rmarkello/abagen,"Genetic spatial correlation We compared each gradient map to Allen Human Brain spatial gene expression patterns using the ‘abagen’ package (https://github.com/rmarkello/abagen) (Arnatkevic ˘i ū t ėet al., 2019 ; Hawrylycz et al., 2012).",1,0,0
10.1016/j.neuroimage.2022.119526,db.humanconnectome.org/data/projects/hcp_1200,"Subjects and data 200 unrelated subjects were selected from the Human Con-nectome Project (HCP) 1200 Subjects Data Release with avail-able resting (task-free) and task fMRI data from a 3T MRI scan-ner (https://db.humanconnectome.org/data/projects/HCP_1200)., Task fMRI analysis Preprocessed task fMRI data for the four tasks from the HCP were analyzed (working memory, motor, language, emotion) (https://db.humanconnectome.org/data/projects/HCP_1200).",0,1,0
10.1016/j.neuroimage.2022.119526,afni.nimh.nih.gov,We used FSL (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/) and AFNI (https://afni.nimh.nih.gov/) for additional fMRI preprocessing.,1,0,0
10.1016/j.neuroimage.2022.119526,github.com/jbrown81/gradients,"All code (latent space derivation, dynamical system modeling, and gene expression corre-lation) and processed data (gradient maps/region weights, gradient timeseries, and region gene expression values) are available at https://github.com/jbrown81/gradients., Data Availability All code and processed data are available at https://github.com/jbrown81/gradients.",0,1,0
10.1016/j.neuroimage.2022.119526,sites.google.com/site/bctnet,Graph the-ory analyses were run using the Brain Connectivity Toolbox (BCT; https://sites.google.com/site/bctnet/).,1,0,0
10.1016/j.neuroimage.2022.119526,humanconnectome.org/study/hcp-young-adult/document/wu-minn-hcp-consortium-open-access-data-use-terms,This study agreed to the Open Access Data Use Terms (https://www.humanconnectome.org/study/hcp-young-adult/document/wu-minn-hcp-consortium-open-access-data-use-terms) and was exempt from the UCSF IRB because investigators could not readily ascertain the identities of the individuals to whom the data belonged.,0,0,1
10.1016/j.neuroimage.2022.119484,github.com/jiaolang771/aicad/tree/main/oap_el_early_prediction,The source code of the proposed model is publicly acces-sible on GitHub: (https://github.com/jiaolang771/aicad/tree/main/OAP_EL_Early_Prediction).,1,0,0
10.1016/j.neuroimage.2022.118953,msi.umn.edu,URL: http://www.msi.umn.edu.,0,0,1
10.1016/j.neuroimage.2022.118953,github.com/opitzlab/tacs_eﬀects_single_neurons,Data and code availability statement NEURON code and data set used in this paper is available at GitHub: https://github.com/OpitzLab/TACS_eﬀects_single_neurons Declaration of Competing Interest The authors declare no competing ﬁnancial interests.,0,1,0
10.1016/j.neuroimage.2022.118904,github.com/teppei-matsui/cap,A code for reproducing essential results is available for download (https://github.com/teppei-matsui/CAP).,1,0,0
10.1016/j.neuroimage.2022.118904,github.com/mariarizzo/energy,(R codes developed by the original authors are also avail-able  https://github.com/mariarizzo/energy ).,1,0,0
10.1016/j.neuroimage.2022.118904,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"To select the vox-els corresponding gray matter, we made a gray matter mask for each individual using the segmentation program implemented in SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/; threshold set at 0.7) and then took a union of the masks.",1,0,0
10.1016/j.neuroimage.2022.118904,humanconnectomeproject.org,Data and code availability statement All data used in the present study publicly available data of resting-state fMRI distributed by the Human Connectome Project (HCP; http://humanconnectomeproject.org/).,0,1,0
10.1016/j.neuroimage.2022.118934,db.humanconnectome.org,"The HCP dataset The HCP dataset is publicly available at https://db.humanconnectome.org., •The HCP dataset is available at https://db.humanconnectome.org.",0,1,0
10.1016/j.neuroimage.2022.118934,github.com/liuwan0208/tractsegwithlabelembedding,"The code of our method will be released at https://github.com/liuwan0208/TractSegWithLabelEmbedding after this work is published., Code availability statement The code of our method will be released at https://github.com/liuwan0208/TractSegWithLabelEmbedding after this work is published.",1,0,0
10.1016/j.neuroimage.2022.118934,github.com/mic-dkfz/tractseg,"Implementation details The proposed method is implemented with PyTorch (Paszke et al., 2019) based on the code of TractSeg (version 2.2) that is available at https://github.com/MIC-DKFZ/TractSeg.",1,0,0
10.1016/j.neuroimage.2022.119081,fsl.fmrib.ox.ac.uk/fsl,GLM analysis of the sensory fMRI data was conducted in FEAT 6.00 (https://fsl.fmrib.ox.ac.uk/fsl/).,1,0,0
10.1016/j.neuroimage.2022.119081,ﬁl.ion.ucl.ac.uk/spm,"Data processing and analysis Each participant’s T1 image was coregistered to the MNI template in SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm), and the T2 image was then coregistered to the T1 image using a rigid-body transformation.",1,0,0
10.1016/j.neuroimage.2022.119081,mrc-cbu.cam.ac.uk/datasets/camcan,"Sample All data used in the preparation of this work were obtained from the Cambridge center Aging and Neuroscience data set (Cam-Can repos-itory, available at http://www.mrc-cbu.cam.ac.uk/datasets/camcan/) see (Shafto et al., 2014 ; Taylor et al., 2017) for full procedural details of the study., Data and code availability statement All data used in the preparation of this work were obtained from the Cambridge center Aging and Neuroscience data set (Cam-Can repository, and are freely available at http://www.mrc-cbu.cam.ac.uk/datasets/camcan/).",0,1,0
10.1016/j.neuroimage.2022.119081,ﬁndlab.stanford.edu/functional_rois.html,"In all cases, ROIs were deﬁned from masking the group mean PBR and NBR maps with the respective mask from the FindLab atlas (https://ﬁndlab.stanford.edu/functional_ROIs.html) (Shirer et al., 2012).",0,1,0
10.1016/j.neuroimage.2022.119664,orcid.org/0000-0003-1935-053x,# The ﬁrst two authors contributed equally to this work. 1 ORCID: https://orcid.org/0000-0002-3781-9293. 2 ORCID: https://orcid.org/0000-0001-5966-7377 3 ORCID: https://orcid.org/0000-0003-1935-053X Behaving in a socially or ecologically sustainable way beyond one’s own generation poses unique and challenging dilemmas for humans (e.g.,0,0,1
10.1016/j.neuroimage.2022.119664,orcid.org/0000-0002-3781-9293,# The ﬁrst two authors contributed equally to this work. 1 ORCID: https://orcid.org/0000-0002-3781-9293. 2 ORCID: https://orcid.org/0000-0001-5966-7377 3 ORCID: https://orcid.org/0000-0003-1935-053X Behaving in a socially or ecologically sustainable way beyond one’s own generation poses unique and challenging dilemmas for humans (e.g.,0,0,1
10.1016/j.neuroimage.2022.119664,orcid.org/0000-0001-5966-7377,# The ﬁrst two authors contributed equally to this work. 1 ORCID: https://orcid.org/0000-0002-3781-9293. 2 ORCID: https://orcid.org/0000-0001-5966-7377 3 ORCID: https://orcid.org/0000-0003-1935-053X Behaving in a socially or ecologically sustainable way beyond one’s own generation poses unique and challenging dilemmas for humans (e.g.,0,0,1
10.1016/j.neuroimage.2022.119664,neuro.uni-jena.de/cat,"Preprocessing of anatomical brain data We used the computational anatomy toolbox (CAT12, version r1742, http://www.neuro.uni-jena.de/cat/, Dahnke et al., 2013) implemented in the statistical parametric mapping software (SPM 12, version v7771, http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) for preprocessing, which consisted of the following steps: We (1) classiﬁed brain tissue into grey matter (GM), white matter (WM), and cerebrospinal ﬂuid (CSF) by using an adaptive maximum a posterior technique, which does not ne-cessitate a priori information on tissue probabilites, and by applying a partial volume segmentation approach, which estimates a simpliﬁed mixed model of a maximum of two tissue types (Tohka et al., 2004).",1,0,0
10.1016/j.neuroimage.2022.119664,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"Preprocessing of anatomical brain data We used the computational anatomy toolbox (CAT12, version r1742, http://www.neuro.uni-jena.de/cat/, Dahnke et al., 2013) implemented in the statistical parametric mapping software (SPM 12, version v7771, http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) for preprocessing, which consisted of the following steps: We (1) classiﬁed brain tissue into grey matter (GM), white matter (WM), and cerebrospinal ﬂuid (CSF) by using an adaptive maximum a posterior technique, which does not ne-cessitate a priori information on tissue probabilites, and by applying a partial volume segmentation approach, which estimates a simpliﬁed mixed model of a maximum of two tissue types (Tohka et al., 2004).",1,0,0
10.1016/j.neuroimage.2022.119514,human.brain-map.org,"Last, we in-vestigate the molecular functions and cell types that mediated the re-lationship between cortical free water and CTH by using the gene ex-pression data from AHBA (Allen human brain atlas, http://human.brain-map.org)., Estimation of regional gene expressions and cell types All of the regional genetic expression data were came from the AHBA dataset (http://human.brain-map.org)., The gene expres-sion data in this article are available via the Allen human brain atlas (http://human.brain-map.org).",0,1,0
10.1016/j.neuroimage.2022.119514,github.com/frantisekvasa/rotate_parcellation,"We used spin test to correct potential confounding eﬀects of spatial autocorrelation (https://github.com/frantisekvasa/rotate_parcellation) (Váš a et al., 2018).",1,0,0
10.1016/j.neuroimage.2022.119514,humanconnectome.org,"First, we in-vestigate the distribution of free water across the whole cerebral cortex by using dMRI data from HCP (Human connectome project, https://www.humanconnectome.org)., Data and code availability statement The dMRI data in this article are available via the Human connec-tome project (https://www.humanconnectome.org).",0,1,0
10.1016/j.neuroimage.2022.119127,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fix/userguide,"Finally, in the testing phase, we adopt a majority voting strategy to determine the ﬁnal decision by combining the outputs of the three diﬀerent models for robust noise IC detection. 1 https://www.fmrib.ox.ac.uk/datasets/FIX-training 2 https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FIX/UserGuide 3 K.-S.",0,0,1
10.1016/j.neuroimage.2022.119127,fmrib.ox.ac.uk/datasets/fix-training,"Finally, in the testing phase, we adopt a majority voting strategy to determine the ﬁnal decision by combining the outputs of the three diﬀerent models for robust noise IC detection. 1 https://www.fmrib.ox.ac.uk/datasets/FIX-training 2 https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FIX/UserGuide 3 K.-S.",0,0,1
10.1016/j.neuroimage.2022.119127,pytorch.org/tion,"Therefore, to handle this issue during evalua-4 https://pytorch.org/tion, we also measure the F1 score (Whalen et al., 2016) and Cohen’s kappa (Cohen, 1960).",0,0,1
10.1016/j.neuroimage.2022.119127,github.com/keunsooheo/automatic-rsfmri-noise-detection/1,"Both CNN models consist of three parts: 3 https://github.com/KeunsooHeo/Automatic-rsfMRI-noise-detection/1) low-level primary feature learning, 2) high-level deep feature learn-ing/reﬁning, and 3) class-discriminative feature learning/classiﬁcation.",1,0,0
10.1016/j.neuroimage.2022.119669,sccn.ucsd.edu/eeglab,"Conven-tional artifact rejection methods were implemented using algorithms of the EEGLab toolbox (Version 2019.1, http://sccn.ucsd.edu/eeglab/) in MATLAB® (Version 2019a, The Mathworks, Natick, MA).",1,0,0
10.1016/j.neuroimage.2021.118775,github.com/xiangruili/dicm2nii,"DICOM datasets were downloaded as provided by the IDEAS Study Image repository and converted to NifTI with dicm2nii (https://github.com/xiangruili/dicm2nii) (Li et al., 2016).",1,0,0
10.1016/j.neuroimage.2021.118775,ideas-study.org/original-study/data-request,"Data/code availability statement Both IDEAS and ADNI data are available conditional to approval of a data request to be submitted through the respective web-sites, at https://www.ideas-study.org/Original-Study/Data-Request and at http://adni.loni.usc.edu/data-samples/access-data/.",0,1,0
10.1016/j.neuroimage.2021.118775,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2021.118775,afni.nimh.nih.gov,"ac.uk/spm/software/spm12/-Analysis of Functional NeuroImages (AFNI) software suite (publicly available), downloadable at https://afni.nimh.nih.gov/.",1,0,0
10.1016/j.neuroimage.2021.118775,adni.loni.usc.edu,"# Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf https://doi.org/10.1016/j.neuroimage.2021.118775., As a secondary validation dataset, we downloaded all available base-line FBP-and FBB-PETs from the ADNI (adni.loni.usc.edu) database (as of December 2020), with a total of N = 1,518 scans (1,249 FBP and 269 FBB)., Data/code availability statement Both IDEAS and ADNI data are available conditional to approval of a data request to be submitted through the respective web-sites, at https://www.ideas-study.org/Original-Study/Data-Request and at http://adni.loni.usc.edu/data-samples/access-data/.",0,0,1
10.1016/j.neuroimage.2021.118775,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf https://doi.org/10.1016/j.neuroimage.2021.118775.,0,0,1
10.1016/j.neuroimage.2021.118775,ﬁl.ion.ucl.ac.uk/spm/doc/spm12_manual.pdf,"In both approaches, using either nine (tracer-independent approach) or three (tracer-dependent approach) templates, SPM12 will try “to ﬁnd the best linear combination of these images i.e., the templates in order to best model the intensities in the source image ”(s e e SPM12 manual at https://www.ﬁl.ion.ucl.ac.uk/spm/doc/spm12_manual.pdf).",0,0,1
10.1016/j.neuroimage.2021.118775,adni-info.org,"For up-to-date information, see www.adni-info.org., www.adni-info.org for details.",0,0,1
10.1016/j.neuroimage.2021.118775,adni.loni.usc.edu/data-samples/access-data,"Data/code availability statement Both IDEAS and ADNI data are available conditional to approval of a data request to be submitted through the respective web-sites, at https://www.ideas-study.org/Original-Study/Data-Request and at http://adni.loni.usc.edu/data-samples/access-data/.",0,1,0
10.1016/j.neuroimage.2021.118775,ﬁl.ion.ucl.ac.uk/spm/software/spm12/spm12_release_notes.pdf,under academic subscription (see also SPM12 Release Notes at https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/SPM12_Release_Notes.pdf).,0,0,1
10.1016/j.neuroimage.2021.118775,github.com/leoiacca/rpop,"The rPOP pipeline enables the comparison and merging of heterogeneous datasets and is publicly available at https://github.com/leoiacca/rPOP., rPOP requires only an attenuation-corrected amyloid-PET scan acquired following manufacturer guidelines (i.e., appropriate acquisition time and radiotracer dose) and is publicly available at https://github.com/leoiacca/rPOP., All the templates are available at: https://neurovault.org/collections/CPHVNXDQ/(see also Fig. 1 A and Supplementary Figure 1) and on https://github.com/leoiacca/rPOP., Source code for rPOP is available at https://github.com/leoiacca/rPOP., The complete source code and ﬁles to run rPOP are available at https://github.com/leoiacca/rPOP.",1,0,0
10.1016/j.neuroimage.2021.118775,nih.gov/pub/dist/doc/program_help/3dfwhmx.html,nih.gov/pub/dist/doc/program_help/3dFWHMx.html) to estimate the Full-Width at Half-Maximum (FWHM) diﬀerential smoothing kernel to be applied to the individual scan.,1,0,0
10.1016/j.neuroimage.2021.118775,nemotos.net/scripts/acpc_coreg.m,Yamashita and is part of an ac/pc co-registration script (parent function available at: http://www.nemotos.net/scripts/acpc_coreg.m).,1,0,0
10.1016/j.neuroimage.2021.118775,oasis-brains.org,The Neuromorphometrics tissue labels are based on MRI scans from the OASIS project (https://www.oasis-brains.org/) and are provided by Neuromorphometrics Inc.,0,0,1
10.1016/j.neuroimage.2021.118775,neurovault.org/collections/cphvnxdq/(see,All the templates are available at: https://neurovault.org/collections/CPHVNXDQ/(see also Fig. 1 A and Supplementary Figure 1) and on https://github.com/leoiacca/rPOP.,0,0,1
10.1016/j.neuroimage.2021.118775,ideas-study.org/original-study,"ideas-study.org/Original-Study)., Data/code availability statement Both IDEAS and ADNI data are available conditional to approval of a data request to be submitted through the respective web-sites, at https://www.ideas-study.org/Original-Study/Data-Request and at http://adni.loni.usc.edu/data-samples/access-data/.",0,1,0
10.1016/j.neuroimage.2021.118775,gaain.org/centiloid-project,"Here we present and validate a standard approach where, us-ing rPOP-warped and smoothed images, we quantify neocortical SU-VRs using the standard Global Alzheimer’s Association Interactive Network (GAAIN, http://www.gaain.org/centiloid-project) regions-of-interest (ROIs).",1,0,0
10.1016/j.neuroimage.2021.118775,ac.uk/spm/software/spm12/-analysis,"ac.uk/spm/software/spm12/-Analysis of Functional NeuroImages (AFNI) software suite (publicly available), downloadable at https://afni.nimh.nih.gov/.",1,0,0
10.1016/j.neuroimage.2022.119401,db.humanconnectome.org,"For the ﬁrst analysis, unprocessed data were down-loaded from the Human Connectome Project database (https://db.humanconnectome.org) and preprocessed using fM-RIPrep 1.4.0 (Esteban et al., 2018b ; Esteban et al., 2018a ; RRID:SCR_016216), which is based on Nipype 1.2.0 (Gorgolewski et al., 2011 ; Gorgolewski et al., 2018 ; RRID:SCR_002502).",1,0,0
10.1016/j.neuroimage.2022.119618,nihtoolbox.org,"Moreover, to link module ﬂexibility to cognitive measures, we explored the correlation between module ﬂexibility and behavioral assessments from the NIH Toolbox, a standardized set of tools to assess cognitive function across a broad set of domains (www.nihtoolbox.org), and the Kaufman Brief Intelligence Test, Second Edition(KBIT-2) which assesses overall intelligence in verbal and nonverbal domains (Dumont and Willis, 2008).",0,0,1
10.1016/j.neuroimage.2022.119618,github.com/changcaoyan/dynamic-decomposition-model,The MATLAB code for dynamic decomposition model are available at https://github.com/changcaoyan/dynamic-decomposition-model.,1,0,0
10.1016/j.neuroimage.2022.119618,openfmri.org/dataset/ds000224,"More details regarding the MSC dataset are available at: https://www.openfmri.org/dataset/ds000224/., Data preprocessing The preprocessing pipeline was the same as Gordon and colleagues, which is publicly available at OpenfMRI (https://www.openfmri.org/dataset/ds000224/) (Cho et al., 2021 ; Gordon et al., 2017 ; Gratton et al., 2018).",0,1,0
10.1016/j.neuroimage.2022.119618,github.com/neurosynth/neurosynth,Decoding was performed using a python notebook obtained from the Neurosynth’s Github webpage (https://github.com/neurosynth/neurosynth).,1,0,0
10.1016/j.neuroimage.2022.119683,people.eecs.berkeley.edu/∼chunlei.liu/software.html,"QSM data processing, subcortical segmentation and iron measurement The Susceptibility Tensor Imaging (STI) Suite V3.0 software pack-age (https://people.eecs.berkeley.edu/∼chunlei.liu/software.html) was used to reconstruct QSM using the following procedure (Fig.",1,0,0
10.1016/j.neuroimage.2022.119301,github.com/ncmlab/cognitivetasks,All software to deliver this task is publicly available at: https://github.com/NCMlab/CognitiveTasks.,1,0,0
10.1016/j.neuroimage.2022.119301,osf.io/w9vte,Data availability The data that support the ﬁndings of this study are openly available in the Center for Open Science at https://osf.io/w9vte/.,0,1,0
10.1016/j.neuroimage.2022.119301,nitrc.org/projects/gcva_pca,"This analysis used the Generalized Covariance Analysis toolbox (https://www.nitrc.org/projects/gcva_pca) (Habeck et al., 2005 ; Habeck and Stern, 2007).",1,0,0
10.1016/j.neuroimage.2022.119301,jamovi.org,Re-trieved from https://www.jamovi.org.,0,0,1
10.1016/j.neuroimage.2022.119395,github.com/nichalas,Data availability All Matlab code and data supporting the ﬁndings will be publicly accessible in full through GitHub (https://github.com/Nichalas) upon acceptance.,0,1,0
10.1016/j.neuroimage.2021.118761,nitrc.org/projects/bioimagesuite,The MATLAB syntax used for CPM is freely available online (https://www.nitrc.org/projects/bioimagesuite/).,1,0,0
10.1016/j.neuroimage.2021.118761,neurobs.com,"Stimulus presentation and data acquisition was performed using Presentation®software (Version 18.0, Neurobehavioral Systems, Inc., Berkeley, CA, www.neurobs.com).",1,0,0
10.1016/j.neuroimage.2021.118761,nitrc.org/projects/conn,Preprocessing Data were preprocessed using the standard pipeline of the CONN toolbox (https://www.nitrc.org/projects/conn) in MATLAB.,1,0,0
10.1016/j.neuroimage.2021.118761,nitrc.org/projects/artifact_detect,"In addition, preprocessing steps included temporal band-pass ﬁltering (0.008–0.09 Hz), linear detrending, and regression of outlying functional volumes (> 97th percentile in normative sam-ple; global-signal z -value threshold = 5, subject-motion mm thresh-old = 0.09) identiﬁed using the artifact removal toolbox (ART) (https://www.nitrc.org/projects/artifact_detect/).",1,0,0
10.1016/j.neuroimage.2021.118837,scikit-learn.org/stable,"This study used openly available software and codes, speciﬁ-cally the BrainVISA software (https://brainvisa.info), the python point cloud library (https://pointclouds.org/), the python scikit-learn library (https://scikit-learn.org/stable/) and the python optimal transport li-brary (https://pythonot.github.io/).",1,0,0
10.1016/j.neuroimage.2021.118837,brainvisa.info,"By adapting the BabySeg and Morpholo-gist anatomical pipelines of the BrainVISA software (BrainVISA suite, https://brainvisa.info), these segmentations allowed a reconstruction of the inner cortical surfaces of both hemispheres, and the extraction of ob-jects depicting the sulci., This study used openly available software and codes, speciﬁ-cally the BrainVISA software (https://brainvisa.info), the python point cloud library (https://pointclouds.org/), the python scikit-learn library (https://scikit-learn.org/stable/) and the python optimal transport li-brary (https://pythonot.github.io/).",1,0,0
10.1016/j.neuroimage.2021.118837,pointclouds.org,"This study used openly available software and codes, speciﬁ-cally the BrainVISA software (https://brainvisa.info), the python point cloud library (https://pointclouds.org/), the python scikit-learn library (https://scikit-learn.org/stable/) and the python optimal transport li-brary (https://pythonot.github.io/).",1,0,0
10.1016/j.neuroimage.2021.118837,pythonot.github.io,"This study used openly available software and codes, speciﬁ-cally the BrainVISA software (https://brainvisa.info), the python point cloud library (https://pointclouds.org/), the python scikit-learn library (https://scikit-learn.org/stable/) and the python optimal transport li-brary (https://pythonot.github.io/).",1,0,0
10.1016/j.neuroimage.2022.119599,marsbar.sourceforge.net,"Speciﬁ-cally, using the MarsBaR (http://marsbar.sourceforge.net), we extracted beta values from the ﬁrst-level contrast images of each participant and stimulation condition, in predeﬁned 6-mm spherical ROIs centered at their MNI coordinates.",1,0,0
10.1016/j.neuroimage.2022.119599,fsl.fmrib.ox.ac.uk,"The functional and structural data were preprocessed using SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/) and the FMRIB Software Library (FSL 6.0, https://fsl.fmrib.ox.ac.uk/).",1,0,0
10.1016/j.neuroimage.2022.119599,ﬁl.ion.ucl.ac.uk/spm,"The functional and structural data were preprocessed using SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/) and the FMRIB Software Library (FSL 6.0, https://fsl.fmrib.ox.ac.uk/).",1,0,0
10.1016/j.neuroimage.2022.119599,github.com/wutlou/empathyisps,Code for data analyses is available at https://github.com/wutlou/EmpathyISPS.,1,0,0
10.1016/j.neuroimage.2022.119599,osf.io/67wpg,Data availability Data that support the ﬁndings of this study is available at https://osf.io/67wpg/.,0,1,0
10.1016/j.neuroimage.2022.119120,fmri.wfubmc.edu/software/pickatlas,"Masks were created for each individual ROI using the integrated AAL atlas (Tzourio-Mazoyer et al., 2002) of the Wake Forest University Pick Atlas toolbox (http://fmri.wfubmc.edu/software/PickAtlas).The ROIs included in this study consisted of the bilateral OFC, insula, hip-pocampus, parahippocampus and amygdala.",1,0,0
10.1016/j.neuroimage.2022.119120,0.10.2.0,Statistical analyses All statistical analyses of the behavioural data were done in JASP 0.10.2.0 (https://jasp-stats.org/) and GraphPad Prism 9 (https://www.graphpad.com/scientiﬁc-software/prism/).,1,0,0
10.1016/j.neuroimage.2022.119120,mr-confon.de,"Sounds were delivered through over ear noise cancelling headphones (Sony MDR-ZX110NA) during the experimental tasks in the sleep lab, through PC speakers (Dell A425) during sleep, and through an MR compatible noise-cancelling headphone unit developed by MR Confon (http://www.mr-confon.de/) inside the MR scanner.",0,0,1
10.1016/j.neuroimage.2022.119120,tinyurl.com/pereiraneuroimage,"Data availability statement The raw behavioural data and the fMRI data used for the statistical analysis in MRM are available at: https://tinyurl.com/PereiraNeuroimage (password: @PEREIRA2021)., Data availability statement The raw behavioural data and the fMRI data used for the statistical analysis in MRM are available at: https://tinyurl.com/PereiraNeuroimage (password: @PEREIRA2021).",0,1,0
10.1016/j.neuroimage.2022.119120,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"Functional volumes were pre-processed and analysed using SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/; Wellcome De-partment of Imaging Neuroscience, London, UK).",1,0,0
10.1016/j.neuroimage.2022.119120,jasp-stats.org,Statistical analyses All statistical analyses of the behavioural data were done in JASP 0.10.2.0 (https://jasp-stats.org/) and GraphPad Prism 9 (https://www.graphpad.com/scientiﬁc-software/prism/).,1,0,0
10.1016/j.neuroimage.2022.119120,nitrc.org/projects/artefact_detect,"The artefact Detection Toolbox (ART, http://www.nitrc.org/projects/artefact_detect/) was used to estimate per-volume movement outliers using > 3 standard deviations from the mean signal intensity and volume-to-volume movement of 1.5 mm.",1,0,0
10.1016/j.neuroimage.2022.119120,graphpad.com/scientiﬁc-software/prism,Statistical analyses All statistical analyses of the behavioural data were done in JASP 0.10.2.0 (https://jasp-stats.org/) and GraphPad Prism 9 (https://www.graphpad.com/scientiﬁc-software/prism/).,1,0,0
10.1016/j.neuroimage.2022.119046,abcdstudy.org/consortium_members,A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.,0,0,1
10.1016/j.neuroimage.2022.119046,nda.nih.gov,"Participants The individuals and data used for our study come from the ABCD Study’s “Curated Annual Release 4.0 ″ (https://nda.nih.gov ; DOI 10.15154/1,523,041).",0,1,0
10.1016/j.neuroimage.2022.119046,abcdstudy.org/federal-partners.html,A full list of supporters is available at https://abcdstudy.org/federal-partners.html.,0,0,1
10.1016/j.neuroimage.2022.119046,abcdstudy.org,"Acknowledgements Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development SM (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA)., A full list of supporters is available at https://abcdstudy.org/federal-partners.html., A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.",0,1,0
10.1016/j.neuroimage.2022.119046,balsa.wustl.edu/study/7,"(https://balsa.wustl.edu/study/7qMqX)., Data is available on BALSA at https://balsa.wustl.edu/study/7qMqX., Complete data (and ﬁgures) for reliability and stability per re-gion and for each contrast by data cleaning and reliability/stability type can be found on BALSA at https://balsa.wustl.edu/study/7 qMqX., Complete data for regional re-liability and stability by movement quartile are available on BALSA at https://balsa.wustl.edu/study/7qMqX. 8 J.T.",0,1,0
10.1016/j.neuroimage.2022.119046,balsa.wustl.edu/study/7qmqx,"(https://balsa.wustl.edu/study/7qMqX)., Data is available on BALSA at https://balsa.wustl.edu/study/7qMqX., Complete data for regional re-liability and stability by movement quartile are available on BALSA at https://balsa.wustl.edu/study/7qMqX. 8 J.T.",0,1,0
10.1016/j.neuroimage.2022.119557,uk.mathworks.com,All binning analyses and visualizations (including those on simulated data) were implemented in Matlab 2016b (9.1; https://uk.mathworks.com/) using custom code (Data and code availability).,1,0,0
10.1016/j.neuroimage.2022.119557,wiki.humanconnectome.org/display/wbpublic/workbench+,"For each half, 6 estimates were obtained for each grayordinate (vertex; https://wiki.humanconnectome.org/display/WBPublic/Workbench+ Glossary), that is, pRF polar angle, pRF eccentricity, pRF size, pRF gain, percentage of 𝑅 2 , and mean signal intensity.",0,0,1
10.1016/j.neuroimage.2021.118739,dmri.slicer.org,"All brain visualizations were created in 3D Slicer, a platform for medical im-age informatics (www.slicer.org) via SlicerDMRI, a module to map the white matter connections (http://dmri.slicer.org) (Norton et al., 2017 ; Zhang et al., 2020).",1,0,0
10.1016/j.neuroimage.2021.118739,humanconnectome.org/study/hcp-young-adult/data-use-terms,This data is publicly available to researchers who agree to the data use terms (www.humanconnectome.org/study/hcp-young-adult/data-use-terms).,0,1,0
10.1016/j.neuroimage.2021.118739,slicer.org,"All brain visualizations were created in 3D Slicer, a platform for medical im-age informatics (www.slicer.org) via SlicerDMRI, a module to map the white matter connections (http://dmri.slicer.org) (Norton et al., 2017 ; Zhang et al., 2020).",1,0,0
10.1016/j.neuroimage.2021.118739,github.com/lzekelma/awmt_lang_tom,Data analysis code is available on (github.com/lzekelma/AWMT_lang_tom).,1,0,0
10.1016/j.neuroimage.2021.118739,github.com/pnlbwh/ukftractography,"Whole brain tractography was computed by applying a two-tensor Unscented Kalman Filter (UKF) method (Malcolm et al., 2010 ; Reddy and Rathi, 2016) to each subject’s preprocessed dMRI data, as im-plemented in the ukftractography package (https://github.com/pnlbwh/ukftractography).",1,0,0
10.1016/j.neuroimage.2021.118739,db.humanconnectome.org,All HCP data may be downloaded through the Connec-tomeDB (db.humanconnectome.org).,0,1,0
10.1016/j.neuroimage.2021.118739,github.com/slicerdmri/whitematteranalysis,Visual and quantitative quality control of the tractography was performed using a quality con-trol tool in the whitematteranalysis (WMA) software package (https://github.com/SlicerDMRI/whitematteranalysis).,1,0,0
10.1016/j.neuroimage.2021.118739,humanconnectome.org,"Data and code availability All data used in this project is from the Human Connec-tome Project (HCP) (www.humanconnectome.org)., This data is publicly available to researchers who agree to the data use terms (www.humanconnectome.org/study/hcp-young-adult/data-use-terms).",0,1,0
10.1016/j.neuroimage.2022.119266,nist.mni.mcgill.ca/mni-average-brain-305-mri,We note that the ICBM152_2009c template used here is deﬁned to be in the same space as the other non-linear unbiased average ICBM152 templates (found at http://nist.mni.mcgill.ca/icbm-152-nonlinear-at lases-2009) and the linear average ICBM152 template (found at http://nist.mni.mcgill.ca/icbm-152lin) as well as the older MNI305 template (http://nist.mni.mcgill.ca/mni-average-brain-305-mri).,1,0,0
10.1016/j.neuroimage.2022.119266,ppmi-info.org,"Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets., Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets.",0,1,0
10.1016/j.neuroimage.2022.119266,ppmi-info.org/fundingpartners,"PPMI is sponsored and partially funded by the Michael J Fox Foundation for Parkinson’s Research and fund-ing partners, including AbbVie, Avid Radiopharmaceuticals, Biogen, Bristol-Myers Squibb, Covance, GE Healthcare, Genentech, Glaxo-SmithKline (GSK), Eli Lilly and Company, Lundbeck, Merck, Meso Scale Discovery (MSD), Pﬁzer, Piramal Imaging, Roche, Servier, and UCB (www.ppmi-info.org/fundingpartners).",0,0,1
10.1016/j.neuroimage.2022.119266,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119266,prevent-alzheimer.ca,"• PREVENT-AD: The PREVENT-AD (Pre-symptomatic Evaluation of Novel or Experimental Treatments for Alzheimer’s Dis-ease, http://www.prevent-alzheimer.ca) program (Tremblay-Mercier et al., 2014) follows healthy individuals age 55 or older with a parental history of AD dementia., PREVENT-AD data were obtained from the Pre-symptomatic Eval-uation of Novel or Experimental Treatments for Alzheimer’s Disease (PREVENT-AD, http://www.prevent-alzheimer.ca) program data re-lease 3.0 (2016-11-30).",0,1,0
10.1016/j.neuroimage.2022.119266,adni.loni.usc.edu,"# Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf., Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets., Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets.",0,1,0
10.1016/j.neuroimage.2022.119266,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.,0,0,1
10.1016/j.neuroimage.2022.119266,github.com/vfonov/darq,"Finally, to allow others to take advantage of our proposed automated QC tool, we have made DARQ publicly available at https://github.com/vfonov/DARQ in-cluding the source code and pre-trained neural networks., The source code of the method, implemented in torch and pyTorch (python) and pre-trained neural network is publicly available at https://github.com/vfonov/DARQ., The source code of the method, implemented in torch and pyTorch (python) and pre-trained neural network is publicly available at https://github.com/vfonov/DARQ.",1,0,0
10.1016/j.neuroimage.2022.119266,github.com/bic-mni/mni_autoreg,The source code is available at https://github.com/BIC-MNI/mni_autoreg.,1,0,0
10.1016/j.neuroimage.2022.119266,github.com/bic-mni/ezminc/blob/itk4/scripts/bestlinreg_claude.pl,The source code is available at https://github.com/BIC-MNI/EZminc/blob/ITK4/scripts/bestlinreg_claude.pl.,1,0,0
10.1016/j.neuroimage.2022.119266,nist.mni.mcgill.ca/icbm-152lin,We note that the ICBM152_2009c template used here is deﬁned to be in the same space as the other non-linear unbiased average ICBM152 templates (found at http://nist.mni.mcgill.ca/icbm-152-nonlinear-at lases-2009) and the linear average ICBM152 template (found at http://nist.mni.mcgill.ca/icbm-152lin) as well as the older MNI305 template (http://nist.mni.mcgill.ca/mni-average-brain-305-mri).,1,0,0
10.1016/j.neuroimage.2022.119266,ppmi-info.org/data,Data used in this article were in part obtained from the Parkin-son’s Progression Markers Initiative (PPMI) database (www.ppmi-info.org/data).,0,1,0
10.1016/j.neuroimage.2022.119266,portal.conp.ca/dataset?id,"Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets., Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets.",0,1,0
10.1016/j.neuroimage.2022.119266,douglas.qc.ca,Private sector contributions are facilitated by the Development Oﬃce of the McGill University Faculty of Medicine and by the Douglas Hospital Research center Foundation (http://www.douglas.qc.ca/). 11 V.S.,0,0,1
10.1016/j.neuroimage.2022.119266,github.com/bic-mni/ezminc/blob/itk4/scripts/bestlinreg_s,The source code is available at https://github.com/BIC-MNI/EZminc/blob/ITK4/scripts/bestlinreg_s.,1,0,0
10.1016/j.neuroimage.2022.119266,nist.mni.mcgill.ca/icbm-152-nonlinear-at,We note that the ICBM152_2009c template used here is deﬁned to be in the same space as the other non-linear unbiased average ICBM152 templates (found at http://nist.mni.mcgill.ca/icbm-152-nonlinear-at lases-2009) and the linear average ICBM152 template (found at http://nist.mni.mcgill.ca/icbm-152lin) as well as the older MNI305 template (http://nist.mni.mcgill.ca/mni-average-brain-305-mri).,1,0,0
10.1016/j.neuroimage.2022.119266,adni.loni.usc.edu,"Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets., Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets.",0,1,0
10.1016/j.neuroimage.2022.119266,progressivemsalliance.org,"• IPMSA: To test the generalizability of the model in a completely in-dependent dataset, we used T1-weighted MRI scans of 1200 patients with secondary progressive multiple sclerosis (SPMS), randomly se-lected from the from International Progressive Multiple Sclerosis Alliance (IPMSA) study (https://www.progressivemsalliance.org/), scanned on 20 diﬀerent scanner models (including both 3T and 1.5T) across 195 sites (Dadar et al., 2020).",0,1,0
10.1016/j.neuroimage.2022.119266,humanconnectome.org,"Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets., Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets.",0,1,0
10.1016/j.neuroimage.2022.119266,ppmi-info.org,"Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets., Data and code availability statement The data that support the ﬁndings of this study were obtained from the ADNI (publicly available at http://adni.loni.usc.edu/), PPMI (pub-licly available at https://www.ppmi-info.org/), HCP (publicly available at https://www.humanconnectome.org/), and PREVENT-AD (publicly available at https://portal.conp.ca/dataset?id = projects/preventad-open) datasets., Data used in this article were in part obtained from the Parkin-son’s Progression Markers Initiative (PPMI) database (www.ppmi-info.org/data)., For up-to-date information on the study, visit www.ppmi-info.org., PPMI is sponsored and partially funded by the Michael J Fox Foundation for Parkinson’s Research and fund-ing partners, including AbbVie, Avid Radiopharmaceuticals, Biogen, Bristol-Myers Squibb, Covance, GE Healthcare, Genentech, Glaxo-SmithKline (GSK), Eli Lilly and Company, Lundbeck, Merck, Meso Scale Discovery (MSD), Pﬁzer, Piramal Imaging, Roche, Servier, and UCB (www.ppmi-info.org/fundingpartners).",0,1,0
10.1016/j.neuroimage.2022.119266,bitbucket.org/bicnist/bic-nist-registration/src/5d253993e7b9/elastix,"We used Mattes mutual information, adaptive stochastic gradient descent optimizer with Similarity Transform with 7 parameters https://bitbucket.org/bicnist/bic-nist-registration/src/5d253993e7b9/Elastix.",1,0,0
10.1016/j.neuroimage.2022.118900,github.com/vpnl/floc,"The localizer used 3 runs, similar to the (Stigliani et al., 2015 experiment available here: https://github.com/VPNL/fLoc). 8 subjects participated in a localizer with an oddball task and 2 categories per domain (as in Stigliani et al., 2015) and 24 subjects participated in an experiment with images from the same 5 domains and 1 category per domain, and a 2-back task (as in Bugatus et al., 2017).",1,0,0
10.1016/j.neuroimage.2022.118900,github.com/vistalab/vistasoft,"Motion correction was performed both within and across functional runs using mrVista (https://github.com/vistalab/vistasoft) motion correction algorithms., Residual correlations were computed using Mat-lab R2014a (mathworks.com), mrVista (https://github.com/vistalab/vistasoft), and SPM8 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/).",1,0,0
10.1016/j.neuroimage.2022.118900,ﬁl.ion.ucl.ac.uk/spm/software/spm8,"Residual correlations were computed using Mat-lab R2014a (mathworks.com), mrVista (https://github.com/vistalab/vistasoft), and SPM8 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/).",1,0,0
10.1016/j.neuroimage.2022.118900,github.com/akjags/att_class_resid,"Data/Code Avail-ability Statement: Data and code are available at https://github.com/akjags/att_class_resid., Data/Code Availability Statement Data and code are available at https://github.com/akjags/att_class_resid.",1,0,0
10.1016/j.neuroimage.2022.118900,ﬁl.ion.ucl.ac.uk/spm,"In brief, we ﬁrst ﬁt data from the Oddball task in each voxel in the brain using a GLM by convolving the design matrix of the Odd-ball experiment with the hemodynamic response function (hRF) imple-mented in SPM8 (https://www.ﬁl.ion.ucl.ac.uk/spm/), to estimate re-sponse amplitudes (betas) for each of the ﬁve categories., Residual correlations were computed using Mat-lab R2014a (mathworks.com), mrVista (https://github.com/vistalab/vistasoft), and SPM8 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/).",1,0,0
10.1016/j.neuroimage.2022.118900,mathworks.com,"In order to perform multiclass classiﬁcation with a linear SVM, we employed the Matlab (mathworks.com) functions ﬁtcsvm and ﬁtcecoc to train a binary classiﬁer for each category (1 vs., Residual correlations were computed using Mat-lab R2014a (mathworks.com), mrVista (https://github.com/vistalab/vistasoft), and SPM8 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/)., All statistical analyses were conducted in MATLAB 2014a (mathworks.com) and R (Version 3.5.0) using RStudio (Version 1.1.383).",1,0,0
10.1016/j.neuroimage.2022.118900,surfer.nmr.mgh.harvard.edu,"Finally, whole-brain anatomical images of each sub-ject’s brain were acquired using a T1-weighted SPGR sequence with a resolution of 1 ×1 ×1 mm, FOV = 240 mm, ﬂip angle = 12°This volume anatomy was used to create a cortical surface reconstruction of each sub-ject’s brain using FreeSurfer 6.0 (https://surfer.nmr.mgh.harvard.edu/)., We used the ROIs deﬁned in the FreeSurfer 6.0 (https://surfer.nmr.mgh.harvard.edu/) average brain (la-bels in FreeSurfer) and using cortex-based alignment in FreeSurfer we transformed these labels into each participant’s native cortical surface.",1,0,0
10.1016/j.neuroimage.2022.119548,2.4.6.2,2.4.6.2.,0,0,1
10.1016/j.neuroimage.2022.119548,3.2.4.3,3.2.4.3.,0,0,1
10.1016/j.neuroimage.2022.119548,3.2.4.2,3.2.4.2.,0,0,1
10.1016/j.neuroimage.2022.119548,3.2.4.1,Additional analyses during the heartbeat detection task 3.2.4.1.,0,0,1
10.1016/j.neuroimage.2022.119548,github.com/azaccaro90/hep_respiration.git,"Data and code availability The code used to analyze the experiment is available in an open repository at the following link: https://github.com/azaccaro90/HEP_respiration.git., Data and code availability statement The code used to analyze the experiment is available in an open repository at the following link: https://github.com/azaccaro90/HEP_respiration.git.",1,0,0
10.1016/j.neuroimage.2022.119548,2.4.6.1,Additional analyses during the heartbeat detection task 2.4.6.1.,0,0,1
10.1016/j.neuroimage.2022.119410,github.com/sunhongfu/deepmri/tree/master/iqsm,"The networks were implemented using Pytorch 1.8; the trained networks and the source codes are available at https://github.com/sunhongfu/deepMRI/tree/master/iQSM., Source codes and trained networks are available at: https://github.com/sunhongfu/deepMRI/tree/master/iQSM.",1,0,0
10.1016/j.neuroimage.2022.119495,6.8.0.1,"Brieﬂy, we obtained vertical and hori-zontal images from both eyes using the Heidelberg Spectralis OCT (Hei-delberg Engineering, Heidelberg, Germany; software version: 6.8.0.1).",1,0,0
10.1016/j.neuroimage.2022.119495,osf.io/ervrh,"Following a pre-registered protocol (https://osf.io/ervrh/), scanning was stopped, and the acquisition repeated, when the subject was observed to lose ﬁx-ation or become sleepy.",0,0,1
10.1016/j.neuroimage.2022.119495,github.com/gkaguirrelab/retinatomeanalysis/tree/master/code/ﬁxelanalysis,Statistical analysis and plotting functions: https://github.com/gkaguirrelab/retinaTOMEAnalysis/tree/master/code/ﬁxelAnalysis.,1,0,0
10.1016/j.neuroimage.2022.119495,github.com/gkaguirrelab/retinatomeanalysis/tree/master/data/ﬁxelresults,"Fixel, RGC, and biometric measurements: https://github.com/gkaguirrelab/retinaTOMEAnalysis/tree/master/data/ﬁxelResults.",0,0,1
10.1016/j.neuroimage.2022.119495,github.com/gkaguirrelab/tomespatialstimuli,The software used to produce the stimuli is pub-licly available (https://github.com/gkaguirrelab/tomeSpatialStimuli).,1,0,0
10.1016/j.neuroimage.2022.119495,github.com/gkaguirrelab/retinatomeanalysis,"The code used for this, and all other analyses reported here is pub-licly available (https://github.com/gkaguirrelab/retinaTOMEAnalysis)., Fixel, RGC, and biometric measurements: https://github.com/gkaguirrelab/retinaTOMEAnalysis/tree/master/data/ﬁxelResults., Statistical analysis and plotting functions: https://github.com/gkaguirrelab/retinaTOMEAnalysis/tree/master/code/ﬁxelAnalysis.",1,0,0
10.1016/j.neuroimage.2022.119495,neuro.uni-jena.de/cat,From their T1 image a measure of their intra-cranial volume (ICV) was derived with CAT12 toolbox (http://www.neuro.uni-jena.de/cat/) for SPM (http://www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119495,ﬁl.ion.ucl.ac.uk/spm,From their T1 image a measure of their intra-cranial volume (ICV) was derived with CAT12 toolbox (http://www.neuro.uni-jena.de/cat/) for SPM (http://www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119495,github.com/gkaguirrelab/ﬁxeltomeanalysis,Scripts for the ﬁxel based analysis: https://github.com/gkaguirrelab/ﬁxelTOMEAnalysis.,1,0,0
10.1016/j.neuroimage.2022.119495,github.com/gkaguirrelab/forwardmodel,"Non-linear model ﬁtting was performed using custom soft-ware (https://github.com/gkaguirrelab/forwardModel), derived in part from the analyzePRF toolbox (Kay et al., 2013).",1,0,0
10.1016/j.neuroimage.2022.119495,3tissue.github.io,"Based on these three tissues, a single-shell, 3-tissue constrained spherical deconvolution method was utilized to calculate FOD maps (Dhollander and Connelly, 2016), using the MRtrix3Tissue package (https://3Tissue.github.io) which is an extension of MRtrix3 (Tournier et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119495,humanconnectome.org,Retinal and brain imaging available through the Human Connectome Project: https://www.humanconnectome.org.,0,0,1
10.1016/j.neuroimage.2022.119495,osf.io/ervrh,"Subjects 42 normally sighted subjects from the University of Pennsylvania and surrounding Philadelphia community were studied under a pre-registered project (https://osf.io/ervrh)., Following a pre-registered protocol (https://osf.io/ervrh/), scanning was stopped, and the acquisition repeated, when the subject was observed to lose ﬁx-ation or become sleepy.",1,0,0
10.1016/j.neuroimage.2022.119635,surfer.nmr.mgh.harvard.edu,"FMRI data were analyzed using FreeSurfer (surfer.nmr.mgh.harvard.edu), FsFast (Dale et al., 1999) and in-house MATLAB codes.",1,0,0
10.1016/j.neuroimage.2022.119635,osf.io/tsz47,Data availability Data and related information supporting this study are available at https://osf.io/tsz47/.,0,1,0
10.1016/j.neuroimage.2021.118809,github.com/mikexcohen/gedtutorial,"MATLAB and Python code that accompanies this tutorial are avail-able at github.com/mikexcohen/GEDtutorial (no additional toolboxes are required), and includes two simulations.",1,0,0
10.1016/j.neuroimage.2022.119725,itk-snap.org,"Registration to the structural image was performed in two steps within ITK-SNAP Version 3.6.0 (www.itk-snap.org) (Yushkevich et al., 2006).",1,0,0
10.1016/j.neuroimage.2022.119725,qmrlab.org,"To this end we used the freely available MATLAB software package qMRLab (http://qmrlab.org/) (Karakuzu et al., 2020) which incorporates the Bloch-McConnell equations (Eq. 1 -4 in the supplemen-tary material).",1,0,0
10.1016/j.neuroimage.2022.119725,github.com/timvanmourik/openfmrianalysis,The layer analysis code is available upon request under the GNU-GPL3.0 license but authors point out that the successor of these tools is available online  https://github.com/TimVanMourik/OpenFmriAnalysis .,1,0,0
10.1016/j.neuroimage.2022.119725,github.com/srikash/mp2rage-utils,Structural Scans The MP2RAGE was denoised using freely available code (https://github.com/srikash/MP2RAGE-utils) and corrected for residual receive bias ﬁelds before being segmented using spm_preproc to create a brain mask.,1,0,0
10.1016/j.neuroimage.2022.119725,qmrilab.org,Data Availability Statement Simulations were performed using the publicly available software package:  http://qmrilab.org/.,1,0,0
10.1016/j.neuroimage.2022.119588,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,"https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release Code Accessibility The shell and python scripts used in the analyses are made available here: https://github.com/HAM-lab-Otago-University/HCP Supplementary materials Supplementary material associated with this article can be found, in the online version, at doi: 10.1016/j.neuroimage.2022.119588.",1,0,0
10.1016/j.neuroimage.2022.119588,neuroimaging-core-docs.readthedocs.io/en/latest/pages/atlases.html,"We also provided MNI coordinates for each re-gion, obtained by transforming voxel coordinates (based on https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/atlases.html) to the MNI space via nilearn.image.coord_transform() using the standard FSL template, MNI152_T1_1 mm, as a reference.",1,0,0
10.1016/j.neuroimage.2022.119588,tmslab.org/netconlab-ﬂuid.php,"Here we downloaded the Activation Likelihood Estimate (ALE) map of signiﬁcant foci that showed associations with various cognitive abilities (Gf_net.nii from http://www.tmslab.org/netconlab-ﬂuid.php) in MNI, volumetric space.",0,0,1
10.1016/j.neuroimage.2022.119588,pingouin-stats.org,"ICC (3,1) is deﬁned as 𝑀 𝑆 𝑝 − 𝑀 𝑆 𝑒 𝑀 𝑆 𝑝 + (𝑘 − 1) 𝑀 𝑆 𝑒 , (6) We computed both types of ICC using the Pingouin pack-age (https://pingouin-stats.org/).",1,0,0
10.1016/j.neuroimage.2022.119588,github.com/ham-lab-otago-university/hcp,"Code accessibility The shell and Python scripts used in the analyses are made available here: https://github.com/HAM-lab-Otago-University/HCP 3., https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release Code Accessibility The shell and python scripts used in the analyses are made available here: https://github.com/HAM-lab-Otago-University/HCP Supplementary materials Supplementary material associated with this article can be found, in the online version, at doi: 10.1016/j.neuroimage.2022.119588.",1,0,0
10.1016/j.neuroimage.2022.119588,github.com/washington-university/hcppipelines,"Brieﬂy, they in-cluded B 0 distortion correction, motion correction, gradient unwrap, boundary-based co-registration to T 1 -weighted image, non-linear reg-istration to MNI152 space, grand-mean intensity normalization and surface generation (see https://github.com/Washington-University/HCPpipelines)., The HCP applied a preprocessing pipeline to resting-state FC that is similar to the pipeline the study applied to tfMRI (Glasser et al., 2013) (see https://github.com/Washington-University/HCPpipelines)., Please see the preprocessing scripts here https://github.com/Washington-University/HCPpipelines.",1,0,0
10.1016/j.neuroimage.2022.119588,nesi.org.nz,URL https://www.nesi.org.nz.,0,0,1
10.1016/j.neuroimage.2022.119374,osf.io/pcvt5,Data availability statement The available data can be found at https://osf.io/pcvt5/.,0,1,0
10.1016/j.neuroimage.2022.119196,marsbar.sourceforge.net,These anatomical ROIs were extracted using the MarsBaR toolbox (http://marsbar.sourceforge.net).,1,0,0
10.1016/j.neuroimage.2022.119196,nitrc.org/projects/bnv,"The Brain-Net Viewer (https://www.nitrc.org/projects/bnv) was used to visualize the results, and the statistical maps were analyzed using the xjView soft-ware, which enabled us to identify signiﬁcantly diﬀerent brain regions.",1,0,0
10.1016/j.neuroimage.2022.119196,rfmri.org/dpabi,"Resting-state functional mri data preprocessing The rs-fMRI standard image data preprocessing was performed us-ing the Data Processing and Analysis of Brain Imaging (DPABI) toolbox, Version 4.1 (http://rfmri.org/dpabi).",1,0,0
10.1016/j.neuroimage.2022.119196,nitrc.org/projects/gretna,"(Bassett and neuroscience, 2017 ; Jin et al., 2021) The GRETNA 2.0.0 toolbox (https://www.nitrc.org/projects/gretna/) was used to perform brain network analysis.",1,0,0
10.1016/j.neuroimage.2022.119247,fcon_1000.projects.nitrc.org/indi/retro/mpi_lemon.html,Data and code availability EEG recordings used in the present work are publicly avail-able in the LEMON database at http://fcon_1000.projects.nitrc.org/indi/retro/MPI_LEMON.html (see also Babayan et al.,0,1,0
10.1016/j.neuroimage.2022.119457,osf.io/wa3qr,"Completely commented Matlab code along with data for one single subject are available on our OSF site at https://osf.io/wa3qr/., Data/code availability The machine learning code for this manuscript is available at https://osf.io/wa3qr/.",1,0,0
10.1016/j.neuroimage.2022.119020,nitrc.org/projects/mcalt,"CT was computed using an in-house methodology in subregions deﬁned by the Mayo Clinic Adult Lifespan Template (MCALT) ADIR122 Atlas (Schwarz et al., 2016) and can also be found on the NITRC website (https://www.nitrc.org/projects/mcalt/).",0,1,0
10.1016/j.neuroimage.2022.119020,github.com/kvsaboo/cogtrajprediction,The code for develop-ing the model is publicly available on GitHub at https://github.com/kvsaboo/CogTrajPrediction.,1,0,0
10.1016/j.neuroimage.2022.119125,cam-can.org,"Participants Data of 649 participants age range 18–88 years; mean = 59.24, standard deviation (SD) = 18.55 were obtained from the second stage of the Cambridge center for Ageing and Neuroscience (Cam-CAN) (http://www.cam-can.org , Shafto et al., 2014).",0,0,1
10.1016/j.neuroimage.2022.119125,nitrc.org/projects/gretna,"Construction of brain functional networks The brain functional network construction was carried out using the graph theoretical network analysis (GRETNA) (http://www.nitrc.org/projects/gretna/, J.",1,0,0
10.1016/j.neuroimage.2022.119125,ﬁl.ion.ucl.ac.uk/spm,"Image preprocessing for R-fMRI was carried out using the Data Processing Assistant for Resting-State fMRI (DPARSF) (http://rfmri.org/DPARSF , Yan and Zang, 2010) toolbox and SPM8 (http://www.ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2022.119125,cam-can.org,Data and Code Availability Statement The Cam-CAN data is publicly available in the database of Cambridge center for Ageing and Neuroscience: https://www.cam-can.org/.,0,1,0
10.1016/j.neuroimage.2022.119125,rfmri.org/dparsf,"Image preprocessing for R-fMRI was carried out using the Data Processing Assistant for Resting-State fMRI (DPARSF) (http://rfmri.org/DPARSF , Yan and Zang, 2010) toolbox and SPM8 (http://www.ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2022.119686,brainextraction.sh,"The T1w-reference was then skull-stripped with a Nipype implementation of the ants BrainExtraction.sh workﬂow, using OASIS30ANTs as the target tem-plate.",1,0,0
10.1016/j.neuroimage.2022.119686,github.com/braindynamicslab/tda-metpro,The code used in this study will be made available upon publication at this address: https://github.com/braindynamicslab/tda-metpro.,1,0,0
10.1016/j.neuroimage.2022.118978,mathworks.com/matlabcentral/ﬁleexchange/42019-hybridvel,"For optical imaging data, as noted in the Methods, the algorithm we developed for assaying blood ﬂow in optical imaging data has been published previously, e.g., see https://www.mathworks.com/matlabcentral/ﬁleexchange/42019-hybridvel.",1,0,0
10.1016/j.neuroimage.2022.118978,layerfmri.com,"Third, LAYNII software carried out ‘layeriﬁcation’ and ‘columniz-ing’ for voxels between the CSF-GM and GM-WM boundaries, in which the delineated boundaries also divide the cortex into multiple image layers with ‘growing layer’ and equal-distance approach (see details in (Huber et al., 2021) and https://layerfmri.com/).",1,0,0
10.1016/j.neuroimage.2021.118849,github.com/sabunculab/brainsurfcnn,"To facilitate future studies, the source code for our models and analysis are publicly available at https://github.com/sabunculab/brainsurfcnn., Data and code availability statement All code that is implemented and used for the present article is made available at https://github.com/sabunculab/brainsurfcnn and will be maintained and supported through this website.",1,0,0
10.1016/j.neuroimage.2022.119465,rfmri.org/dpabi,"Resting state fMRI data processing Resting state fMRI data preprocessing was performed in Sta-tistical Parametric Mapping 12 (SPM12)-based DPABI software (http://rfmri.org/dpabi) running in MATLAB R2017a (Brett et al., 2002 ; Yan et al., 2016).",1,0,0
10.1016/j.neuroimage.2022.119363,github.com/honglabthu/mi-bci,"The an-imated illustration is available at https://github.com/HongLabTHU/MI-BCI., Data and code availability statement The code and data are available at https://github.com/HongLabTHU/MI-BCI.",0,1,0
10.1016/j.neuroimage.2022.119443,osf.io/bndjg,"Data and code availability statement EEG datasets used to create the ﬁgure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/and osf.io/bndjg/.",0,1,0
10.1016/j.neuroimage.2022.119443,osf.io/guwnm,"Code used to reproduce the plots in Fig. 1 , as well as averaged ERP data, is available from osf.io/guwnm/.",1,0,0
10.1016/j.neuroimage.2022.119443,osf.io/eucqf,"Data and code availability statement EEG datasets used to create the ﬁgure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/and osf.io/bndjg/.",0,1,0
10.1016/j.neuroimage.2022.119443,osf.io/gazx2,"Data and code availability statement EEG datasets used to create the ﬁgure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/and osf.io/bndjg/.",0,1,0
10.1016/j.neuroimage.2022.119443,osf.io/thsqg/and,"Data and code availability statement EEG datasets used to create the ﬁgure in this commentary are freely available at osf.io/gazx2/, osf.io/eucqf/, osf.io/thsqg/and osf.io/bndjg/.",0,1,0
10.1016/j.neuroimage.2022.119390,github.com/pine-lab/happe,"The HAPPILEE pipeline is freely available as part of HAPPE 2.0 software under the terms of the GNU General Public License at: https://github.com/PINE-Lab/HAPPE., HAPPILEE may be ac-cessed at: https://github.com/PINE-Lab/HAPPE., Code and data availability All code used in the HAPPILEE pipeline is freely available under the terms of the GNU General Public License at: https://github.com/PINE-Lab/HAPPE.",1,0,0
10.1016/j.neuroimage.2022.119390,zenodo.org/record/5088346,"artifact and artifact addition approaches and simulated signals are publicly available at: https://zenodo.org/record/5088346 (Lopez et al., 2021). 22 K.L.",0,0,1
10.1016/j.neuroimage.2022.119390,sccn.ucsd.edu/wiki/makoto%27s_preprocessing_pipeline#why_does_ic_rejection_increase_gamma_power.2c_or_why_is_an_ic_not_broadband-independent,"Speciﬁ-cally, HAPPILEE applies the updated version of CleanLine’s multi-taper regression (called CleanLineNoise, implemented in the PREP pipeline; Bigdely-Shamlo et al. 2015 b) which is more eﬀective in addressing line noise than the original CleanLine version present in HAPPE 1.0 (Gabard-Durnam et al., 2018 a) software (purportedly a bug ﬁx in the CleanLine code, see Makoto’s pipeline page for unpublished evidence: https://sccn.ucsd.edu/wiki/Makoto%27s_preprocessing_pipeline#Why_does_IC_rejection_increase_gamma_power.2C_or_why_is_an_IC_not_broadband-independent).",1,0,0
10.1016/j.neuroimage.2022.119390,"zenodo.org/record/5,088,346","The EEG ﬁles contributing to this example dataset may be freely assessed at: https://zenodo.org/record/5,088,346 (Lopez et al., 2021)., All EEG and simulated EEG data used to optimize the HAPPILEE pipeline in this manuscript are freely available as a dataset through Zen-odo as Lopez et al., 2021 : https://zenodo.org/record/5,088,346.",0,1,0
10.1016/j.neuroimage.2022.118876,db.humanconnectome.org,Findings The HCP data that support the ﬁndings of this study are publicly available on the Connectome DB database (https://db.humanconnectome.org).,0,1,0
10.1016/j.neuroimage.2022.118876,fmrib.ox.ac.uk/fsl,Data preprocessing Diﬀusion MRI: The HCP diﬀusion images with 1.25 mm isotropic spatial resolution were preprocessed by the HCP diﬀusion pre-processing pipeline using the FMRIB diﬀusion toolbox (FSL 5.0; http://www.fmrib.ox.ac.uk/fsl).,1,0,0
10.1016/j.neuroimage.2022.118876,atlas.brain-map.org,The ex-vivo dMRI data from this donor was acquired with a 3T scanner at 900-micron resolution (http://atlas.brain-map.org/).,0,1,0
10.1016/j.neuroimage.2022.118876,rotman-baycrest.on.ca/pls,"Correlation analysis between MD-prefrontal functional connectivity and cognitive measures We used PLSC to analyze the relationship between the func-tional connectivity of the MD subregions and both the pre-frontal lobe and the behavioral measures (behavioral PLS correla-tion, http://www.rotman-baycrest.on.ca/pls) (McIntosh et al., 1996 ; McIntosh and Lobaugh, 2004 ; Mi š i ćet al., 2016).",1,0,0
10.1016/j.neuroimage.2022.118876,wiki.humanconnectome.org/display/publicdata/hcp,A detailed description of the measures is available at https://wiki.humanconnectome.org/display/PublicData/HCP + Data + Dictionary + P+ Updated + for + the + 1200 + Subject + Release 2.2.,0,0,1
10.1016/j.neuroimage.2022.119199,civmvoxport.vm.duke.edu,"The diﬀusion MRI met-rics are available from: https://civmvoxport.vm.duke.edu upon request., The FA, DWI, MD, AD, RD, and NQA maps at dMRI datasets are available through https://civmvoxport.vm.duke.edu upon request.",0,1,0
10.1016/j.neuroimage.2022.119199,mrtrix.org,"To validate the ﬁber orientation distribution at each voxel for the brain, the constrained spherical deconvolution (CSD) method provided by MR-Trix3 (https://www.mrtrix.org/) was also performed with a maximum harmonic order of 6.",1,0,0
10.1016/j.neuroimage.2022.119199,connectivity.brain-map.org,The tracer injection and projection density images were downloaded from AMBA (25 μm resolution) (https://connectivity.brain-map.org/) and converted to NIFTI format.,0,0,1
10.1016/j.neuroimage.2022.118974,github.com/tbardouille/papto,"Python code for all subsequent analysis is available at https://github.com/tbardouille/papto_camcan., Detecting transient events The PAPTO technique (https://github.com/tbardouille/papto) is a modiﬁed amplitude thresholding approach of detecting transient events.",1,0,0
10.1016/j.neuroimage.2022.118974,fooof-tools.github.io/fooof/index.html,"Modeling aperiodic neural power spectra Aperiodic activity was modeled via the open source “ﬁtting oscilla-tions & one over f ”(fooof) (Donoghue et al., 2020) algorithm (available at https://fooof-tools.github.io/fooof/index.html) for each of the 2400 anatomical ROI estimated single-trial timecourses.",1,0,0
10.1016/j.neuroimage.2022.118974,github.com/tbardouille/papto_camcan,Python code for all subsequent analysis is available at https://github.com/tbardouille/papto_camcan.,1,0,0
10.1016/j.neuroimage.2022.118974,mrc-cbu.cam.ac.uk/datasets/camcan,Data acquisition Data were obtained from the Cam-CAN repository (available at http://www.mrc-cbu.cam.ac.uk/datasets/camcan/) (M.,0,1,0
10.1016/j.neuroimage.2022.118974,github.com/hs13/betaevents,"(2017) https://github.com/hs13/BetaEvents , herein referred to as the “Shin method ”.",0,0,1
10.1016/j.neuroimage.2022.119214,meduniwien.ac.at/neuroimaging/mrna.html,Maps are publicly available at http://www.meduniwien.ac.at/neuroimaging/mRNA.html.,0,0,1
10.1016/j.neuroimage.2022.119214,ﬁl.ion.ucl.ac.uk,"This is achieved by warping the Colin27 MNI template (Holmes et al., 1998) to the subjects T1 using SPM12 (The Wellcome Centre for Human Neuroimaging, www.ﬁl.ion.ucl.ac.uk) to the individual space of each brain from AHBA separately.",1,0,0
10.1016/j.neuroimage.2022.119189,osf.io/knx63,"The ap-proved Stage 1 manuscript, unchanged from the point of IPA, may be downloaded from https://osf.io/knx63/., The data collected, as well as the experimental task, the MatLab®scripts, the SPSS syntaxes, and the logs of the data processing may be freely downloaded from https://osf.io/knx63/.",0,0,1
10.1016/j.neuroimage.2022.119189,3.2.6.1,3.2.6.1.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.4.3,3.2.4.3.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.6.2,3.2.6.2.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.4.2,3.2.4.2.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.1.2,"3.2.1.2., 3.2.1.2.",0,0,1
10.1016/j.neuroimage.2022.119189,3.2.4.1,3.2.4.1.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.5.3,3.2.5.3.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.1.1,3.2.1.1.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.2.1,3.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.1.3,"3.2.1.3., 3.2.1.3.",0,0,1
10.1016/j.neuroimage.2022.119189,3.2.5.2,3.2.5.2.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.6.3,3.2.6.3.,0,0,1
10.1016/j.neuroimage.2022.119189,3.2.5.1,3.2.5.1.,0,0,1
10.1016/j.neuroimage.2022.119661,osf.io/3s89p,"Data and code availability statement Pre-processed raw data, their derived data from multi-echo ﬁt-ting, and design matrices used for the analyses are available at https://osf.io/3s89p/.",0,1,0
10.1016/j.neuroimage.2022.119661,fmrib.ox.ac.uk/fsl,The pre-processing and analyses steps have been well documented in the Methods section and were mostly exe-cuted using publicly available Statistical Parametric Mapping (SPM) (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and FMRIB Soft-ware Library (FSL) (http://www.fmrib.ox.ac.uk/fsl) utilities.,1,0,0
10.1016/j.neuroimage.2022.119661,ﬁl.ion.ucl.ac.uk/spm/software/spm12,The pre-processing and analyses steps have been well documented in the Methods section and were mostly exe-cuted using publicly available Statistical Parametric Mapping (SPM) (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and FMRIB Soft-ware Library (FSL) (http://www.fmrib.ox.ac.uk/fsl) utilities.,1,0,0
10.1016/j.neuroimage.2021.118790,cs.waikato.ac.nz/ml/weka,"Each patch was manually masked for background and tau inclusion with the help of Fiji’s Trainable Weka Segmentation plugin (www.cs.waikato.ac.nz/ml/weka) (Fig. 3 d-e) (Schindelin et al., 2012).",1,0,0
10.1016/j.neuroimage.2021.118790,gimp.org,"First, a user retrained and reﬁned the initial segmentation using an image editor (Gimp) (https://www.gimp.org).",1,0,0
10.1016/j.neuroimage.2021.118790,keras.io,The network was developed in Keras (https://keras.io) on top of TensorFlow (https://www.tensorﬂow.org).,1,0,0
10.1016/j.neuroimage.2021.118790,github.com/grinberglab/high-res-3d-tau,"Algorithms used in this study can be found in (https://github.com/grinberglab/high-res-3D-tau)., Our scanner software can be downloaded at (https://github.com/grinberglab/high-res-3D-tau).",1,0,0
10.1016/j.neuroimage.2021.118790,wynton.ucsf.edu,"The patch extraction routine was written in Python and completely automated, running on UCSFs’ Wyn-ton cluster (https://wynton.ucsf.edu), exploring computational paral-lelism while extracting patches, i.e., the diﬀerent images are split into patches simultaneously (Fig. 1 f).",1,0,0
10.1016/j.neuroimage.2022.118887,clinicaltrials.gov,The study was registered at clinicaltrials.gov with the identiﬁer NCT02753738.,0,0,1
10.1016/j.neuroimage.2022.118887,ﬁl.ion.ucl.ac.uk,"Structural T1 weighted images were also acquired in each session using the following parameters: TE/TR = 2.95/2300 ms, TI = 900 ms, ﬂip angle = 9°, GRAPPA 2, 240 ×256 mm ﬁeld of view, 176 slices, 1.05 ×1.05 ×1.20 mm 3 , TA = 5:09 min Task and resting-state data were preprocessed using Statistical Para-metric Mapping version 12 build 7771 (SPM12, The Wellcome Centre for Human Neuroimaging, http://www.ﬁl.ion.ucl.ac.uk/), where slice-timing correction was performed to the temporally middle slice, fol-lowed by a two-pass realignment.",1,0,0
10.1016/j.neuroimage.2022.119537,neurosynth.org/with,"For the ROIs, we chose the brain areas that are intersections between the area established as active to faces (a meta-analysis using ‘Neurosynth’ https://neurosynth.org/with the term ‘face’) and the area detected as face-positive in our facial expres-sion detection task.",1,0,0
10.1016/j.neuroimage.2022.119537,pymvpa.org,"All other analyses were per-formed using Matlab (version 2018a, The MathWorks Inc, Natick, MA) and PyMVPA (Hanke et al., 2009) (http://www.pymvpa.org).",1,0,0
10.1016/j.neuroimage.2022.119537,neurosynth.org/analyses/terms/face,"However, because no non-face stimuli were included in the task, we deﬁned the ROIs as the intersection with the regions obtained by a Neurosynth meta-analysis when using the term ‘face’ (https://neurosynth.org/analyses/terms/face/).",0,0,1
10.1016/j.neuroimage.2022.119124,clini-caltrials.gov,This study was registered with Clini-caltrials.gov on 24/09/2015 (NCT02559063).,0,0,1
10.1016/j.neuroimage.2022.119124,github.com/sbci-brain/sbci_pipeline,"The SBCI pipeline (https://github.com/sbci-brain/SBCI_Pipeline) was then used for further preprocessing using Freesurfer: motion correction, sam-pling to the surface (left and right), and surface smoothing with a Gaus-sian kernel with FWHM 5 mm.",1,0,0
10.1016/j.neuroimage.2022.119124,fmrib.ox.ac.uk/fsl,Data analysis Imaging data preprocessing Task fMRI data were analyzed with FEAT FSL Version 6.0.0 (www.fmrib.ox.ac.uk/fsl).,1,0,0
10.1016/j.neuroimage.2022.119124,freesurfer.net,T1 images were parcellated using FreeSurfer 6.0.0 (http://freesurfer.net/).,1,0,0
10.1016/j.neuroimage.2022.119570,adni.loni.usc.edu,"Yeo). 1 Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) and the Australian Imaging Biomarkers and Lifestyle Study of Aging (AIBL) database (www.aibl.csiro.au)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf., These ROIs were generated by ADNI after several preprocessing steps (http://adni.loni.usc.edu/methods/mri-tool/mri-pre-processing/) followed by the FreeSurfer version 4.3 (ADNI1) and 5.1 (ADNI2/GO) recon-all pipeline.",0,0,1
10.1016/j.neuroimage.2022.119570,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.,0,0,1
10.1016/j.neuroimage.2022.119570,macc.sg,The MACC dataset can be obtained via a data-transfer agreement with the MACC (http://www.macc.sg/).,0,1,0
10.1016/j.neuroimage.2022.119570,adni.loni.usc.edu/methods/mri-tool/mri-pre-processing,These ROIs were generated by ADNI after several preprocessing steps (http://adni.loni.usc.edu/methods/mri-tool/mri-pre-processing/) followed by the FreeSurfer version 4.3 (ADNI1) and 5.1 (ADNI2/GO) recon-all pipeline.,1,0,0
10.1016/j.neuroimage.2022.119570,fnih.org,Private sector contri-butions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119570,aibl.csiro.au,Yeo). 1 Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) and the Australian Imaging Biomarkers and Lifestyle Study of Aging (AIBL) database (www.aibl.csiro.au).,0,1,0
10.1016/j.neuroimage.2022.119570,github.com/thomasyeolab/cbig/tree/master/stable_projects/predict_phenotypes/an2022_gcvae,Data and code availability Code for the various harmonization algorithms can be found here (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/predict_phenotypes/An2022_gcVAE).,1,0,0
10.1016/j.neuroimage.2022.119570,nscc.sg,"Our computational work was partially performed on resources of the National Supercomputing center, Singapore (https://www.nscc.sg).",1,0,0
10.1016/j.neuroimage.2022.119570,github.com/jfortin1/combatharmonization,Here we utilized the R implementation of the algorithm (https://github.com/Jfortin1/ComBatHarmonization).,1,0,0
10.1016/j.neuroimage.2022.119570,ida.loni.usc.edu,The ADNI and the AIBL datasets can be accessed via the Image & Data Archive (https://ida.loni.usc.edu/).,0,1,0
10.1016/j.neuroimage.2022.119738,mrtrix.org,"Issue description The study used the MRtrix3 software package ((Tournier et al., 2019); www.mrtrix.org) to perform streamline tractography, transform streamlines to standard space, and compute streamline lengths; version 3.0_RC3 was used.",1,0,0
10.1016/j.neuroimage.2022.119738,db.humanconnectome.org,"Data availability Data are available and described in the manuscript Data available at: https://db.humanconnectome.org MRtrix3 software is available at: https://github.com/MRtrix3/mrtrix3 Results from original paper are available at: https://balsa.wustl.edu/study/1K3l Updated results are available at: https://balsa.wustl.edu/study/gmq9M Credit authorship contribution statement Claude J Bajada: Conceptualization, Methodology, Formal analy-sis, Writing – original draft, Writing –r e v i e w & editing.",0,0,1
10.1016/j.neuroimage.2022.119738,github.com/mrtrix3/mrtrix3,"Data availability Data are available and described in the manuscript Data available at: https://db.humanconnectome.org MRtrix3 software is available at: https://github.com/MRtrix3/mrtrix3 Results from original paper are available at: https://balsa.wustl.edu/study/1K3l Updated results are available at: https://balsa.wustl.edu/study/gmq9M Credit authorship contribution statement Claude J Bajada: Conceptualization, Methodology, Formal analy-sis, Writing – original draft, Writing –r e v i e w & editing.",0,0,1
10.1016/j.neuroimage.2022.119738,community.mrtrix.org,"This could be a software-speciﬁc forum such as community.mrtrix.org , or a more broadly-applicable but nevertheless domain-speciﬁc platform such as neurostars.org and nipy.discourse.io.",0,0,1
10.1016/j.neuroimage.2022.119738,balsa.wustl.edu/study/gmq9m,"Data availability Data are available and described in the manuscript Data available at: https://db.humanconnectome.org MRtrix3 software is available at: https://github.com/MRtrix3/mrtrix3 Results from original paper are available at: https://balsa.wustl.edu/study/1K3l Updated results are available at: https://balsa.wustl.edu/study/gmq9M Credit authorship contribution statement Claude J Bajada: Conceptualization, Methodology, Formal analy-sis, Writing – original draft, Writing –r e v i e w & editing.",0,0,1
10.1016/j.neuroimage.2022.119738,nipy.discourse.io,"This could be a software-speciﬁc forum such as community.mrtrix.org , or a more broadly-applicable but nevertheless domain-speciﬁc platform such as neurostars.org and nipy.discourse.io.",0,0,1
10.1016/j.neuroimage.2022.119738,balsa.wustl.edu/study/1k3l,"Data availability Data are available and described in the manuscript Data available at: https://db.humanconnectome.org MRtrix3 software is available at: https://github.com/MRtrix3/mrtrix3 Results from original paper are available at: https://balsa.wustl.edu/study/1K3l Updated results are available at: https://balsa.wustl.edu/study/gmq9M Credit authorship contribution statement Claude J Bajada: Conceptualization, Methodology, Formal analy-sis, Writing – original draft, Writing –r e v i e w & editing.",0,0,1
10.1016/j.neuroimage.2022.119738,neurostars.org,"This could be a software-speciﬁc forum such as community.mrtrix.org , or a more broadly-applicable but nevertheless domain-speciﬁc platform such as neurostars.org and nipy.discourse.io.",0,0,1
10.1016/j.neuroimage.2021.118819,cloud.tsinghua.edu.cn/d/3d176032a5a545c1b927,"(https://cloud.tsinghua.edu.cn/d/3d176032a5a545c1b927/) Supplementary materials Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.neuroimage.2021.118819. 10 X.",0,0,1
10.1016/j.neuroimage.2022.119409,osf.io/7awdm/upon,Data availability statement The data and analysis code for this paper will be made available at https://osf.io/7awdm/upon publication.,0,1,0
10.1016/j.neuroimage.2022.119409,meica.py,"Data were preprocessed using the meica.py script (v2.5b11) with 2nd order polynomial detrending, default despiking, and no smoothing (Kundu et al., 2013).",1,0,0
10.1016/j.neuroimage.2022.119033,keras.io,SDnDTI denoising implementation The MU-Net of SDnDTI was implemented using the Keras applica-tion programming interface (https://keras.io) with a Tensorﬂow back-end (https://www.tensorﬂow.org).,1,0,0
10.1016/j.neuroimage.2022.119033,dipy.org,"“Patch2Self ”(Fadnavis et al., 2020) is another recently proposed self-supervised learning based method for removing noise from diﬀu-sion MRI data (publicly available as the “patch2self ”function from the Dipy software (Garyfallidis et al., 2014), https://dipy.org), with a diﬀer-ent denoising mechanism compared to SDnDTI.",1,0,0
10.1016/j.neuroimage.2022.119033,ﬁl.ion.ucl.ac.uk/spm,"Image processing For each subject, the diﬀusion data were corrected for spatially vary-ing intensity biases using the averaged b = 0 image volume with the uniﬁed segmentation routine implementation in the Statistical Paramet-ric Mapping software (SPM, https://www.ﬁl.ion.ucl.ac.uk/spm) with a full-width at half-maximum of 60 mm and a sampling distance of 2 mm.",1,0,0
10.1016/j.neuroimage.2022.119033,surfer.nmr.mgh.harvard.edu,"In addition, the volumet-ric segmentation results from the FreeSurfer (Fischl, 2012 ; Dale et al., 1999) (https://surfer.nmr.mgh.harvard.edu) reconstruction on the T 1 -weighted data were also used in this study to derive brain tissue masks for results evaluation. 4 Q.",1,0,0
10.1016/j.neuroimage.2022.119033,github.com/nyu-diﬀusionmri/mppca_denoise,"The MPPCA denoising was performed with 5 ×5 ×5 kernel and “full ”s a m p l i n g using the publicly available MATLAB-based software (https://github.com/NYU-DiﬀusionMRI/mppca_denoise)., The source codes of MPPCA implemented using MATLAB are publicly available (https://github.com/NYU-DiﬀusionMRI/mppca_denoise).",1,0,0
10.1016/j.neuroimage.2022.119033,sites.google.com/site/pierrickcoupe/softwares/denoising-for-medical-imaging/mri-denoising/mri-denoising-software,"The AONLM was performed assuming Ri-cian noise with 3 ×3 ×3 block and 7 ×7 ×7 search volume (Coupé et al., 2008 ; Manjón et al., 2010) using the publicly available MATLAB-based software (https://sites.google.com/site/pierrickcoupe/softwares/denoising-for-medical-imaging/mri-denoising/mri-denoising-software)., The MATLAB-based software of AONLM is publicly available (https://sites.google.com/site/pierrickcoupe/softwares/denoising-for-medical-imaging/mri-denoising/mri-denoising-software).",1,0,0
10.1016/j.neuroimage.2022.119033,github.com/qiyuantian/sdndti,"The codes for SDnDTI are publicly available (https://github.com/qiyuantian/SDnDTI) which can be used to pre-train the CNN of SDnDTI using HCP data for ﬁne-tuning, a recommended approach in practice, or train the CNN for super-vised learning using HCP data for ﬁne-tuning if additional high-quality data is available from a few subjects in some applications, after all the supervised denoising achieves the highest performance (Figs. 5 , 6 , 8 , 9 , Tables 2 , 3).",1,0,0
10.1016/j.neuroimage.2022.119033,humanconnectome.org,"humanconnectome.org) were used for this study., Data availability The diﬀusion and T 1 -weighted MRI data of 20 subjects from the Human Connectome Project WU-Minn-Ox Consortium and are pub-licly available (https://www.humanconnectome.org).",0,1,0
10.1016/j.neuroimage.2022.119033,humanconnectome.org,Data availability The diﬀusion and T 1 -weighted MRI data of 20 subjects from the Human Connectome Project WU-Minn-Ox Consortium and are pub-licly available (https://www.humanconnectome.org).,0,1,0
10.1016/j.neuroimage.2022.119033,fsl.fmrib.ox.ac.uk,"The diﬀusion data were corrected for gradient nonlinearity, eddy current and susceptibility induced distortions and co-registered using the FMRIB Software Library (FSL) software (Smith et al., 2004 ; Jenkinson et al., 2012 ; Andersson et al., 2003 ; Andersson and Sotiropou-los, 2016) (https://fsl.fmrib.ox.ac.uk).",1,0,0
10.1016/j.neuroimage.2021.118778,nitrc.org/projects/niistat,"Lesion analyses In order to assess the overall relationship between our behavioral measures and damage to language-relevant regions, we performed ROI-based univariate and multivariate regression analyses in NiiStat (https://www.nitrc.org/projects/niistat/) using the set of four ROIs we derived from Matchin et al.",1,0,0
10.1016/j.neuroimage.2021.118778,dropbox.com/sh/3w4aeizgypfs7sd/aab-w8yn5qdufebj90wksbqaa?dl,"All of the lesion maps and behavioral data for subjects enrolled in this study are available for download at https://www.dropbox.com/sh/3w4aeizgypfs7sd/AAB-W8Yn5qDUFeBj90WKsBqAa?dl = 0., Data availability We have made all of the data publicly available for down-load at https://www.dropbox.com/sh/3w4aeizgypfs7sd/AAB-W8Yn5qDUFeBj90WKsBqAa?dl = 0. 9 W.",0,1,0
10.1016/j.neuroimage.2022.119492,github.com/pneuvial/sanssouci.python,"The proposed statistical methods are implemented in the sanssouci package: https://github.com/pneuvial/sanssouci.python., This code relies on the sanssouci package available at https://github.com/pneuvial/sanssouci.python.",1,0,0
10.1016/j.neuroimage.2022.119492,neurovault.org/collections/1952,"FMRI data To investigate the potential gain in number of detections yielded by using data-driven templates, we performed experiments on an fMRI dataset, collection 1952 (Varoquaux et al., 2018) of the Neu-rovault database (http://neurovault.org/collections/1952).",0,1,0
10.1016/j.neuroimage.2022.119492,github.com/alexblnn/notip,"The experiments presented in this section can be reproduced using the code at: https://github.com/alexblnn/Notip., The Python code used in this paper is available at https://github.com/alexblnn/Notip., This table can be generated using script https://github.com/alexblnn/Notip/blob/master/scripts/table_2.py.",1,0,0
10.1016/j.neuroimage.2022.119492,github.com/alexblnn/notip/blob/master/scripts/table_2.py,This table can be generated using script https://github.com/alexblnn/Notip/blob/master/scripts/table_2.py.,1,0,0
10.1016/j.neuroimage.2022.119492,github.com/sjdavenport/pyrft,"Synthetic data For some of the experiments described below, we have gener-ated simulated data using the pyrft package: https://github.com/sjdavenport/pyrft.",1,0,0
10.1016/j.neuroimage.2022.119195,github.com/mathesong/simba_materials,"Data and code availability statement The R and STAN code used to apply this method are provided in an open repository (https://github.com/mathesong/SiMBA_Materials), including a sample simulated dataset., The R and STAN code used to apply this method are provided in an open repository (https://github.com/mathesong/SiMBA_Materials).",0,1,0
10.1016/j.neuroimage.2022.119092,fsl.fmrib.ox.ac.uk/fsl/fslwiki/atlases,"In addition to these whole-brain GM and WM VOIs, two smaller, anatomically deﬁned regions were selected from at-lases (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases) included in the FSL framework (Smith et al., 2004).",0,1,0
10.1016/j.neuroimage.2022.119092,nist.gov/programs-projects/quantitative-mri,"Additionally, the NIST/ISMRM system phantom (System Stan-dard Model 130, CaliberMRI (previously QalibreMD), Boulder, Col-orado; https://www.nist.gov/programs-projects/quantitative-mri) was scanned in three separate scan sessions at room temperature (about 20°C).",0,1,0
10.1016/j.neuroimage.2022.119092,github.com/tleutritz-cbs/hmri-toolbox,"DICOM to NIfTI conversion was performed using the hMRI toolbox (version v0.1.3-dev; RRID: SCR_017682; https://github.com/tleutritz-cbs/hMRI-toolbox) and the Statistical Parametric Mapping (SPM) framework (SPM12, version v7771; RRID: SCR_007037; https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).",1,0,0
10.1016/j.neuroimage.2022.119092,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"DICOM to NIfTI conversion was performed using the hMRI toolbox (version v0.1.3-dev; RRID: SCR_017682; https://github.com/tleutritz-cbs/hMRI-toolbox) and the Statistical Parametric Mapping (SPM) framework (SPM12, version v7771; RRID: SCR_007037; https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).",1,0,0
10.1016/j.neuroimage.2022.119092,itksnap.org,"First, the phantom data were masked manually using ITK-SNAP (RRID: SCR_002010; http://www.itksnap.org) to obtain a mask of the whole volume of the phantom, which was needed for parameter map calculation.",1,0,0
10.1016/j.neuroimage.2022.118877,cran.r-project.org/web/packages/ciftitools/index.html,via the Connectome 1 https://cran.r-project.org/web/packages/ciftiTools/index.html 2 https://github.com/mandymejia/ciftiTools/Workbench.,0,0,1
10.1016/j.neuroimage.2022.118877,nitrc.org/frs/?group_id,"These are provided according to the HCP Data Use Terms: “Data were provided in part by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Cen-ters that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington 3 https://www.nitrc.org/frs/?group_id = 454 University ”(Glasser et al., 2013). 4 The ﬁles are accessed with the command ciftiTools.files()$surf or load_surf.",0,1,0
10.1016/j.neuroimage.2022.118877,github.com/mandymejia/ciftitools/workbench,via the Connectome 1 https://cran.r-project.org/web/packages/ciftiTools/index.html 2 https://github.com/mandymejia/ciftiTools/Workbench.,0,0,1
10.1016/j.neuroimage.2022.119508,github.com/lcnhappe/happe,"Second, the Harvard Automated Processing Pipeline for EEG (HAPPE; Gabard-Durnam et al., 2018 ; https://github.com/lcnhappe/happe) has been proposed to optimize the processing of short-duration resting-state and task-related recordings obtained with diﬀerent elec-trode layouts and systems.",1,0,0
10.1016/j.neuroimage.2022.119508,github.com/childdevlab/made-eeg-preprocessing-pipeline,"First, the “Maryland analysis of developmental EEG pipeline ”(MADE; Debnath et al., 2020 ; https://github.com/ChildDevLab/MADE-EEG-preprocessing-pipeline) is a Matlab-based toolbox that com-bines EEGLAB functions with customizable processing parameters that are well adapted for infants EEG data such as trial-level channel inter-polation (Buzzell et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.118892,neurosynth.org,"The decoder was trained to weight a term list that characterizes a 3D brain map based on meta-analysis of functional brain mapping (https://neurosynth.org/decoder/; Yarkoni et al., 2011)., Meta-analysis maps To further characterize the current activation maps, meta-analysis maps were obtained from Neurosynth (https://neurosynth.org/; Yarkoni et al., 2011).",0,0,1
10.1016/j.neuroimage.2022.118892,ﬁl.ion.ucl.ac.uk/spm/software/spm12,Image preprocessing Image preprocessing was performed using SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).,1,0,0
10.1016/j.neuroimage.2022.118892,fsl.fmrib.ox.ac.uk/fsl/fslwiki/randomize,"Statistical test-ing was performed based on nonparametric permutation testing (5000 times) implemented in randomize in FSL suite (Winkler et al., 2014) (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/randomize).",1,0,0
10.1016/j.neuroimage.2022.118892,neurosynth.org/decoder,"The decoder was trained to weight a term list that characterizes a 3D brain map based on meta-analysis of functional brain mapping (https://neurosynth.org/decoder/; Yarkoni et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119506,osf.io/pse3k/3,Data and Code availability statement Data and codes for the behavior analyses and fMRI group analyses are available here: https://osf.io/pse3k/3.,0,1,0
10.1016/j.neuroimage.2022.119142,github.com/mickcrosse/permutools,"To this end, we performed permutation-based correlation analyses (number of per-mutations = 10000) with the PERMUTOOLS toolbox in Matlab (https://github.com/mickcrosse/PERMUTOOLS) to explore the associations between the Condition -modulated theta-network measures (i.e., the dif-ference values between RW and RS) and participants’ reading, RAN and phonological awareness skills.",1,0,0
10.1016/j.neuroimage.2022.119714,tmslab.org/netconlab.php,"We conﬁrm that we have provided a current, correct email address which is accessible by the Correspond-ing Author and which has been conﬁgured to accept email from: ari-anna.menardi@gmail.com Data sharing Raw and preprocessed data are available at Berenson-Allen Cen-ter for Noninvasive Brain Stimulation site (http://www.tmslab.org/netconlab.php).",0,0,1
10.1016/j.neuroimage.2022.119714,simnibs.org,"Further-more, the scalp-to-cortex distance and the induced normalized electric ﬁeld were computed by means of SimNIBS v3.2.6 (www.simnibs.org) to control for the possibility that diﬀerences in the induced activity be-tween stimulation conditions were not due to diﬀerences in the amount of stimulation reaching the brain (see Supplementary Materials).",1,0,0
10.1016/j.neuroimage.2022.119185,nitrc.org/projects/bioimagesuite,Data and code availability statement Matlab scripts to run the CPM analyses can be found at https://www.nitrc.org/projects/bioimagesuite/.,1,0,0
10.1016/j.neuroimage.2022.119185,nitrc.org/projects/gretna,"Using the GRETNA (http://www.nitrc.org/projects/gretna/) tool-box (Wang et al., 2015), the Shen brain atlas was applied to parcellate the brain into 268 regions of interest including the cortex, subcortex and cerebellum to deﬁne the network nodes (Shen et al., 2013), as in previous CPM studies (Beaty et al., 2018 ; Finn et al., 2015 ; Lake et al., 2019 ; Rosenberg et al., 2016 ; Yip et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119185,ﬁl.ion.ucl.ac.uk/spm,SPM12 software (http://www.ﬁl.ion.ucl.ac.uk/spm) was used for pre-processing.,1,0,0
10.1016/j.neuroimage.2022.119185,bioimagesuite.com,"The results were visualized using BioImage Suite (http://bioimagesuite.com/)., Tools used for visual-ization can be accessed at http://bioimagesuite.com/.",1,0,0
10.1016/j.neuroimage.2021.118797,github.com/jdızhu,Data code availability statement The data acquired for this study are available at the following link: https://cloudstor.aarnet.edu.au/plus/s/x70WHJ2pzl3TmN4 The code used in this study can be accessed at: https://github.com/JDıZhu Appendix A.,0,1,0
10.1016/j.neuroimage.2021.118797,github.com/macquarie-meg-research/memes,"Instead, we used the MEMES toolbox (Seymour, 2018 ; https://github.com/Macquarie-MEG-Research/MEMES) to search through an existing MRI database and estimate the best-matching structural scan for each participant (based on their head shape infor-mation collected during the MEG session).",1,0,0
10.1016/j.neuroimage.2021.118797,cloudstor.aarnet.edu.au/plus/s/x70whj2pzl3tmn4,Data code availability statement The data acquired for this study are available at the following link: https://cloudstor.aarnet.edu.au/plus/s/x70WHJ2pzl3TmN4 The code used in this study can be accessed at: https://github.com/JDıZhu Appendix A.,0,1,0
10.1016/j.neuroimage.2021.118797,ausnc.org.au/corpora/ice),"The lexical frequency data for English were retrieved from the Aus-tralian component of the International Corpus of English (ICE-AUS; https://www.ausnc.org.au/corpora/ice), and the lexical frequency data for Mandarin were retrieved from the Chinese Corpus Online database (www.cncorpus.org).",0,1,0
10.1016/j.neuroimage.2021.118797,cncorpus.org,"The lexical frequency data for English were retrieved from the Aus-tralian component of the International Corpus of English (ICE-AUS; https://www.ausnc.org.au/corpora/ice), and the lexical frequency data for Mandarin were retrieved from the Chinese Corpus Online database (www.cncorpus.org).",0,1,0
10.1016/j.neuroimage.2022.119735,dandiarchive.org,"Here, we contribute to chang-ing the status quo by making our dataset freely available through the DANDI Archive (https://dandiarchive.org/) –a data sharing platform designed to accommodate neurophysiology, electrophysiology, opto-physiology and behavioral time-series data., (2015)) format and download-able from DANDI (https://dandiarchive.org/dandiset/000244)., Data Availability The data are available on DANDI; https://dandiarchive.org/dandiset/000244.",0,1,0
10.1016/j.neuroimage.2022.119735,github.com/bioimagesuite,"Alternatively, BIS-MID can be accessed and installed by following the instructions on GitHub: https://github.com/bioimagesuiteweb/bisweb., These steps are all accomplished using tools that are freely available in BIS (https://github.com/bioimagesuiteweb/bisweb)., Code availability This pipeline is available as part of BioImage Suite (github.com/bioimagesuite) for installation, or as a singularity container.",1,0,0
10.1016/j.neuroimage.2022.119735,bioimagesuiteweb.github.io/bisweb-manual,"Brieﬂy, aﬃne registration matrices are generated between the atlas and the reference image for each dataset which are then applied (framewise) to the data (refer to BioImage Suite documentation for more details: https://bioimagesuiteweb.github.io/bisweb-manual/).",0,0,1
10.1016/j.neuroimage.2022.119735,github.com/yalemrrc/calprep,"An example image can be found at https://github.com/YaleMRRC/calPrep., Methods for triaging mislabeled frames are detailed online (https://github.com/YaleMRRC/calPrep)., To make installation as easy as possible, we have created a container using singularity; down-loadable from a link on our GitHub repository: https://github.com/YaleMRRC/calPrep., Detailed in-structions for usage can be found here: https://github.com/YaleMRRC/calPrep.",0,0,1
10.1016/j.neuroimage.2022.119735,dandiarchive.org/dandiset/000244,"(2015)) format and download-able from DANDI (https://dandiarchive.org/dandiset/000244)., Data Availability The data are available on DANDI; https://dandiarchive.org/dandiset/000244.",0,1,0
10.1016/j.neuroimage.2022.119735,bioimagesuiteweb.github.io/webapp/dualviewer.html,Registration to the atlas was accomplished using the manual registration tool in BioImage Suite Web (https://bioimagesuiteweb.github.io/webapp/dualviewer.html).,1,0,0
10.1016/j.neuroimage.2022.119735,dandiarchive.org,"Here, we address some of the practical challenges of sharing data by leveraging existing infrastructure ((Teeters et al., 2015) & https://www.dandiarchive.org/).",0,1,0
10.1016/j.neuroimage.2022.119735,sylabs.io/guides/3.5/user-guide/index.html,Resources for singularity are located here: https://sylabs.io/guides/3.5/user-guide/index.html.,0,0,1
10.1016/j.neuroimage.2022.119735,atlas.brain-map.org,"Creation of 2D Allen Atlas in mesoscale imaging common space The annotated CCFv3 (Allen mouse Common Coordinate Framework version 3), data, and ontology were downloaded from the Allen Insti-tute (http://atlas.brain-map.org/).",0,1,0
10.1016/j.neuroimage.2022.119735,github.com/bioimagesuiteweb/bisweb,"Alternatively, BIS-MID can be accessed and installed by following the instructions on GitHub: https://github.com/bioimagesuiteweb/bisweb., These steps are all accomplished using tools that are freely available in BIS (https://github.com/bioimagesuiteweb/bisweb).",1,0,0
10.1016/j.neuroimage.2022.119735,nilearn.github.io/stable/modules/generated/nilearn.plotting.show.html,"NetworkX and matplotlib can be combined, along with Nilearn, to generate brain-based network graphs if needed: https://nilearn.github.io/stable/modules/generated/nilearn.plotting.show.html.",1,0,0
10.1016/j.neuroimage.2021.118714,childrens.harvard.edu,"Levenstein), simon.warﬁeld@ childrens.harvard.edu (S.",0,0,1
10.1016/j.neuroimage.2021.118714,partners.org,"Brüggemann), nsharma@ partners.org (N.",0,0,1
10.1016/j.neuroimage.2022.119676,osf.io/b8pfa/?view_only,Data Availability Preprocessed fMRI data are available at https://osf.io/b8pfa/?view_only = b6dbb5dd6a044989a7eecdc99facb43c.,0,1,0
10.1016/j.neuroimage.2022.119676,github.com/yozaﬁrova/monkey-fmri-codes,Codes for the fMRI data analysis at https://github.com/Yozaﬁrova/monkey-fMRI-codes and for the CNN data analysis at https://github.com/RajaniRaman/face_body_integration.,1,0,0
10.1016/j.neuroimage.2022.119676,shutterstock.com,"Both experiments employed static images (modiﬁed from Shutterstock, https://www.shutterstock.com).",0,0,1
10.1016/j.neuroimage.2022.119676,caﬀe.berkeleyvision.org/model_zoo.html,Another version of pre-trained AlexNet was im-ported from Caﬀe Model Zoo (https://caﬀe.berkeleyvision.org/model_zoo.html).,1,0,0
10.1016/j.neuroimage.2022.119676,github.com/rajaniraman/face_body_integration,Codes for the fMRI data analysis at https://github.com/Yozaﬁrova/monkey-fMRI-codes and for the CNN data analysis at https://github.com/RajaniRaman/face_body_integration.,1,0,0
10.1016/j.neuroimage.2022.119676,nitrc.org/projects/jip,"Data preprocessing Functional volumes were realigned and motion-corrected with the Statistical Parametric Mapping software (SPM12, RRID: SCR_007037), followed by non-rigid co-registration (using JIP, http://www.nitrc.org/projects/jip , RRID: SCR_009588) to the high-resolution anatomical template of the skull-stripped brain of each monkey.",1,0,0
10.1016/j.neuroimage.2022.119676,clippingmagic.com,"NeuroImage 264 (2022) 119676 All image transformations were done with Clipping Magic (https://clippingmagic.com), ImageMagick, GIMP, Microsoft Paint, the MATLAB SHINE toolbox, and custom MATLAB code.",1,0,0
10.1016/j.neuroimage.2022.119617,bic.mni.mcgill.ca/~noel/noel-myelin,"Code and data availability The MYATLAS, lookup tables, source codes and the scripts for ap-plying the atlas to Colin27 and Conte69 brain templates as well as to individual brains (“mapping_colin27_labels_onto_individuals_batch.sh ”) are available from this link (https://www.bic.mni.mcgill.ca/~noel/noel-myelin/) and from the BALSA neuroimaging database (https://balsa., Code and data availability The MYATLAS, lookup tables, source codes and the scripts for ap-plying the atlas to Colin27 and Conte69 brain templates as well as to individual brains (“mapping_colin27_labels_onto_individuals_batch.sh ”) are available from this link (https://www.bic.mni.mcgill.ca/~noel/noel-myelin/) and from the BALSA neuroimaging database (https://balsa.",1,0,0
10.1016/j.neuroimage.2022.119617,github.com/kwagstyl/surface_tools.git,"To correlate the depth of proﬁles acquired from digitized histo-logical data microphotographs with in vivo intracortical qT1, we posi-tioned 10 equivolume surfaces between the inner and outer cortical in-terface using equivolumetric_surfaces.py (https://github.com/kwagstyl/surface_tools.git).",1,0,0
10.1016/j.neuroimage.2022.119617,wustl.edu,"wustl.edu)., wustl.edu).",0,0,1
10.1016/j.neuroimage.2022.119617,surfer.nmr.mgh.harvard.edu/fswiki/tksurfer,Labelling was performed with “tksurfer ”(surfer.nmr.mgh.harvard.edu/fswiki/TkSurfer).,1,0,0
10.1016/j.neuroimage.2022.119617,packages.bic.mni.mcgill.ca/mni-models/colin27/mni_colin27_1998_nifti.zip,"MRI processing We utilized a stereotaxic average of the individual Colin27 brain comprising 27 T1w MRI with 1mm isotropic voxel resolution (http://packages.bic.mni.mcgill.ca/mni-models/colin27/mni_colin27_1998_nifti.zip ; Holmes et al., 1998).",0,0,1
10.1016/j.neuroimage.2022.118918,ﬁl.ion.ucl.ac.uk,fMRI preprocessing The data were preprocessed and analyzed using SPM12 (http://www.ﬁl.ion.ucl.ac.uk).,1,0,0
10.1016/j.neuroimage.2022.118918,0.14.0.0,"Statistical analyses were conducted with SPSS 27.0, JASP 0.14.0.0 and R software (R Core Team, 2018).",1,0,0
10.1016/j.neuroimage.2022.118872,subcortex.eu/app,"We also analyzed the R1, R2 ∗ , and QSM values, which can be found in the online app (https://subcortex.eu/app)., To help navigate the winning models of each structure and measure (in-cluding R1, R2 ∗ , and QSM values), we developed an online interactive app, which is accessible at https://subcortex.eu/app (see also Fig., S4, https://subcortex.eu/app) that can be used to create interactive and intuitive 3D visualizations of the human subcortex across the lifespan and across modalities., All derived participant-wise and region-wise measures can be downloaded from our app at https://subcortex.eu/app.",0,0,1
10.1016/j.neuroimage.2022.118872,osf.io/mvdbe,All code used to estimate the models and pro-duce the ﬁgures can be found at https://osf.io/mvdbe/.,1,0,0
10.1016/j.neuroimage.2022.119517,pixabay.com,"Stimuli Stimuli were 400 realistic colour images collected from free online image databases (www.pixabay.com , www.pexels.com) under Creative Commons 0 licenses, and were used in all studies.",0,1,0
10.1016/j.neuroimage.2022.119517,pavlovia.org,"Stimulus validation and model generation To validate the stimulus set and generate behavioural models to com-pare to the EEG data, we ran an online experiment using Amazon’s Me-chanical Turk platform, guided by Grootswagers (2020) , programmed using jsPsych (de Leeuw, 2015) and hosted on Pavlovia (Peirce et al., 2019 , https://www.pavlovia.org/).",0,0,1
10.1016/j.neuroimage.2022.119517,osf.io/jxhcs,"Stimuli, analysis scripts, results and data from the online stim-ulus validation study are also publicly available at https://osf.io/jxhcs/., Stimuli, analysis scripts, results and data are publicly available at https://osf.io/jxhcs/., Stimuli, analy-sis scripts, results and data from the online stimulus validation study are also publicly available at https://osf.io/jxhcs/.",0,0,1
10.1016/j.neuroimage.2022.119517,github.com/cechava/rectilinearity_toolbox,"(2014) , calculated using publicly available code from  https://github.com/cechava/Rectilinearity_Toolbox .",1,0,0
10.1016/j.neuroimage.2022.119517,pexels.com,"Stimuli Stimuli were 400 realistic colour images collected from free online image databases (www.pixabay.com , www.pexels.com) under Creative Commons 0 licenses, and were used in all studies.",0,1,0
10.1016/j.neuroimage.2022.119253,neurosynth.org,"Then, we obtained four association test masks from Neurosynth (https://neurosynth.org/) using the following keywords, respectively: reward, selfreferential, moral, and cognitive control.",0,0,1
10.1016/j.neuroimage.2022.119253,bic.mni.mcgill.ca/servicesatlases/homepage,The functional data from individual native space was transformed to the standard Montreal Neurological Institute (MNI) space (http://www.bic.mni.mcgill.ca/ServicesAtlases/HomePage).,0,1,0
10.1016/j.neuroimage.2022.119253,food-pics.sbg.ac.at,The food images were selected from Food-Pics Extended (http://food-pics.sbg.ac.at).,0,0,1
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/simulated,"The 4 https://brainets.github.io/frites/api/generated/frites.workﬂow.WfStats.html 5 https://brainets.github.io/frites/simulated data consisted in two variables: a 3D multivariate variable 𝑋, which simulates the spatio-temporal structure of electrophysiologi-cal data with the dimensions describing the number of trials, the num-ber of brain regions and the number of time points and a univariate variable 𝑌 , reﬂecting a task-related continuous variable with the same number of trials as in X.",0,1,0
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/auto_examples/simulations/plot_ground_truth.html#sphx-glr-auto-examples-simulations-plot-ground-truth-py,"The open-source accessibility of the package and the ability to track both internal and external variables, contribute to a reproducible science as it allows sharing self-contained 8 https://brainets.github.io/frites/auto_examples/simulations/plot_ground_truth.html#sphx-glr-auto-examples-simulations-plot-ground-truth-py ﬁles that include all of the parameters used for understanding and re-producing a result.",1,0,0
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/auto_examples/estimators/plot_est_comparison.html#sphx-glr-auto-examples-estimators-plot-est-comparison-py,The present work tends to provide more robust inferences on populations and therefore is a con-tributing piece toward more reproducible results. 9 https://brainets.github.io/frites/auto_examples/estimators/plot_est_comparison.html#sphx-glr-auto-examples-estimators-plot-est-comparison-py 14 E.,1,0,0
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/api/generated/frites.workﬂow.wfmi.html,The key objective 3 https://brainets.github.io/frites/api/generated/frites.workﬂow.WfMi.html 4 E.,0,0,1
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/index.html,"An example illustrating the simulated ground-truths, the computations of the statistics at the group-level such as the deﬁnition of the fractions and statistical metrics can be found in the documentation of the software 6. 6 https://brainets.github.io/frites/index.html 6 E.",1,0,0
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/2,Permutation-based signiﬁcance testing shortcuts distributional assumptions by relying on the exchangeability 2 https://brainets.github.io/frites/2 E.,1,0,0
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/level,"This frame-work supports any type of measure of information and can be used at the level of single brain area activity, connectivity-level inter-areal interac-tions (e.g., (un)directed functional connectivity measures) and network-1 https://brainets.github.io/frites/level (e.g., on graph-theoretical measures).",1,0,0
10.1016/j.neuroimage.2022.119347,humanconnectomeproject.org/13,"Overall, the GCMI is a versatile and gen-7 http://www.humanconnectomeproject.org/13 E.",0,0,1
10.1016/j.neuroimage.2022.119347,brainets.github.io/frites/api/generated/frites.workﬂow.wfstats.html,"The 4 https://brainets.github.io/frites/api/generated/frites.workﬂow.WfStats.html 5 https://brainets.github.io/frites/simulated data consisted in two variables: a 3D multivariate variable 𝑋, which simulates the spatio-temporal structure of electrophysiologi-cal data with the dimensions describing the number of trials, the num-ber of brain regions and the number of time points and a univariate variable 𝑌 , reﬂecting a task-related continuous variable with the same number of trials as in X.",0,1,0
10.1016/j.neuroimage.2022.118991,sccn.ucsd.edu/eeglab/download.php,Code availability Preprocessing of the EEG data was done using the publicly available HAPPE pipeline V1 (DOI: 10.3389/fnins.2018.00097; download: https://github.com/lcnhappe/happe) in EEGLAB v2019.1 (DOI: https://doi.org/10.1515/bmt-2013-4182 ; download: https://sccn.ucsd.edu/eeglab/download.php) and in ﬁeldtrip (ver-sion from 20200521) (DOI: https://doi.org/10.1155/2011/156869 ; download: https://www.ﬁeldtriptoolbox.org/download.php).,1,0,0
10.1016/j.neuroimage.2022.118991,github.com/lcnhappe/happe,Code availability Preprocessing of the EEG data was done using the publicly available HAPPE pipeline V1 (DOI: 10.3389/fnins.2018.00097; download: https://github.com/lcnhappe/happe) in EEGLAB v2019.1 (DOI: https://doi.org/10.1515/bmt-2013-4182 ; download: https://sccn.ucsd.edu/eeglab/download.php) and in ﬁeldtrip (ver-sion from 20200521) (DOI: https://doi.org/10.1155/2011/156869 ; download: https://www.ﬁeldtriptoolbox.org/download.php).,1,0,0
10.1016/j.neuroimage.2022.119385,ukbiobank.ac.uk/ethics,Ethical procedures are controlled by a dedicated Ethics Advisory Com-mittee (http://www.ukbiobank.ac.uk/ethics).,0,0,1
10.1016/j.neuroimage.2022.119385,git.fmrib.ox.ac.uk/ndcn1032/mrneuroimg,Code Availability Supporting code for the example applications is available at https://git.fmrib.ox.ac.uk/ndcn1032/mrneuroimg.,1,0,0
10.1016/j.neuroimage.2022.119385,gwas.mrcieu.ac.uk,GWAS data are openly available from the IEU Open GWAS Project at https://gwas.mrcieu.ac.uk/.,0,1,0
10.1016/j.neuroimage.2022.119723,tedana.py,"For optimally combined and ME-ICA denoised procedures, the optimal combination of the three echoes was calculated, and the echoes were combined to form a single, optimally weighted timeseries (T2smap.py, distributed with tedana.py (DuPre et al., 2021))., The ME-ICA data were denoised using TE-dependent multi-echo ICA denoising (tedana.py, version 0.0.1 (DuPre et al., 2021 , ; Evans et al., 2015 ; Kundu et al., 2013 , 2012)).",1,0,0
10.1016/j.neuroimage.2022.119723,t2smap.py,"For optimally combined and ME-ICA denoised procedures, the optimal combination of the three echoes was calculated, and the echoes were combined to form a single, optimally weighted timeseries (T2smap.py, distributed with tedana.py (DuPre et al., 2021)).",1,0,0
10.1016/j.neuroimage.2022.119702,github.com/larmarbar/r2orientationnewborns,The analysis code is publicly available through GitHub (github.com/Larmarbar/R2OrientationNewborns).,1,0,0
10.1016/j.neuroimage.2021.118748,memory.psych.upenn.edu/data,"Data availability All PEERS data, including the full dataset reported and analyzed in the present manuscript, may be freely downloaded from our pub-lic repository http://memory.psych.upenn.edu/data.",0,1,0
10.1016/j.neuroimage.2022.118994,github.com/autoreject/autoreject,"For feature-based methods, we instead precom-puted augmented datasets by applying the augmentation multiple times to each window (10 for pathology detection, 5 for sleep staging), and then extracting features from the augmented windows. 7 https://github.com/autoreject/autoreject 5 H.",1,0,0
10.1016/j.neuroimage.2022.118994,github.com/hubertjb/dynamic-spatial-ﬁltering,"Deep learning and baseline models were trained using a combi-nation of the braindecode (Schirrmeister et al., 2017), MNE-Python (Gramfort et al., 2014), PyTorch (Paszke et al., 2019), pyRiemann (Barachant et al., 2013a), mne-features (Schiratti et al., 2018) and scikit-learn (Pedregosa et al., 2011) packages. 5 Finally, deep learning models were trained on 1 or 2 Nvidia Tesla V100 or P4 GPUs for anywhere from a few minutes to 7 h, depending on the amount of data, early stopping and GPU conﬁguration. 5 Code used for data analysis can be found at https://github.com/hubertjb/dynamic-spatial-ﬁltering. 4 H.",1,0,0
10.1016/j.neuroimage.2022.118994,choosemuse.com/legal/privacy/and,This data was collected in accordance with the privacy pol-icy (July 2020) users must agree to when using the Muse headband 8 8 https://choosemuse.com/legal/privacy/and which ensures their informed consent concerning the use of EEG data for scientiﬁc research purposes.,0,1,0
10.1016/j.neuroimage.2022.119441,ipnp-https://crl.ucsd.edu/experiments/ipnp,"Stimuli We selected forty-eight pictures from the International Picture Nam-ing Project (IPNP-https://crl.ucsd.edu/experiments/ipnp/) based on the following variables in both Dutch and English: RT (mean), number of letters and syllables, initial fricative, H statistics, and word complexity (see Table 1 for a summary of the variables).",0,0,1
10.1016/j.neuroimage.2022.119441,crl.ucsd.edu/experiments/ipnp/method/getdata/uspnovariables.html,https://crl.ucsd.edu/experiments/ipnp/method/getdata/uspnovariables.html.,0,0,1
10.1016/j.neuroimage.2022.119441,fmrib.ox.ac.uk/fsl,"Pre-processing of fMRI data We processed the fMRI data using FSL software version 5.0.10 (FM-RIB’s Software Library, www.fmrib.ox.ac.uk/fsl) and we applied the following pre-statistics processing: motion correction using MCFLIRT (Jenkinson et al., 2002), spatial smoothing using a Gaussian kernel of FWHM 5 mm, non-brain removal using BET (Smith, 2002), grand-mean intensity normalization of the entire 4D dataset by a single multi-plicative factor, high-pass temporal ﬁltering (Gaussian-weighted least-squares straight line ﬁtting, with sigma = 50.0 s).",1,0,0
10.1016/j.neuroimage.2022.119460,jstatsoft.org,Phase analyses were performed using MATLAB CircStat toolbox (https://www.jstatsoft.org).,1,0,0
10.1016/j.neuroimage.2022.119460,openfnirs.org,FNIRS preprocessing was performed using HOMER2 (http://openfnirs.org/).,1,0,0
10.1016/j.neuroimage.2022.119460,iso2mesh.sourceforge.net,"The segmentation and FEM modeling were per-formed using FreeSurfer (https://surfer.nmr.mgh.harvard.edu), Mesh-Mixer (https://www.meshmixer.com), and ISO2MESH toolbox (http://iso2mesh.sourceforge.net/).",1,0,0
10.1016/j.neuroimage.2022.119460,meshmixer.com,"The segmentation and FEM modeling were per-formed using FreeSurfer (https://surfer.nmr.mgh.harvard.edu), Mesh-Mixer (https://www.meshmixer.com), and ISO2MESH toolbox (http://iso2mesh.sourceforge.net/).",1,0,0
10.1016/j.neuroimage.2022.119460,milab.host.dartmouth.edu/nirfast,The Jacobian was calculated using NIRFAST (https://milab.host.dartmouth.edu/nirfast/).,1,0,0
10.1016/j.neuroimage.2022.119460,sccn.ucsd.edu,Group spatial ICA was performed using MATLAB runica function (https://sccn.ucsd.edu/).,1,0,0
10.1016/j.neuroimage.2022.119460,mathworks.com/help/stats/kmeans.html,Clustering was performed us-ing MATLAB kmeans function (https://www.mathworks.com/help/stats/kmeans.html).,1,0,0
10.1016/j.neuroimage.2022.119460,surfer.nmr.mgh.harvard.edu,"The segmentation and FEM modeling were per-formed using FreeSurfer (https://surfer.nmr.mgh.harvard.edu), Mesh-Mixer (https://www.meshmixer.com), and ISO2MESH toolbox (http://iso2mesh.sourceforge.net/).",1,0,0
10.1016/j.neuroimage.2022.119460,polhemus.com,"The Polhemus Patriot handheld electromag-netic digitizer (polhemus.com) was used to record the 3D locations of optodes and head landmarks (i.e., nasion, left, and right preauricular points) before recording sessions.",1,0,0
10.1016/j.neuroimage.2022.119171,hosa.io,"NeuroImage 255 (2022) 119171 Contents lists available at ScienceDirect NeuroImage journal homepage: www.elsevier.com/locate/neuroimage Insights from an autism imaging biomarker challenge: Promises and threats to biomarker discovery Nicolas Traut a , i , † , Katja Heuer a , d , i , † , Guillaume Lemaître b , c , † , Anita Beggiato a , k , David Germanaud e , Monique Elmaleh f , Alban Bethegnies g , Laurent Bonnasse-Gahot l , Weidong Cai j , Stanislas Chambon p , Freddy Cliquet a , Ayoub Ghriss h , Nicolas Guigui e , Amicie de Pierrefeu e , Meng Wang n , o , Valentina Zantedeschi m , Alexandre Boucaud b , c , Joris van den Bossche b , c , Balázs Kegl q , Richard Delorme a , k , Thomas Bourgeron a , Roberto Toro a , + , Gaël Varoquaux b , r , + , ∗ a Institut Pasteur, Université de Paris, Département de neuroscience, F-75015 Paris, France b Parietal, Inria, Saclay, France c Paris-Saclay Center for Data Science, Université Paris Saclay, Saclay, France d Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany e Neurospin CEA, Saclay, France f Department of Radiology, Robert Debré, APHP, Paris, France g Hosa.io, Paris, France h University of Colorado, Boulder, US i Center for Research and Interdisciplinarity (CRI), Université Paris Descartes, Paris, France j Stanford University School of Medicine, Palo Alto, US k Child and Adolescent Psychiatry Department, Robert Debré, APHP, Paris, France l Centre d’Analyse et de Mathématique Sociales, EHESS, CNRS, PSL, Paris, France m Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d’Optique Graduate School, Laboratoire Hubert Curien UMR 5516, F-42023, Saint-Etienne, France n Brainnetome Center and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China o University of Chinese Academy of Sciences, Beijing 100049, China p Rythm.co, 75009 Paris q Huawei, Paris r Soda, Inria, Saclay, France a r t i c l e i n f o Keywords: Autism diagnostic machine learning benchmark overﬁt prediction a b s t r a c t MRI has been extensively used to identify anatomical and functional diﬀerences in Autism Spectrum Disorder (ASD).",0,0,1
10.1016/j.neuroimage.2022.119171,ramp.studio,Participants submitted their solutions on the RAMP website (https://ramp.studio).,0,0,1
10.1016/j.neuroimage.2022.119171,github.com/neuroa,"Data and code availability: The public data, the code of the ten best submissions, the MRI preprocessing scripts and the scripts used to generate the ﬁgures are available at https://github.com/neuroa natomy/autism-challenge.",0,1,0
10.1016/j.neuroimage.2022.119171,rythm.co,"NeuroImage 255 (2022) 119171 Contents lists available at ScienceDirect NeuroImage journal homepage: www.elsevier.com/locate/neuroimage Insights from an autism imaging biomarker challenge: Promises and threats to biomarker discovery Nicolas Traut a , i , † , Katja Heuer a , d , i , † , Guillaume Lemaître b , c , † , Anita Beggiato a , k , David Germanaud e , Monique Elmaleh f , Alban Bethegnies g , Laurent Bonnasse-Gahot l , Weidong Cai j , Stanislas Chambon p , Freddy Cliquet a , Ayoub Ghriss h , Nicolas Guigui e , Amicie de Pierrefeu e , Meng Wang n , o , Valentina Zantedeschi m , Alexandre Boucaud b , c , Joris van den Bossche b , c , Balázs Kegl q , Richard Delorme a , k , Thomas Bourgeron a , Roberto Toro a , + , Gaël Varoquaux b , r , + , ∗ a Institut Pasteur, Université de Paris, Département de neuroscience, F-75015 Paris, France b Parietal, Inria, Saclay, France c Paris-Saclay Center for Data Science, Université Paris Saclay, Saclay, France d Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany e Neurospin CEA, Saclay, France f Department of Radiology, Robert Debré, APHP, Paris, France g Hosa.io, Paris, France h University of Colorado, Boulder, US i Center for Research and Interdisciplinarity (CRI), Université Paris Descartes, Paris, France j Stanford University School of Medicine, Palo Alto, US k Child and Adolescent Psychiatry Department, Robert Debré, APHP, Paris, France l Centre d’Analyse et de Mathématique Sociales, EHESS, CNRS, PSL, Paris, France m Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d’Optique Graduate School, Laboratoire Hubert Curien UMR 5516, F-42023, Saint-Etienne, France n Brainnetome Center and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China o University of Chinese Academy of Sciences, Beijing 100049, China p Rythm.co, 75009 Paris q Huawei, Paris r Soda, Inria, Saclay, France a r t i c l e i n f o Keywords: Autism diagnostic machine learning benchmark overﬁt prediction a b s t r a c t MRI has been extensively used to identify anatomical and functional diﬀerences in Autism Spectrum Disorder (ASD).",0,0,1
10.1016/j.neuroimage.2022.118888,hpc.nih.gov,This work utilized the computational resources of the NIH HPC Biowulf cluster (http://hpc.nih.gov).,1,0,0
10.1016/j.neuroimage.2022.119744,neurosynth.org/analyses/terms,"To determine the ROIs in the brain outside the hypothalamus associ-ated with reward processing, we used the term-based meta-analysis sys-tem on Neurosynth (neurosynth.org/analyses/terms/) (Yarkoni et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119744,ﬁl.ion.ucl.ac.uk/spm,"After spatial smoothing (FWHM = 4 mm), a general linear model was applied to each voxel using SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119198,github.com/andy1764/fcharmony,The harmonization methods used in the paper are implemented in the FCHarmony package available at https://github.com/andy1764/FCHarmony.,1,0,0
10.1016/j.neuroimage.2022.119198,fmrib.ox.ac.uk/fsl,All pre-processing steps are carried out using FSL FEAT (www.fmrib.ox.ac.uk/fsl).,1,0,0
10.1016/j.neuroimage.2022.119198,aaronclauset.github.io/wsbm,We ﬁx the number of communities at 𝐾 = 14 for comparison with the atlas and implement the WSBM using code available online (aaronclauset.github.io/wsbm ; version 1.3).,1,0,0
10.1016/j.neuroimage.2022.119198,sites.google.com/site/bctnet,Network analyses are performed using the Brain Connec-tivity Toolbox (BCT) version 2019-03-03 available on their website at https://sites.google.com/site/bctnet/.,1,0,0
10.1016/j.neuroimage.2021.118738,readme.md,More details on installation and execution of the code can be found in the README.md ﬁle in the main folder of the repository.,0,0,1
10.1016/j.neuroimage.2021.118738,translationalneuromodeling.org/tapas,"Image and fMRI analyses were performed using SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm , distributed under GPLv2) and the in-house developed UniQC Toolbox (Bollmann et al., 2018), publicly available under a GPLv3 license as a beta release within the TAPAS soft-ware collection (https://www.translationalneuromodeling.org/tapas , (Frässle et al., 2021)).",1,0,0
10.1016/j.neuroimage.2021.118738,github.com/mrikasper/paper-advances-in-spiral-fmri,All custom analysis and data visualization scripts for this publication are available on https://github.com/mrikasper/paper-advances-in-spiral-fmri.,1,0,0
10.1016/j.neuroimage.2021.118738,neurovault.org/collections/6086,"Mean spiral fMRI images with corresponding activation t-maps for all sub-jects are also made available on NeuroVault for interactive viewing ((Gorgolewski et al., 2015), https://neurovault.org/collections/6086/).",0,0,1
10.1016/j.neuroimage.2021.118738,ﬁl.ion.ucl.ac.uk/spm,"Image and fMRI analyses were performed using SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm , distributed under GPLv2) and the in-house developed UniQC Toolbox (Bollmann et al., 2018), publicly available under a GPLv3 license as a beta release within the TAPAS soft-ware collection (https://www.translationalneuromodeling.org/tapas , (Frässle et al., 2021)).",1,0,0
10.1016/j.neuroimage.2021.118738,github.com/mrtm-zurich/rrsg-arbitrary-sense,"A demonstration of that algorithm is publicly available on GitHub (https://github.com/mrtm-zurich/rrsg-arbitrary-sense), with a static compute capsule for reproducible online re-execution on 16 L.",1,0,0
10.1016/j.neuroimage.2021.118738,github.com/mrikasper/julia-recon-advances-in-spiral-fmri,"An example reconstruction pipeline including static B 0 correction for the spiral data presented here is available on GitHub as well (https://github.com/mrikasper/julia-recon-advances-in-spiral-fmri), utilizing MRIReco.jl (Knopp and Grosser, 2021), an MRI reconstruction framework written in Julia (Bezanson et al., 2017).",1,0,0
10.1016/j.neuroimage.2021.118738,ﬁl.ion.ucl.ac.uk/spm,"All computations were performed in Matlab R2019b, using the Uni-ﬁed NeuroImaging Quality Control Toolbox (UniQC, (Bollmann et al., 2018 ; Frässle et al., 2021)), and SPM12 (Wellcome Centre for Human Neuroimaging, London, UK, http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119665,pubmed.ncbi.nlm.nih.gov,"Literature search We searched for relevant neuroimaging studies in MEDLINE us-ing PubMed (https://pubmed.ncbi.nlm.nih.gov/) through the following search string: (fMRI OR PET) AND (""attention control"" OR ""attention regulation"" OR ""attention shift"" OR ""attention shifts"" OR ""attention shift-ing"" OR ""attention switch"" OR ""attention switches"" OR ""attention switch-ing"" OR ""attentional control"" OR ""attentional regulation"" OR ""attentional shift"" OR ""attentional shifts"" OR ""attentional shifting"" OR ""attentional switch"" OR ""attentional switches"" OR ""attentional switching"" OR ""cog-nitive control"" OR ""cognitive ﬂexibility"" OR ""conﬂict monitoring"" OR ""conﬂict resolution"" OR ""eﬀortful control"" OR ""executive attention"" OR ""executive control"" OR ""executive function"" OR ""executive functioning"" OR ""executive functions"" OR ""goal directed behavior"" OR ""goal directed behavior"" OR ""goal directed control"" OR ""goal directed response"" OR 2 G.",0,0,1
10.1016/j.neuroimage.2022.119665,osf.io/d85pb/?view_only,"In concordance with the principles of Data Sharing and Trans-parency, the 3D maps of the above mentioned results (NIfTI ﬁles for global, domain and task analyses) are available at https://osf.io/d85pb/?view_only = f0b3fcad577d4ﬀ39cb4e6f3cd34e63f., NeuroImage 264 (2022) 119665 Data and code availability statement The individual reports of the studies included in the global anal-yses and in each category (domains and subdomains), the Matlab codes and resulting NIfTI ﬁles can be consulted at: https://osf.io/d85pb/?view_only = f0b3fcad577d4ﬀ39cb4e6f3cd34e63f.",0,0,1
10.1016/j.neuroimage.2022.119665,osf.io/63pt5/?view_only,"Data and code availability statement The individual reports of the studies included in the global analyses and in each category (domains and subdomains), the Matlab codes and resulting NIfTI ﬁles can be consulted at: https://osf.io/63pt5/?view_only = f0b3fcad577d4ﬀ39cb4e6f3cd34e63f.",1,0,0
10.1016/j.neuroimage.2022.119459,web.conn-toolbox.org/conn-in-pictures,These networks were composed of 32 ROIs (see Table S1 for the list of ROIs and CONN website for their locations https://web.conn-toolbox.org/conn-in-pictures).,0,0,1
10.1016/j.neuroimage.2022.119459,nitrc.org/projects/conn,"To conﬁrm this, the task-related functional connectivity in SL-related brain regions (obtained from the parametric modulation re-sults) was tested using a whole-brain seed-to-voxel correlation analysis with CONN toolbox (www.nitrc.org/projects/conn, RRID:SCR_009550) (Whitﬁeld-Gabrieli and Nieto-Castanon, 2012).",1,0,0
10.1016/j.neuroimage.2022.119459,icatb.sourceforge.net,"In the present study, group ICA was performed using GIFT toolbox (http://icatb.sourceforge.net/) (Calhoun et al., 2001).",1,0,0
10.1016/j.neuroimage.2022.119459,github.com/jungtak05/statistical-learning-fmri-study,The behavioral data and analysis code are released at https://github.com/jungtak05/Statistical-learning-fMRI-study.,1,0,0
10.1016/j.neuroimage.2022.119459,ncss.com/software/pass,"A sample size was computed using PASS software (https://www.ncss.com/software/pass/) based on a previous study (Park et al., 2020) that used the same statistical learning task.",1,0,0
10.1016/j.neuroimage.2022.119459,ﬁl.ion.ucl.ac.uk/spm,"fMRI preprocessing Preprocessing of fMRI data was done using the SPM12 software pack-age (http://www.ﬁl.ion.ucl.ac.uk/spm/) with the following steps: slice timing correction with the ﬁrst slice as a reference slice, realignment to correct head motion artifacts using 6 aﬃne head motion parameters, co-registration to individual T1 structural image, normalisation to Mon-treal Neurological Institute (MNI) template brain image, and smoothing with a Gaussian kernel of 6 mm full width at half maximum and resam-pling of functional images to 3 ×3 ×3 mm.",1,0,0
10.1016/j.neuroimage.2022.119459,openneuro.org/datasets/ds003401/versions/1.0.1,"The detailed information about raw fMRI data can be found in https://openneuro.org/datasets/ds003401/versions/1.0.1. 3 J., Data and code availability statement All raw MRI data was uploaded on ’OpenNeuro’ (https://openneuro.org/datasets/ds003401/versions/1.0.1) and it is publicly available.",0,1,0
10.1016/j.neuroimage.2022.119197,scikit-learn.org/stable,On the other 3 https://scikit-learn.org/stable/.,0,0,1
10.1016/j.neuroimage.2022.119197,framagit.org/cpoupon/gkg,The preprocessing results in an 1 https://framagit.org/cpoupon/gkg.,1,0,0
10.1016/j.neuroimage.2022.119197,surfer.nmr.mgh.harvard.edu,"However, without a careful cross-subject alignment of the local folding pattern, the ﬁber bundles 2 https://surfer.nmr.mgh.harvard.edu/. 3 M.",0,0,1
10.1016/j.neuroimage.2022.119749,neuroimage.usc.edu/brainstorm,"Code and data availability MEG and MRI data were analyzed in MATLAB 2019a (The Math-works, Inc.) using the FieldTrip v20190419 (ﬁeldtriptoolbox.org) and Brainstorm, v060320 (neuroimage.usc.edu/brainstorm/) toolboxes.",1,0,0
10.1016/j.neuroimage.2022.119749,surfer.nmr.mgh.harvard.edu,"To obtain a description of the participant’s cortical sheet, cortical surface re-construction was performed using the Freesurfer image analysis suite (surfer.nmr.mgh.harvard.edu).",1,0,0
10.1016/j.neuroimage.2022.119749,nist.mni.mcgill.ca,"For group-level analysis, participant-speciﬁc source estimates (un-thresholded t-statistics computed in the ﬁrst-level step) were projected onto a default surface anatomy (MNI-152, nist.mni.mcgill.ca).",1,0,0
10.1016/j.neuroimage.2022.119749,github.com/vyoussofzadeh/dics-beamformer-for-brainstorm,"A Brainstorm implementation of the DICS beamformer is available at, github.com/vyoussofzadeh/DICS-beamformer-for-Brainstorm.",1,0,0
10.1016/j.neuroimage.2022.119749,aesopfables.com,"The story blocks present participants with brief auditory stories (5–9 sentences) adapted from Aesop’s fables (www.aesopfables.com), followed by a 2-alternative forced-choice question that asks participants about the topic of the story., The Story task pre-sented participants with brief spoken stories adapted from Aesop’s fa-bles (aesopfables.com), followed by a 2-alternative forced-choice ques-tion about the topic of the story.",0,0,1
10.1016/j.neuroimage.2022.119749,aesopfables.com,"The story blocks present participants with brief auditory stories (5–9 sentences) adapted from Aesop’s fables (www.aesopfables.com), followed by a 2-alternative forced-choice question that asks participants about the topic of the story.",0,0,1
10.1016/j.neuroimage.2022.119303,surfer.nmr.mgh.harvard.edu/fswiki/tracula,"White matter tracts were assessed using FreeSurfer-TRACULA (TRActs Constrained by UnderLying Anatomy) (version 7.2.2, freely available online https://surfer.nmr.mgh.harvard.edu) which reconstructs 42 major white matter pathways using global probabilistic tractography with anatomical neighborhood priors (https://surfer.nmr.mgh.harvard.edu/fswiki/Tracula).",1,0,0
10.1016/j.neuroimage.2022.119303,surfer.nmr.mgh.harvard.edu,"White matter tracts were assessed using FreeSurfer-TRACULA (TRActs Constrained by UnderLying Anatomy) (version 7.2.2, freely available online https://surfer.nmr.mgh.harvard.edu) which reconstructs 42 major white matter pathways using global probabilistic tractography with anatomical neighborhood priors (https://surfer.nmr.mgh.harvard.edu/fswiki/Tracula).",1,0,0
10.1016/j.neuroimage.2022.119659,dsi-studio.labsolver.org,"Diﬀusion and Functional MRI Pre-processing From the minimally pre-processed HCP diﬀusion-weighted MRI data, white matter ﬁbres were reconstructed using generalized q-sampling imaging (Yeh et al., 2010) and deterministic streamline tractogra-phy was performed (Yeh et al., 2013) using DSI studio (http://dsi-studio.labsolver.org).",1,0,0
10.1016/j.neuroimage.2022.119659,mathworks.com,All post-processing of adjacency matrices was completed using (MATLAB 2018) (http://www.mathworks.com) and Python (version 3.0).,1,0,0
10.1016/j.neuroimage.2022.119659,github.com/parsonsn/sc-fc-multiplex-bandwidth,"Our full weighted and unweighted multiplex connectivity python code can be found here: https://github.com/parsonsn/SC-FC-Multiplex-Bandwidth., NeuroImage 263 (2022) 119659 Data and code availability statement Our full source code can be found at: https://github.com/parsonsn/SC-FC-Multiplex-Bandwidth.",1,0,0
10.1016/j.neuroimage.2022.119659,humanconnectome.org,"In the present study, we utilized a publicly available dataset, provided by the Human Connectome Project (HCP; http://www.humanconnectome.org) from the Washington University-University of Minnesota (WU-Minn) consortium, including 484 healthy participants from the Q4 release (500 subject; Nov 25, 2014).",0,1,0
10.1016/j.neuroimage.2022.119659,humanconnectome.org,"humanconnectome.org) from the Washington University-University of Minnesota (WUMinn) consortium, including 484 healthy participants from the Q4 release (500 subject; Nov 25, 2014; 272 females, 212 males; age = 29.15 ± 3.47)., In the present study, we utilized a publicly available dataset, provided by the Human Connectome Project (HCP; http://www.humanconnectome.org) from the Washington University-University of Minnesota (WU-Minn) consortium, including 484 healthy participants from the Q4 release (500 subject; Nov 25, 2014).",0,1,0
10.1016/j.neuroimage.2022.119117,github.com/lindehesse/fetalsubcortsegm_code,"All network training code has been made available on: https://github.com/lindehesse/FetalSubcortSegm_Code , and weights of the trained networks can be requested by email-ing the corresponding author.",1,0,0
10.1016/j.neuroimage.2022.118938,nitrc.org/projects/niistat,"Lesion-mapping analysis The NiiStat toolbox (www.nitrc.org/projects/niistat) was used to conduct lesion-mapping analyses to identify localized brain lesions within the selected ROIs that predict impaired eﬀerence copy and vo-cal sensorimotor integration mechanisms, as indexed by modulation of neurophysiological responses in aphasic speakers compared with con-trols.",1,0,0
10.1016/j.neuroimage.2022.119672,github.com/margulies/topography/tree/master/utils,"We extracted the relative spatial layout of regions along the cor-tical surface by using existing scripts (https://github.com/margulies/topography/tree/master/utils), based on an algorithm developed to approximate the exact geodesic distance from triangular meshes (Oligschläger et al., 2017).",1,0,0
10.1016/j.neuroimage.2022.119672,networkx.org,(https://networkx.org/).,0,0,1
10.1016/j.neuroimage.2022.119672,osf.io/6xm8n,"Codes and derivatives are available here: https://osf.io/6xm8n/)., The package including the atlas can be downloaded here: https://osf.io/6xm8n/.",1,0,0
10.1016/j.neuroimage.2022.119672,clinicaltrials.gov,"ANSM: 141200B-31, ClinicalTrials.gov ID: NCT02830100) (CPP: 09-CHUG-14, 04/06/2009).",0,0,1
10.1016/j.neuroimage.2022.119672,nilearn.github.io/stable/index.html,"Nilearn (https://nilearn.github.io/stable/index.html ; Abraham et al., 2014) was used to delineate ROIs and extract the signal.",1,0,0
10.1016/j.neuroimage.2022.118969,surfer.nmr.mgh.harvard.edu,"We identiﬁed the coordinates and anatomical label of all contacts in each individual brain model using pre-surgical MRI, post-surgical CT images, Freesurfer software (http://surfer.nmr.mgh.harvard.edu), and the iEEGview tool-box (Li et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119743,humanconnectome.org/study/hcp-lifespan-aging/project-protocol/imaging-protocols-hcp-aging,"Brieﬂy, T1w and T2w structural images were acquired on Siemens 3T Prisma scanners using the multi-echo magnetization-prepared rapid gradi-ent echo (MPRAGE; TE = 1.8/3.6/5.4/7.2 ms, TR/TI = 2500/1000 ms, ﬂip angle = 8°, 0.8 mm isotropic voxel size) and the vari-able ﬂip angle turbo spin-echo (T2w SPACE; TR/TE = 3200/564 ms, 0.8 mm isotropic voxel size) sequences (for the entire imaging protocol, see https://www.humanconnectome.org/study/hcp-lifespan-aging/project-protocol/imaging-protocols-hcp-aging).",1,0,0
10.1016/j.neuroimage.2022.119743,nda.nih.gov,Data availability statement Data used in the preparation of this manuscript were collected as part of the Human Connectome Project in Aging (U01AG052564 and U01AG052564-S1) and are available on the National Institute of Mental Health (NIMH) Data Archive (NDA) at https://nda.nih.gov.,0,1,0
10.1016/j.neuroimage.2022.119743,humanconnectome.org/software/workbench-command/-surface-cortex-layer,"The precision of surface sampling and the accuracy of tissue gradient characterization may also be improved by employing algorithms that compensate for cortical folding, such as the ones implemented in the Connectome Workbench (https://www.humanconnectome.org/software/workbench-command/-surface-cortex-layer).",1,0,0
10.1016/j.neuroimage.2022.119168,3tissue.github.io,"NeuroImage 254 (2022) 119168 (https://3Tissue.github.io , a fork of MRtrix3), and FSL (Jenkinson et al., 2012) software packages.",1,0,0
10.1016/j.neuroimage.2022.119352,github.com/washington-university/cifti-matlab),"With the MATLAB script (cifti toolbox: https://github.com/Washington-University/cifti-matlab), we extracted and averaged the cleaned timeseries of all the grayordinates in each region of the HCP-MMP 1.0 atlas (Glasser et al., 2016a), which is a group-based parcellation deﬁned in the HCP grayordinate standard space having 180 brain regions per hemisphere, and is a surface-based atlas provided in CIFTI format.",1,0,0
10.1016/j.neuroimage.2022.119352,nitrc.org/projects/bct,"A community analysis was performed on the eﬀective connectivity matrix for all 180 cortical regions in the left hemisphere, using the Brain Connectivity Toolbox (Rubinov and Sporns, 2010) (https://www.nitrc.org/projects/bct).",1,0,0
10.1016/j.neuroimage.2022.119352,github.com/decolab/eﬀective-connectivity–hopf,Code for the Hopf eﬀective connectiv-ity algorithm is available at https://www.github.com/decolab/Eﬀective-Connectivity–Hopf.,1,0,0
10.1016/j.neuroimage.2022.119352,github.com/washington-university/hcppipelines),"(2013) , based on the updated 7T data pipeline (v3.21.0, https://github.com/Washington-University/HCPpipelines), including gradient distortion correction, head motion correction, image distor-tion correction, spatial transformation to the Montreal Neurological Institute space using one step spline resampling from the original functional images followed by intensity normalization.",1,0,0
10.1016/j.neuroimage.2022.119352,humanconnectome.org,"Further details of the 7T rs-fMRI acquisition protocols are given in the HCP reference manual (https://humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf)., The data were from the Human Connectome Project, and the WU-Minn HCP Consortium obtained full informed consent from all participants, and research procedures and ethical guidelines were followed in accordance with the Institutional Review Boards (IRB), with details at the HCP web-site http://www.humanconnectome.org/Data and code availability The data are available at the HCP website http://www., humanconnectome.org/.",0,1,0
10.1016/j.neuroimage.2022.119352,humanconnectome.org/storage/app/media/documentation/s1200/hcp_s1200_release_reference_manual.pdf,Further details of the 7T rs-fMRI acquisition protocols are given in the HCP reference manual (https://humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf).,0,0,1
10.1016/j.neuroimage.2022.119352,humanconnectome.org/data,"The data were from the Human Connectome Project, and the WU-Minn HCP Consortium obtained full informed consent from all participants, and research procedures and ethical guidelines were followed in accordance with the Institutional Review Boards (IRB), with details at the HCP web-site http://www.humanconnectome.org/Data and code availability The data are available at the HCP website http://www.",0,1,0
10.1016/j.neuroimage.2022.119336,antsbrainextraction.sh,"Afterwards, they were skull-stripped using ANTs’ antsBrainExtraction.sh v2.1.0 (OA-3 M.",1,0,0
10.1016/j.neuroimage.2022.119336,github.com/jachtzehn/timepath/tree/wholebrain-cv/mri/rsa,Details for this analysis can be found at https://github.com/jAchtzehn/timePath/tree/wholeBrain-CV/mri/rsa.,0,0,1
10.1016/j.neuroimage.2022.119336,github.com/jachtzehn/timepath,"Details for this analysis can be found at https://github.com/jAchtzehn/timePath/tree/wholeBrain-CV/mri/rsa., Data and Code Availability Statement The datasets generated during and/or analysed during the current study are available at GitHub: https://github.com/jAchtzehn/timePath.",1,0,0
10.1016/j.neuroimage.2022.119311,en.wikibooks.org/wiki/spm/timeseries_extraction,We extracted the time series of each ROI according to the guide-line (https://en.wikibooks.org/wiki/SPM/Timeseries_extraction).,0,0,1
10.1016/j.neuroimage.2022.119311,osf.io/n2cez,Data and codes availability Data that support the ﬁndings of this study is available at OSF (https://osf.io/n2cez/).,0,1,0
10.1016/j.neuroimage.2022.119289,surfer.nmr.mgh.harvard.edu,"Preprocessing of fMRI data The functional and structural data were analyzed using FreeSurfer and FS-FAST version 7.1.0 (http://surfer.nmr.mgh.harvard.edu ; Dale et al., 1999 ; Fischl et al., 1999).",1,0,0
10.1016/j.neuroimage.2022.119289,3.2.1.2,3.2.1.2.,0,0,1
10.1016/j.neuroimage.2022.119289,aspredicted.org,"Material and Methods Preregistration of the study’s design and methods was done be-fore the start of data collection on aspredicted.org (https://www., aspredicted.org/36jq7.pdf).",0,0,1
10.1016/j.neuroimage.2022.119289,3.2.1.1,Regions of interest analysis 3.2.1.1.,0,0,1
10.1016/j.neuroimage.2022.119289,aspredicted.org/36jq7.pdf,aspredicted.org/36jq7.pdf).,0,0,1
10.1016/j.neuroimage.2022.119289,3.2.1.3,3.2.1.3.,0,0,1
10.1016/j.neuroimage.2022.119153,pstnet.com/eprime,Stimulus presentation and data collection were in E-Prime 2.0 Professional (www.pstnet.com/eprime).,1,0,0
10.1016/j.neuroimage.2022.119153,ﬁl.ion.ucl.ac.uk/spm,"EPIs from each participant’s two runs were pre-processed and an-alyzed using SPM8 (www.ﬁl.ion.ucl.ac.uk/spm), facilitated by a cus-tom suite of scripts for fMRI analysis (https://github.com/ddwagner/SPM8w).",1,0,0
10.1016/j.neuroimage.2022.119153,alivelearn.net/xjview,The clusters were visual-ized using xjView toolbox (http://www.alivelearn.net/xjview).,1,0,0
10.1016/j.neuroimage.2022.119153,osf.io/pydbk,Aggregated data and analysis scripts that support the ﬁndings have been deposited in OSF at https://osf.io/pydbk/. 12 T.P.,1,0,0
10.1016/j.neuroimage.2022.119153,jakewestfall.shinyapps.io/pangea/(westfall,"We conducted a power analysis for the ROI analy-ses using a linear mixed eﬀects model with the PANGEA package (v0.2), publicly available at https://jakewestfall.shinyapps.io/pangea/(Westfall, 2015).",1,0,0
10.1016/j.neuroimage.2022.119153,github.com/ddwagner/spm8w,"EPIs from each participant’s two runs were pre-processed and an-alyzed using SPM8 (www.ﬁl.ion.ucl.ac.uk/spm), facilitated by a cus-tom suite of scripts for fMRI analysis (https://github.com/ddwagner/SPM8w).",1,0,0
10.1016/j.neuroimage.2022.119084,quspin.com,"This is because as ﬁeld is increased, the lin-earity of the OPM response to ﬁeld is lost 1 ; a change in background ﬁeld of ∼3.5 nT would be equivalent to a gain error of 5% (www.quspin.com), raising to 10% for a ﬁeld change of 5 nT.",0,0,1
10.1016/j.neuroimage.2022.119240,cni.stanford.edu,"MRI data acquisition and processing MRI data were acquired on a 3T Discovery MR750 scanner (Gen-eral Electric Healthcare, Milwaukee, WI, USA) equipped with a 32-channel head coil (Nova Medical, Wilmington, MA, USA) at the Cen-ter for Cognitive and Neurobiological Imaging at Stanford Univer-sity (www.cni.stanford.edu).",0,1,0
10.1016/j.neuroimage.2022.119240,github.com/mezera/mrq,"Quantitative T1 (relaxation time, seconds) maps were calculated us-ing mrQ , (https://github.com/mezera/mrQ), an open-source software package implemented in MATLAB R2012a (Mathworks, Natick, MA).",1,0,0
10.1016/j.neuroimage.2022.119240,github.com/vistalab/vistasoft/mrdiﬀusion,"Diﬀusion weighted images were pre-processed with Vistasoft (http://github.com/vistalab/vistasoft/mrDiﬀusion) , an open-source software package implemented in MATLAB R2012a (Mathworks, Natick, MA).",1,0,0
10.1016/j.neuroimage.2022.119240,ﬁl.ion.ucl.ac.uk/spm,"Each diﬀusion weighted image was registered to the mean of the b = 0 images and the mean b = 0 image was registered automatically to the participant’s T1w image, using a rigid body transformation (imple-mented in SPM8, http://www.ﬁl.ion.ucl.ac.uk/spm/; no warping was applied).",1,0,0
10.1016/j.neuroimage.2021.118767,langneurosci.mc.vanderbilt.edu/resources.html,"To assess correlations between behavior and lesion pattern the voxel-based lesion symptom software ‘vlsm2’ developed by Stephen Wil-son was used (https://langneurosci.mc.vanderbilt.edu/resources.html, Bates et al., 2003), adaptation in communication with S.",1,0,0
10.1016/j.neuroimage.2021.118767,ﬁl.ion.ucl.ac.uk/spm,For normalization and transfor-mation of the lesion masks into standard stereotactic space (MNI) the ‘clinical toolbox’ (www.nitrc.org/projects/clinicaltbx/) in SPM8 (ﬁl.ion.ucl.ac.uk/spm) was used.,1,0,0
10.1016/j.neuroimage.2021.118767,nitrc.org/projects/clinicaltbx,For normalization and transfor-mation of the lesion masks into standard stereotactic space (MNI) the ‘clinical toolbox’ (www.nitrc.org/projects/clinicaltbx/) in SPM8 (ﬁl.ion.ucl.ac.uk/spm) was used.,1,0,0
10.1016/j.neuroimage.2022.118976,biorender.com,The graphical repre-sentation of the models in Fig. 1 was created with BioRender.com.,1,0,0
10.1016/j.neuroimage.2022.118976,neuromorpho.org,"Ten reconstructions were obtained from NeuroMorpho.org (Ascoli et al., 2007) under the Allen Brain Atlas (Koch and Jones, 2016) in swc format (connected nodes with associated radii)., Using the full expression for the sphere diﬀusivity (Balinov et al., 1993), and a gamma distribution for the soma radius with mean 7.1 μm and standard deviation 3.6 μm matching those of rat pyramidal cells from NeuroMorpho.org (Palombo et al., 2021) gives a rough estimate of the expected MR radius of 11.1 μm –a considerable overestimation.",0,1,0
10.1016/j.neuroimage.2022.119396,adni.loni.usc.edu,"Silva-Rodríguez). 1 FJLG and ACS equally contributed to this work and share the ﬁrst authorship. 2 Data used in the preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wpcontent/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf https://doi.org/10.1016/j.neuroimage.2022.119396., To achieve this, we analysed FTP PET data from a large sample of 309 HC A 𝛽-subjects from the Alzheimer’s Disease Neuroimaging Ini-tiative (ADNI, adni.loni.usc.edu) and the Harvard Aging Brain Study (HABS, habs.mgh.harvard.edu) together with a large set of synthetic images generated using a well-controlled Monte Carlo (MC) simulation framework (Paredes-Pacheco et al., 2021)., For this work, we used images in pre-processing levels two and four as described by ADNI (http://adni.loni.usc.edu/methods/pet-analysis-method/pet-analysis/), which corresponds to co-registered and averaged images (level two), further reoriented to a standard image matrix and smoothed to 8 mm isotropic resolution (level four)., Each series in each exam underwent quality control following the steps described in detail in ADNI’s MRI pro-tocol (http://adni.loni.usc.edu/methods/mri-tool/mri-analysis/).",0,0,1
10.1016/j.neuroimage.2022.119396,habs.mgh.harvard.edu,"To achieve this, we analysed FTP PET data from a large sample of 309 HC A 𝛽-subjects from the Alzheimer’s Disease Neuroimaging Ini-tiative (ADNI, adni.loni.usc.edu) and the Harvard Aging Brain Study (HABS, habs.mgh.harvard.edu) together with a large set of synthetic images generated using a well-controlled Monte Carlo (MC) simulation framework (Paredes-Pacheco et al., 2021).",0,1,0
10.1016/j.neuroimage.2022.119396,surfer.nmr.mgh.harvard.edu,"This code takes as inputs the co-registered tau PET and MRI im-ages, and the segmentations from both FreeSurfer (FreeSurfer version 6, http://surfer.nmr.mgh.harvard.edu/) and SPM (Statistical Paramet-ric Mapping version 12, http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119396,adni.loni.usc.edu/methods/pet-analysis-method/pet-analysis,"For this work, we used images in pre-processing levels two and four as described by ADNI (http://adni.loni.usc.edu/methods/pet-analysis-method/pet-analysis/), which corresponds to co-registered and averaged images (level two), further reoriented to a standard image matrix and smoothed to 8 mm isotropic resolution (level four).",0,1,0
10.1016/j.neuroimage.2022.119396,adni.loni.usc.edu/methods/mri-tool/mri-analysis,Each series in each exam underwent quality control following the steps described in detail in ADNI’s MRI pro-tocol (http://adni.loni.usc.edu/methods/mri-tool/mri-analysis/).,0,0,1
10.1016/j.neuroimage.2022.119396,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119396,ﬁl.ion.ucl.ac.uk/spm,"This code takes as inputs the co-registered tau PET and MRI im-ages, and the segmentations from both FreeSurfer (FreeSurfer version 6, http://surfer.nmr.mgh.harvard.edu/) and SPM (Statistical Paramet-ric Mapping version 12, http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119396,adni.loni.usc.edu/wpcontent/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wpcontent/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf https://doi.org/10.1016/j.neuroimage.2022.119396.,0,0,1
10.1016/j.neuroimage.2022.119662,osf.io/zaef2,"Data availability The structured summary data in tabular form and scripts used to gen-erate the results reported here are openly available and can be down-loaded from https://osf.io/zaef2/., Data and code availability The structured summary data in tabular form and the scripts used to generate the results reported here are openly available and can be downloaded from https://osf.io/zaef2/.",0,1,0
10.1016/j.neuroimage.2022.119662,surfer.nmr.mgh.harvard.edu,"Diﬀusion-weighted MRI data preprocessing Prior to preprocessing, the high-resolution, T1-weighted images were anatomically segmented with Freesurfer (v.6.0; Fischl, 2012) (http://surfer.nmr.mgh.harvard.edu/).",1,0,0
10.1016/j.neuroimage.2022.119662,github.com/brainspaces/glasser360/blob/master/glasser360mni.nii.gz,"To create the ROIs for the present study, the selected multimodal areas in MNI space were ﬁrst extracted from the original map of 360 areas  https://github.com/brainspaces/glasser360/blob/master/glasser360MNI.nii.gz  and bina-rized (as all voxels had as intensity value the area number).",0,1,0
10.1016/j.neuroimage.2022.119662,r-project.org,"4.1.0; R Core Team 2022 ; R Studio Team, 2022 ; https://www.R-project.org/).",1,0,0
10.1016/j.neuroimage.2022.119662,lavaan.ugent.be,"0.6-9; Rosseel, 2012) (https://lavaan.ugent.be/) in R.",0,0,1
10.1016/j.neuroimage.2022.119662,surfer.nmr.mgh.harvard.edu/fswiki/tracula,"Prepro-cessing of diﬀusion-weighted MRI data was done using the TRActs Constrained by UnderLying Anatomy (TRAC-ULA; https://surfer.nmr.mgh.harvard.edu/fswiki/Tracula) tool (Yendiki et al., 2011) in Freesurfer.",1,0,0
10.1016/j.neuroimage.2022.119662,fsl.fmrib.ox.ac.uk/fsl/fslwiki/feat,"Brieﬂy, preprocess-ing was conducted using FSL FEAT (Woolrich et al., 2001) (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FEAT), and included removal of the ﬁrst ﬁve volumes, head motion correction (Jenkinson et al., 2002), removal of non-brain structures (Smith, 2002), susceptibility-derived distortion correction (Smith et al., 2004), co-registration to the anatom-ical image and, subsequently, to the MNI 2-mm standard space, spatial smoothing (4-mm full width at half maximum), and 4D grand-mean scaling.",1,0,0
10.1016/j.neuroimage.2022.119662,fsl.fmrib.ox.ac.uk,"Preprocessing included eddy-current distortion correction based on FMRIB Software Library (FSL) tools (v.5.0.8; Jenkinson et al., 2012) (https://fsl.fmrib.ox.ac.uk) and head motion correction by registering the diﬀusion-weighted to the b = 0 images., Brieﬂy, preprocess-ing was conducted using FSL FEAT (Woolrich et al., 2001) (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FEAT), and included removal of the ﬁrst ﬁve volumes, head motion correction (Jenkinson et al., 2002), removal of non-brain structures (Smith, 2002), susceptibility-derived distortion correction (Smith et al., 2004), co-registration to the anatom-ical image and, subsequently, to the MNI 2-mm standard space, spatial smoothing (4-mm full width at half maximum), and 4D grand-mean scaling.",1,0,0
10.1016/j.neuroimage.2022.119087,brain-connectivity-toolbox.net,"Then, temporal community structure was detected using the Lou-vain algorithm (Blondel et al., 2008) from the Brain Connectivity Tool-box (BCT, http://www.brain-connectivity-toolbox.net/) (Rubinov and Sporns, 2010)., The Brain Connectiv-ity Toolbox code used for graph-theoretical analyses is freely available online (http://www.brain-connectivity-toolbox.net/).",1,0,0
10.1016/j.neuroimage.2022.119087,github.com/thomasyeolab/cbig/tree/master/stable_projects/brain_parcellation/yeo2011_fcmri_clustering,The 51 resting-state ROIs (derived from the Yeo2011 resting-state networks) can be found here at https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/brain_parcellation/Yeo2011_fcMRI_clustering.,0,0,1
10.1016/j.neuroimage.2022.119087,humanconnectome.org,Details of the data collection can be found on the HCP website (http://www.humanconnectome.org/).,0,0,1
10.1016/j.neuroimage.2022.119087,github.com/macshine/integration,"(2016) at https://github.com/macshine/integration/)., Code for the “cartographic proﬁle ”i s freely available online (https://github.com/macshine/integration/).",1,0,0
10.1016/j.neuroimage.2022.119535,humanconnectomeproject.org/and,"The data used in this study is openly available via http://www.humanconnectomeproject.org/and (Tian et al., 2022).",0,1,0
10.1016/j.neuroimage.2022.119535,git.fmrib.ox.ac.uk/amyh/noddi-axialdiﬀusivity,"At https://git.fmrib.ox.ac.uk/amyh/noddi-axialdiﬀusivity we provide a cuDIMOT implementation of NODDI (Hernandez-Fernandez et al., 2019) where the assumed diﬀusivities 𝑑 ∥and 𝑑 𝑖𝑠𝑜 are user-deﬁned at runtime, and MATLAB scripts to implement the modiﬁed NODDI model.",1,0,0
10.1016/j.neuroimage.2022.118873,headit.ucsd.edu,"The data are available at HeadIT website (http://headit.ucsd.edu , Imagined Emotion with Continuous Data under Studies).",0,1,0
10.1016/j.neuroimage.2022.118873,github.com/japalmer29/amica,"AMICA code is available at https://github.com/japalmer29/amica and as an open-source plug-in for EEGLAB (Delorme and Makeig, 2004).",1,0,0
10.1016/j.neuroimage.2022.118873,bioimagesuiteweb.github.io/webapp,Areas of signiﬁcant diﬀerences are labeled with their Broad-mann areas (BA) deﬁned in the MNI coordinates using the MNI to Ta-lairach mapping application in the Yale BioImage Suite (BIS) package https://bioimagesuiteweb.github.io/webapp/.,1,0,0
10.1016/j.neuroimage.2022.118873,github.com/sccn/iclabel,ICLabel code and a detailed tutorial can be found in its github repository https://github.com/sccn/ICLabel.,1,0,0
10.1016/j.neuroimage.2022.118973,ebrains.eu,"NeuroImage 251 (2022) 118973 a r t i c l e i n f o Keywords: Brain modelling Cloud Connectome Neuroimaging Network model High performance computing Reproducibility Data protection a b s t r a c t The Virtual Brain (TVB) is now available as open-source services on the cloud research platform EBRAINS (ebrains.eu)., Introduction This paper introduces cloud services for brain simulation that are now being oﬀered on the open brain research platform EBRAINS (eu-ropean brain research infrastructures; ebrains.eu), which makes scien-tiﬁc data, tools, and results accessible to everyone within a protected environment that promotes reproducible work., Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce., To use TVB cloud services a user must therefore agree to terms that clarify its personal responsibility regarding compliance with GDPR with respect to security precautions, access permissions, contact persons, personal re-sponsibilities, monitoring, logging, and passing of information to third parties (ebrains.eu/terms)., Data availability The datasets generated and analysed during the current study are available in the EBRAINS KnowledgeGraph repository, search.kg.ebrains.eu., The EBRAINS database service KnowledgeGraph (search.kg.ebrains.eu) is a DOI-minting repos-itory where all data and software services are indexed and archived., Source codes are deployed as cloud services that can be used on ebrains.eu and as standalone download versions that can be pulled as container images from Docker Hub (Table 1).",0,0,1
10.1016/j.neuroimage.2022.118973,ebrains.eu/terms,"To use TVB cloud services a user must therefore agree to terms that clarify its personal responsibility regarding compliance with GDPR with respect to security precautions, access permissions, contact persons, personal re-sponsibilities, monitoring, logging, and passing of information to third parties (ebrains.eu/terms).",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/mouse-stroke-brain-network-model/tvb-ready,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,docs.thevirtualbrain.org,"TVB’s main documentation is hosted at docs.thevirtualbrain.org., The Virtual Brain The methods behind the main TVB neuroinformatics simulator are extensively described in several publications (Ritter et al., 2013 ; Sanz-Leon et al., 2015 , 2013) and in online documentation (Table 1 ; docs.thevirtualbrain.org).",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/ins-amu/bvep,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com,"An additional beneﬁt of TVB on EBRAINS workﬂows is that mechanisms for data management, provenance tracking and reproducible research are directly embedded using DataLad (Halchenko et al., 2021), which en-ables explicit tracking of all inputs, codes and processing steps that pro-duced a result in a manner similar to how GitHub (github.com) is used for source code management., Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce., Notation Description access control (computer security) selective restriction to consume, enter or use a resource annotation of data categorization and labelling of data authentication (computer security) verifying the identity of a computer system user authorisation (computer security) specifying access rights and privileges to resources; access control rules are used to decide whether access requests from (authenticated) users shall be granted or not API; application programming interface interface that connects computers or software BIDS Brain Imaging Data Structure; a standard for organizing neuroscience data brain network model system of coupled diﬀerential equations for simulating brain activity checksum a small block of data that contains information about the contents of another block of data for the purpose of detecting errors cloud computing on-demand availability of computing power and storage over the internet cloud service infrastructure, platforms, or software made available through the internet container image, containerization (software) (creating) executable packages of software that include all dependencies needed to run an application reliably in diﬀerent computing environments controlled vocabulary carefully selected list of words and phrases for unambiguous tagging of units of information curation organization and integration of data collected from various sources data sharing agreement legal contracts that detail what data are being shared and the appropriate use for the data diﬀerential equation equation that relates functions and their derivatives (rate at which the value of a function changes with respect to a change of its argument) EBRAINS European Brain Research INfrastructureS encryption converting information into secret code that hides the information’s true meaning functional connectivity statistical relationships between brain signals represented as a network; often a matrix of pairwise correlation coeﬃcients between region-average fMRI signals General Data Protection Regulation a regulation in European Union law on data protection and privacy with the aim to increase individual’s control and rights over their personal data GUI graphical user interface Jupyter notebooks open-source web application to create and share documents that contain live code, equations, visualizations and narrative text JupyterLab web-based interactive development environment for Jupyter notebooks key (computer security) a piece of information, which, when processed through a cryptographic algorithm, can encode or decode cryptographic data knowledge graph a data model and database for linking, integrating, and storing information in a graph structure licensing (software) providing a software product with a legal statement (license) that governs its use and redistribution metadata data that provides information (annotations) about other data metadata schema a deﬁnition how metadata is structured MRI magnetic resonance imaging neuromorphic systems electronic analogue circuits to mimic neuro-biological architectures ontology (information science) a way to organize data, information, knowledge by deﬁning concepts, categories and their relationships openMINDS speciﬁcations for structuring metadata in neuroscience (github.com/HumanBrainProject/openMINDS) persistent identiﬁers a long-lasting reference to an (often digital) object (e.g., document, ﬁle, web page); one example are digital object identiﬁers (DOI, doi.org), which are widely used to identify publications and data sets public-key cryptography a system that uses a diﬀerent key for decryption than for encryption; this has the advantage that the decryption key needs not to be communicated via insecure channels, while the key for encryption can be known by everyone (""public"") without compromising safety RESTful API an architectural style for APIs where resources are provided in a textual representation that can be read and modiﬁed with a predeﬁned set of operations sandbox (computer security) security mechanism for separating running programs in an eﬀort to protect computing systems from failure or attacks, often used to run untrusted programs and code structural connectivity aggregated descriptions of the networks that couple neurons, neural populations and brain areas supercomputer a computer that is shared by many users and that provides a high level of performance regarding processor time, memory and storage space TVB The Virtual Brain, a software to simulate brain network models UNICORE interface for exchanging data and commands between diﬀerent computers in a network (unicore.eu) versioning (software) assigning unique version names or unique version numbers to unique states of computer software version control tracking and managing changes to software code or data sets virtual robots computer simulation of a physical robot face triangulations, projection matrices for predicting EEG, and brain parcellations., With DataLad all data and code ﬁles are version-controlled and managed in a manner that is comparable to how software is managed with GitHub (github.com), allowing to cap-ture complex hierarchical project structures and all computational steps from raw data to ﬁnal ﬁgures.",1,0,0
10.1016/j.neuroimage.2022.118973,tvb-nest.apps.hbp.eu,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/humanbrainproject/openminds,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce., Notation Description access control (computer security) selective restriction to consume, enter or use a resource annotation of data categorization and labelling of data authentication (computer security) verifying the identity of a computer system user authorisation (computer security) specifying access rights and privileges to resources; access control rules are used to decide whether access requests from (authenticated) users shall be granted or not API; application programming interface interface that connects computers or software BIDS Brain Imaging Data Structure; a standard for organizing neuroscience data brain network model system of coupled diﬀerential equations for simulating brain activity checksum a small block of data that contains information about the contents of another block of data for the purpose of detecting errors cloud computing on-demand availability of computing power and storage over the internet cloud service infrastructure, platforms, or software made available through the internet container image, containerization (software) (creating) executable packages of software that include all dependencies needed to run an application reliably in diﬀerent computing environments controlled vocabulary carefully selected list of words and phrases for unambiguous tagging of units of information curation organization and integration of data collected from various sources data sharing agreement legal contracts that detail what data are being shared and the appropriate use for the data diﬀerential equation equation that relates functions and their derivatives (rate at which the value of a function changes with respect to a change of its argument) EBRAINS European Brain Research INfrastructureS encryption converting information into secret code that hides the information’s true meaning functional connectivity statistical relationships between brain signals represented as a network; often a matrix of pairwise correlation coeﬃcients between region-average fMRI signals General Data Protection Regulation a regulation in European Union law on data protection and privacy with the aim to increase individual’s control and rights over their personal data GUI graphical user interface Jupyter notebooks open-source web application to create and share documents that contain live code, equations, visualizations and narrative text JupyterLab web-based interactive development environment for Jupyter notebooks key (computer security) a piece of information, which, when processed through a cryptographic algorithm, can encode or decode cryptographic data knowledge graph a data model and database for linking, integrating, and storing information in a graph structure licensing (software) providing a software product with a legal statement (license) that governs its use and redistribution metadata data that provides information (annotations) about other data metadata schema a deﬁnition how metadata is structured MRI magnetic resonance imaging neuromorphic systems electronic analogue circuits to mimic neuro-biological architectures ontology (information science) a way to organize data, information, knowledge by deﬁning concepts, categories and their relationships openMINDS speciﬁcations for structuring metadata in neuroscience (github.com/HumanBrainProject/openMINDS) persistent identiﬁers a long-lasting reference to an (often digital) object (e.g., document, ﬁle, web page); one example are digital object identiﬁers (DOI, doi.org), which are widely used to identify publications and data sets public-key cryptography a system that uses a diﬀerent key for decryption than for encryption; this has the advantage that the decryption key needs not to be communicated via insecure channels, while the key for encryption can be known by everyone (""public"") without compromising safety RESTful API an architectural style for APIs where resources are provided in a textual representation that can be read and modiﬁed with a predeﬁned set of operations sandbox (computer security) security mechanism for separating running programs in an eﬀort to protect computing systems from failure or attacks, often used to run untrusted programs and code structural connectivity aggregated descriptions of the networks that couple neurons, neural populations and brain areas supercomputer a computer that is shared by many users and that provides a high level of performance regarding processor time, memory and storage space TVB The Virtual Brain, a software to simulate brain network models UNICORE interface for exchanging data and commands between diﬀerent computers in a network (unicore.eu) versioning (software) assigning unique version names or unique version numbers to unique states of computer software version control tracking and managing changes to software code or data sets virtual robots computer simulation of a physical robot face triangulations, projection matrices for predicting EEG, and brain parcellations.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/the-virtual-brain-multiscale,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/bayesian-virtual-epileptic-patient,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/brainmodes/tvb-pipeline,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/fzj-inm1-bda/siibra-api,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/tvb-mouse-brains,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,virtualbraincloud-2020.eu,"TVB cloud services (Tables 1 , 2) were developed by the Human Brain Project subproject ""The Virtual Brain"" in collaboration with the two Human Brain Project partnering projects TVB-Cloud (virtualbraincloud-2020.eu) and TVB-CD (bit.ly/3ogLYtb).",1,0,0
10.1016/j.neuroimage.2022.118973,search.kg.ebrains.eu,"Data availability The datasets generated and analysed during the current study are available in the EBRAINS KnowledgeGraph repository, search.kg.ebrains.eu., The EBRAINS database service KnowledgeGraph (search.kg.ebrains.eu) is a DOI-minting repos-itory where all data and software services are indexed and archived.",0,1,0
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/fast-tvb,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/multiscale-cosim/tvb-nest,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,brainsimulation.org/atlasweb_multiscale,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/the-virtual-brain/tvb-root,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,gauss-centre.eu,(www.gauss-centre.eu) for supporting this project by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercom-puter JUWELS at Jülich Supercomputing Centre (JSC).,0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/user-story-tvb,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,hub.docker.com/r/thevirtualbrain/tvb-nest,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/fzj-inm1-bda/siibra-python,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,datalad.org,"The pipeline uses DataLad (datalad.org) to make its workﬂow reproducible in an actionable manner: all software and data are tracked in a way that the entire workﬂow or just individual steps can be easily re-run, archived, published and shared.",1,0,0
10.1016/j.neuroimage.2022.118973,hub.docker.com/r/thevirtualbrain/tvb_converter,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/openminds-metadata-for-tvb-ready-data,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,thevirtualbrain.apps.hbp.eu,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,unicore.eu,"Notation Description access control (computer security) selective restriction to consume, enter or use a resource annotation of data categorization and labelling of data authentication (computer security) verifying the identity of a computer system user authorisation (computer security) specifying access rights and privileges to resources; access control rules are used to decide whether access requests from (authenticated) users shall be granted or not API; application programming interface interface that connects computers or software BIDS Brain Imaging Data Structure; a standard for organizing neuroscience data brain network model system of coupled diﬀerential equations for simulating brain activity checksum a small block of data that contains information about the contents of another block of data for the purpose of detecting errors cloud computing on-demand availability of computing power and storage over the internet cloud service infrastructure, platforms, or software made available through the internet container image, containerization (software) (creating) executable packages of software that include all dependencies needed to run an application reliably in diﬀerent computing environments controlled vocabulary carefully selected list of words and phrases for unambiguous tagging of units of information curation organization and integration of data collected from various sources data sharing agreement legal contracts that detail what data are being shared and the appropriate use for the data diﬀerential equation equation that relates functions and their derivatives (rate at which the value of a function changes with respect to a change of its argument) EBRAINS European Brain Research INfrastructureS encryption converting information into secret code that hides the information’s true meaning functional connectivity statistical relationships between brain signals represented as a network; often a matrix of pairwise correlation coeﬃcients between region-average fMRI signals General Data Protection Regulation a regulation in European Union law on data protection and privacy with the aim to increase individual’s control and rights over their personal data GUI graphical user interface Jupyter notebooks open-source web application to create and share documents that contain live code, equations, visualizations and narrative text JupyterLab web-based interactive development environment for Jupyter notebooks key (computer security) a piece of information, which, when processed through a cryptographic algorithm, can encode or decode cryptographic data knowledge graph a data model and database for linking, integrating, and storing information in a graph structure licensing (software) providing a software product with a legal statement (license) that governs its use and redistribution metadata data that provides information (annotations) about other data metadata schema a deﬁnition how metadata is structured MRI magnetic resonance imaging neuromorphic systems electronic analogue circuits to mimic neuro-biological architectures ontology (information science) a way to organize data, information, knowledge by deﬁning concepts, categories and their relationships openMINDS speciﬁcations for structuring metadata in neuroscience (github.com/HumanBrainProject/openMINDS) persistent identiﬁers a long-lasting reference to an (often digital) object (e.g., document, ﬁle, web page); one example are digital object identiﬁers (DOI, doi.org), which are widely used to identify publications and data sets public-key cryptography a system that uses a diﬀerent key for decryption than for encryption; this has the advantage that the decryption key needs not to be communicated via insecure channels, while the key for encryption can be known by everyone (""public"") without compromising safety RESTful API an architectural style for APIs where resources are provided in a textual representation that can be read and modiﬁed with a predeﬁned set of operations sandbox (computer security) security mechanism for separating running programs in an eﬀort to protect computing systems from failure or attacks, often used to run untrusted programs and code structural connectivity aggregated descriptions of the networks that couple neurons, neural populations and brain areas supercomputer a computer that is shared by many users and that provides a high level of performance regarding processor time, memory and storage space TVB The Virtual Brain, a software to simulate brain network models UNICORE interface for exchanging data and commands between diﬀerent computers in a network (unicore.eu) versioning (software) assigning unique version names or unique version numbers to unique states of computer software version control tracking and managing changes to software code or data sets virtual robots computer simulation of a physical robot face triangulations, projection matrices for predicting EEG, and brain parcellations.",0,1,0
10.1016/j.neuroimage.2022.118973,bit.ly/3oglytb,"TVB cloud services (Tables 1 , 2) were developed by the Human Brain Project subproject ""The Virtual Brain"" in collaboration with the two Human Brain Project partnering projects TVB-Cloud (virtualbraincloud-2020.eu) and TVB-CD (bit.ly/3ogLYtb).",1,0,0
10.1016/j.neuroimage.2022.118973,hub.docker.com/r/thevirtualbrain/fast_tvb,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,thevirtualbrain.org,"The Virtual Brain TVB (thevirtualbrain.org) is an open-source software for simulating and analysing brain network models, which describe the brain as a graph that is composed of nodes that represent brain areas and edges that rep-resent physical connections between these areas (Supplementary Note: Brain simulation with TVB) (Ritter et al., 2013 ; Sanz-Leon et al., 2013)., TVB’s main documentation is hosted at docs.thevirtualbrain.org., The Virtual Brain The methods behind the main TVB neuroinformatics simulator are extensively described in several publications (Ritter et al., 2013 ; Sanz-Leon et al., 2015 , 2013) and in online documentation (Table 1 ; docs.thevirtualbrain.org).",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/sga3-d1–1-showcase-1,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/tvb-pipeline,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/co-simulation-tvb-and-nest-high-computer,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,fenix-ri.eu,"To provide supercomputing resources, the Human Brain Project oﬀers as part of the Interactive Computing E-Infrastructure project access to compute and storage resources of the Fenix infrastructure (fenix-ri.eu), a network of six European supercomputing centres.",0,0,1
10.1016/j.neuroimage.2022.118973,hub.docker.com/r/thevirtualbrain/tvb-run,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/the-virtual-brain,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/brainmodes/fast_tvb,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,kg.ebrains.eu/search/instances/dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,"zenodo.org/record/4,263,723#.yyrpgl1bzxg","Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,github.com/the-virtual-brain/tvb-multiscale,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,training.incf.org/collection/virtual-brain-simulation-platform,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,wiki.ebrains.eu/bin/view/collabs/rateml-tvb/source,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.118973,pypi.org/project/tvb-library,"Service URLs The Virtual Brain Web-App Brain network simulation thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain End-to-end use case wiki.ebrains.eu/bin/view/Collabs/user-story-tvb Source code github.com/the-virtual-brain/tvb-root Python library pypi.org/project/tvb-library Container image hub.docker.com/r/thevirtualbrain/tvb-run Demo brain network model data zenodo.org/record/4,263,723#.YYRPgL1Bzxg TVB Image Processing Pipeline Web-App Connectome analysis thevirtualbrain.apps.hbp.eu Collab wiki.ebrains.eu/bin/view/Collabs/tvb-pipeline Source code github.com/BrainModes/tvb-pipeline Container images hub.docker.com/r/thevirtualbrain/tvb_converter Multiscale Co-Simulation Web-App (TVB-Multiscale) Two toolboxes for concurrent simulation of large-scale and spiking networks tvb-nest.apps.hbp.eu Collab (TVB-Multiscale) wiki.ebrains.eu/bin/view/Collabs/the-virtual-brain-multiscale Collab (Parallel CoSimulation) wiki.ebrains.eu/bin/view/Collabs/co-simulation-tvb-and-nest-high-computer Source code (TVB-Multiscale) github.com/the-virtual-brain/tvb-multiscale Source code (Parallel CoSimulation) github.com/multiscale-cosim/TVB-NEST Container image (TVB-Multiscale) hub.docker.com/r/thevirtualbrain/tvb-nest TVB-HPC Collab Automatic code generation wiki.ebrains.eu/bin/view/Collabs/rateml-tvb/Source code github.com/the-virtual-brain/tvb-root Fast_TVB Collab Parallelized simulation (multithreading) wiki.ebrains.eu/bin/view/Collabs/fast-tvb Source code github.com/BrainModes/fast_tvb Container image hub.docker.com/r/thevirtualbrain/fast_tvb Bayesian Virtual Epileptic Patient Collab Epilepsy modelling wiki.ebrains.eu/bin/view/Collabs/bayesian-virtual-epileptic-patient Source code github.com/ins-amu/BVEP TVB Mouse Brains Collabs Mouse brain simulation wiki.ebrains.eu/bin/view/Collabs/tvb-mouse-brains wiki.ebrains.eu/bin/view/Collabs/mouse-stroke-brain-network-model/TVB-ready dataset kg.ebrains.eu/search/instances/Dataset/a696ccc7-e742–4301–8b43-d6814f3e5a44 SC, FC, and fMRI from tumour patients and controls openMINDS metadata for TVB-ready data Collab Metadata in JSON-LD format wiki.ebrains.eu/bin/view/Collabs/openminds-metadata-for-tvb-ready-data openMINDS schema github.com/HumanBrainProject/openMINDS TVB atlas adapter Collab Brain atlas wiki.ebrains.eu/bin/view/Collabs/sga3-d1–1-showcase-1 Source code github.com/FZJ-INM1-BDA/siibra-python; github.com/FZJ-INM1-BDA/siibra-api Visualizer brainsimulation.org/atlasweb_multiscale INCF TVB training space training.incf.org/collection/virtual-brain-simulation-platform Education and training and adapt it for another problem, without necessarily needing domain knowledge about the used software, which helps to make workﬂows and results more robust and easier to review and reproduce.",0,0,1
10.1016/j.neuroimage.2022.119594,sdmproject.com,They then created a brain map of the eﬀect size of the diﬀerence between the two conditions for each study using AES-SDM software (www.sdmproject.com/) and with these maps conducted a voxel-wise random-eﬀects meta-analysis with weighting for sample size and variance.,1,0,0
10.1016/j.neuroimage.2022.119594,github.com/canlab,"Code and data availability Data were analyzed using CANlab neuroimaging analysis tools available at https://github.com/canlab/, analysis code speciﬁc for this publication available from https://github.com/s-kline/aversive-appetitive-conditioning.",1,0,0
10.1016/j.neuroimage.2022.119594,canlab.github.io,"For this study, we additionally created group level con-trast maps using paired t-tests on CSF-scaled and winsorized appCS + and appCS-maps with custom code available from the authors’ website (https://canlab.github.io ; CANlab, code used for this publication available from https://github.com/s-kline/aversive-appetitive-conditioning)., Finally, to judge how well activation data can be distinguished be-tween appCS + and appCS-condition without the aversive pattern, we performed multivariate predictive modeling analyses on the appetitive data only using custom code (https://canlab.github.io ; CANlab, 2020 , https://github.com/s-kline/aversive-appetitive-conditioning)., Similarity analysis We followed the same analysis steps in each sample, using cus-tom code available from the authors’ website (https://canlab.github.io ; CANlab; code used for this publication available from https://github., 0.197 0.053 3.74 <.001 0.61 by music and ﬁlms (Kragel and LaBar, 2015) available from the authors’ website (https://canlab.github.io ; CANlab)., Us-ing an existing software toolbox (https://canlab.github.io ; CANlab), and an openly available meta-analysis (Fullana et al., 2016), we could eﬃ-9 S.",1,0,0
10.1016/j.neuroimage.2022.119594,github.com/s-kline/aversive-appetitive-conditioning,"For this study, we additionally created group level con-trast maps using paired t-tests on CSF-scaled and winsorized appCS + and appCS-maps with custom code available from the authors’ website (https://canlab.github.io ; CANlab, code used for this publication available from https://github.com/s-kline/aversive-appetitive-conditioning)., Finally, to judge how well activation data can be distinguished be-tween appCS + and appCS-condition without the aversive pattern, we performed multivariate predictive modeling analyses on the appetitive data only using custom code (https://canlab.github.io ; CANlab, 2020 , https://github.com/s-kline/aversive-appetitive-conditioning)., Code and data availability Data were analyzed using CANlab neuroimaging analysis tools available at https://github.com/canlab/, analysis code speciﬁc for this publication available from https://github.com/s-kline/aversive-appetitive-conditioning.",1,0,0
10.1016/j.neuroimage.2022.119265,osf.io/hadz3,"Statement of data processing In response to the Standard Reviewer Disclosure Request (2014) en-dorsed by the Center for Open Science (http://osf.io/hadz3), the authors conﬁrm that in this manuscript, they have reported all measures, con-ditions, data exclusions, and how they determined their sample sizes.",0,0,1
10.1016/j.neuroimage.2022.119265,ﬁl.ion.ucl.ac.uk/spm,"Image data analysis was conducted using SPM12 (Version 6225; Wellcome Department of Imaging Neuroscience, University College Lon-don; http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119265,osf.io/c8sxh,Data and code availability Behavioral datasets and codes are available on the OSF repository (https://osf.io/c8sxh/).,0,1,0
10.1016/j.neuroimage.2022.119368,osf.io/rgjzk/supplementary,"A Link to Stage 1 Protocol, Study Logs and Data OSF: https://osf.io/rgjzk/Supplementary materials Supplementary material associated with this article can be found, in the online version, at doi: 10.1016/j.neuroimage.2022.119368.",0,0,1
10.1016/j.neuroimage.2022.119368,fmripower.org,"Power analysis and sample size calculations For the fMRI experiment, we used fMRIpower software package (http://fmripower.org) developed by Mumford and Nichols (2008) to calculate power and perform sample size estimation.",1,0,0
10.1016/j.neuroimage.2022.119368,simnibs.de,"We used Simulation for Non-invasive Brain Stimulation (SimNIBS 2.1.1, http://simnibs.de/) to build a head model and tested the optimal elec-trode placement to generate the maximal electric ﬁeld to the peak acti-vations in right DLPFC and cerebellum as observed in the fMRI experi-ment.",1,0,0
10.1016/j.neuroimage.2022.119655,osf.io/vybwm,"Data and code availability The following data tables and codes are available on the Open Sci-ence Framework (https://osf.io/vybwm): rating data table; EMG re-sponse tables containing extracted EMG response values, excluding runs in which participants fell asleep and trials with inaccurate live facial expression performances; and R codes for statistical analyses of rating and EMG data.",0,1,0
10.1016/j.neuroimage.2022.119655,ﬁl.ion.ucl.ac.uk/spm,Multi-echo EPI pre-processing was performed using the tedana v0.0.8 library (https://doi.org/10.5281/zenodo.4725985) and SPM12 v7771 software (http://www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119655,scipy.org,"This workﬂow used the NumPy (van der Walt et al., 2011), sciPy (http://www.scipy.org/), pandas (McKinney, 2010), scikit-learn (Pedregosa et al., 2011), Nilearn, NiBabel (http://doi.org/10.5281/zenodo.3233118), and the Dice simi-larity index libraries (Dice, 1945 ; Sørensen, 1948) in a conda environ-ment with Python v3.6.12.",1,0,0
10.1016/j.neuroimage.2022.119483,2.3.2.1,Structural MRI analysis 2.3.2.1.,0,0,1
10.1016/j.neuroimage.2022.119483,dbm.neuro.uni-jena.de/cat12,"VBM analysis of sMRI images was conducted using the SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/) and Computational Anatomy Toolbox (CAT12, http://dbm.neuro.uni-jena.de/cat12/).",1,0,0
10.1016/j.neuroimage.2022.119483,2.3.3.2,2.3.3.2.,0,0,1
10.1016/j.neuroimage.2022.119483,ﬁl.ion.ucl.ac.uk/spm,"VBM analysis of sMRI images was conducted using the SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/) and Computational Anatomy Toolbox (CAT12, http://dbm.neuro.uni-jena.de/cat12/).",1,0,0
10.1016/j.neuroimage.2022.119483,2.3.2.3,2.3.2.3.,0,0,1
10.1016/j.neuroimage.2022.119483,2.3.3.1,Resting-state fMRI analysis 2.3.3.1.,0,0,1
10.1016/j.neuroimage.2022.119483,rfmri.org/dpabi,"Resting-state fMRI data were preprocessed us-ing SPM12 and Data Processing & Analysis for Brain Imaging (DPABI, http://rfmri.org/dpabi).",1,0,0
10.1016/j.neuroimage.2022.119483,2.3.2.2,2.3.2.2.,0,0,1
10.1016/j.neuroimage.2022.119511,4dfp.readthedocs.io,"Initial FC preprocessing FC pre-processing generally followed previously described methods (Laumann et al., 2017 ; Gratton et al., 2019) implemented in the 4dfp suite of tools (http://4dfp.readthedocs.io).",1,0,0
10.1016/j.neuroimage.2022.119511,dian-info.org,Participants ADAD participants (Table 1) were drawn from 14 international sites in the Dominantly Inherited Alzheimer Network (DIAN) observational study (www.dian-info.org).,0,0,1
10.1016/j.neuroimage.2022.119519,humanconnectome.org/credit,"Lindquist NeuroImage 261 (2022) 119519 https://github.com/hhonar/MDApproach_PS.git The data is available through the Human Connectome Project web-site: https://www.humanconnectome.org/Credit authorship contribution statement Hamed Honari: Conceptualization, Methodology, Software, Formal analysis, Writing –original draft.",0,1,0
10.1016/j.neuroimage.2022.119519,github.com/hhonar/mdapproach_ps.git,"Lindquist NeuroImage 261 (2022) 119519 https://github.com/hhonar/MDApproach_PS.git The data is available through the Human Connectome Project web-site: https://www.humanconnectome.org/Credit authorship contribution statement Hamed Honari: Conceptualization, Methodology, Software, Formal analysis, Writing –original draft.",0,1,0
10.1016/j.neuroimage.2022.119500,brainmap.org,"Brief introduction of ALE The Ginger ALE 3.0.2 software (http://www.brainmap.org/) was used to perform coordinate-based meta-analyses of neuroimaging results (Eickhoﬀ et al., 2009 ; Laird et al., 2005 ; Turkeltaub et al., 2002)., In order to explore the coactivation patterns of the ROIs, we carried out a meta-analytic con-nectivity mapping (MACM) analysis by using the BrainMap Database (http://www.brainmap.org/) (Laird et al., First, we downloaded all the whole-brain peak coordinates in the Brain-Map Database (http://www.brainmap.org/) if the results of a particular study include at least one activation foci in the ROIs., For visualization purposes, all the results were registered onto a standard brain in an MNI template (Ch2better.nii, a template from the Dpabi; Yan et al., 2016) using Mango (http://www.brainmap.org/).",0,1,0
10.1016/j.neuroimage.2022.119500,neurosynth.org,"In addition, to assess to what extent this coactivation pattern of the right insula overlapped with the SN, we obtained a SN mask by online meta-analysis (A total of 126 studies with 4237 foci were included) using neurosynth (https://www.neurosynth.org/).",1,0,0
10.1016/j.neuroimage.2022.119500,pubmed.com,"We searched the literature using PubMed (www.pubmed.com) and Google Scholar before 10 th August 2020., Moreover, to select the literature related to decision making, we searched the PubMed (www.pubmed.com) and Google Scholar before 10 th August 2020 using the following keywords: ""decision making"" AND ""fMRI"" OR ""functional magnetic resonance imaging"", OR ""risk decision"" AND ""fMRI"" OR ""functional magnetic resonance imaging""., Furthermore, for emotion regulation, we searched the PubMed (www.pubmed.com) and Google Scholar before 10 th August 2020 based on the keywords ""Emotion regulation"" AND ""fMRI"" OR ""functional mag-netic resonance imaging"", OR ""cognitive reappraisal"" AND ""fMRI"" OR ""functional magnetic resonance imaging"", OR ""suppression"" AND ""fMRI"" OR ""functional magnetic resonance imaging"".",0,0,1
10.1016/j.neuroimage.2022.119452,git.fmrib.ox.ac.uk/hossein/bench,Software BENCH is an open source software implemented in python and avail-able at https://git.fmrib.ox.ac.uk/hossein/bench.,1,0,0
10.1016/j.neuroimage.2022.119768,colorbrewer2.org,The color schemes of several of the ﬁgures are from ColorBrewer 2.0 (http://colorbrewer2.org/) by Cynthia A.,0,0,1
10.1016/j.neuroimage.2022.119768,mathworks.com,"Then, we ran the segmentation and surface pipeline for all the images using the Computational Anatomy Toolbox (CAT) 12.7 v 1727 (http://www.neuro.uni-jena.de/cat/) with SPM12 v7771 in the background, running on MATLAB R2020a (The MathWorks, Natick, USA; https://www.mathworks.com)., tight_subplot(Nh, Nw, gap, marg_h, marg_w) (https://www.mathworks.com/matlabcentral/ﬁleexchange/27991-tight_subplot-nh-nw-gap-marg_h-marg_w), MATLAB Central File Exchange.",1,0,0
10.1016/j.neuroimage.2022.119768,fcon_1000.projects.nitrc.org/indi/corr/html/bnu_1.html),"The fourth dataset was a pooled 1 version of four diﬀerent datasets: the Beijing Normal University (BNU) dataset 1 (Lin et al., 2015) (n = 57; available at: https://fcon_1000.projects.nitrc.org/indi/CoRR/html/bnu_1.html), BNU dataset 2 (Huang et al., 2016) (n = 61; available at: https://fcon_1000.projects.nitrc.org/indi/CoRR/html/bnu_2.html), BNU dataset 3 (n = 48; available at: https://fcon_1000.projects.",0,1,0
10.1016/j.neuroimage.2022.119768,brain-development.org/ixi-dataset,The second dataset consisted of the scans acquired at the Guy’s Hospital and available as part of the IXI dataset (available at https://brain-development.org/ixi-dataset/) and composed of 322 subjects (henceforth referred to as “Guys ” dataset).,0,1,0
10.1016/j.neuroimage.2022.119768,fcon_1000.projects.nitrc.org/indi/corr/html/bnu_2.html),"The fourth dataset was a pooled 1 version of four diﬀerent datasets: the Beijing Normal University (BNU) dataset 1 (Lin et al., 2015) (n = 57; available at: https://fcon_1000.projects.nitrc.org/indi/CoRR/html/bnu_1.html), BNU dataset 2 (Huang et al., 2016) (n = 61; available at: https://fcon_1000.projects.nitrc.org/indi/CoRR/html/bnu_2.html), BNU dataset 3 (n = 48; available at: https://fcon_1000.projects.",0,1,0
10.1016/j.neuroimage.2022.119768,nitrc.org/indi/corr/html/bnu_3.html),"nitrc.org/indi/CoRR/html/bnu_3.html), from the Consortium for Re-liability and Reproducibility (CoRR) dataset (Zuo et al., 2014), and the “Beijing_Zang ” dataset (n = 192) from the 1000 Functional Con-nectomes Project (Biswal et al., 2010).",0,1,0
10.1016/j.neuroimage.2022.119768,nitrc.org/projects/art,"For each image, we set the origin (i.e., (0,0,0) coordinate) to correspond to the anterior commissure (AC) using acpcdetect v2 (Ardekani, 2018 ; Ardekani et al., 1997 ; Ardekani and Bachman, 2009) (available at: https://www.nitrc.org/projects/art/).",1,0,0
10.1016/j.neuroimage.2022.119768,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"We then, visually examined the images and manually set the origin to the AC using the display utility in SPM12 v7771 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) in case of acpcdetect failure.",1,0,0
10.1016/j.neuroimage.2022.119768,neuro.uni-jena.de/cat,"Then, we ran the segmentation and surface pipeline for all the images using the Computational Anatomy Toolbox (CAT) 12.7 v 1727 (http://www.neuro.uni-jena.de/cat/) with SPM12 v7771 in the background, running on MATLAB R2020a (The MathWorks, Natick, USA; https://www.mathworks.com).",1,0,0
10.1016/j.neuroimage.2022.119768,github.com/parekhpravesh/harmonizationpaper,"The code for the experiments is available at https://github.com/parekhpravesh/HarmonizationPaper., The code for the experiments is available at https://github.com/parekhpravesh/HarmonizationPaper.",1,0,0
10.1016/j.neuroimage.2022.119768,mathworks.com/matlabcentral/ﬁleexchange/27991-tight_subplot-nh-nw-gap-marg_h-marg_w,"tight_subplot(Nh, Nw, gap, marg_h, marg_w) (https://www.mathworks.com/matlabcentral/ﬁleexchange/27991-tight_subplot-nh-nw-gap-marg_h-marg_w), MATLAB Central File Exchange.",1,0,0
10.1016/j.neuroimage.2022.119768,github.com/rpomponio/neuroharmonize,"Harmonization For all the experiments, we used the neuroHarmonize (Pomponio et al., 2020) (available at: https://github.com/rpomponio/neuroHarmonize/) toolbox for harmonizing the features across scan-ners.",1,0,0
10.1016/j.neuroimage.2022.119598,vent.post.lat,A > 0 Cluster L/R t -statistic x y z Voxels 1 Heschl’s gyrus L 12.83 -48 -13 3 7317 Superior temporal gyrus (posterior division) L 11.81 -64 -16 9 Superior temporal gyrus (anterior division) L 6.68 -48 12 -13 Insula/Operculum L 11.2 -42 -6 18 Lentiform nucleus (putamen) L 4.12 -31 -9 -5 Lentiform nucleus (lateral globus pallidus) L 4.55 -21 -13 -2 Thalamus (vent.post.lat.,0,0,1
10.1016/j.neuroimage.2022.119660,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,"For further in-formation and details of data acquisition and processing in this sam-ple, please see: https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release.",0,0,1
10.1016/j.neuroimage.2022.119660,github.com/mezerlab/mrgrad,The code for creating the midbrain proﬁles is implemented in MAT-LAB (https://github.com/MezerLab/mrGrad).,1,0,0
10.1016/j.neuroimage.2022.119660,github.com/mezera/mrq,"Whole-brain R1 maps, as well as ﬁeld bias maps of excitation (B1 +), were computed using the mrQ software (https://github.com/mezera/mrQ).",1,0,0
10.1016/j.neuroimage.2022.119660,academic.oup.com/cercor/article/31/2/1211/5934909,"Interest-ingly, the age-related increase in R1 stands in contrast to studies that ﬁnd that R1 decreases with age in the white matter (Yeatman et al., 2014) and in the cortex (Gracien et al., 2017 ; Seiler et al., 2020) https://academic.oup.com/cercor/article/31/2/1211/5934909.",0,0,1
10.1016/j.neuroimage.2022.119334,photo-ac.com,We subjectively chose 159 candidate images that induce pos-itive emotions as stimuli from a web-based photo database (photoAC: https://www.photo-ac.com/).,0,1,0
10.1016/j.neuroimage.2022.119334,2.2.2.2,2.2.2.2.,0,0,1
10.1016/j.neuroimage.2022.119334,osf.io/tk3mr,NeuroImage 257 (2022) 119334 Data/Code availability The data and code are publicly available via the open science frame-work at https://osf.io/tk3mr/.,0,1,0
10.1016/j.neuroimage.2022.119334,editage.jp,Acknowledgments We would like to thank Editage (www.editage.jp) for English lan-guage editing.,0,0,1
10.1016/j.neuroimage.2022.119334,nitrc.org/projects/conn,"We used the CONN toolbox, ver-sion 20.b (www.nitrc.org/projects/conn , RRID: SCR_009550; Whitﬁeld-Gabrieli and Nieto-Castanon, 2012) for the analysis.",1,0,0
10.1016/j.neuroimage.2022.119334,2.2.2.1,Whole-brain and region of interest (ROI) analysis 2.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119334,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"fMRI data were preprocessed and analyzed using SPM 12 (http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and MAT-LAB R2020b (MathWorks, Natick, MA, USA).",1,0,0
10.1016/j.neuroimage.2022.119334,r-project.org,Behavioral data analysis Behavioral data analysis was conducted using R software version 4.0.0 (https://www.R-project.org/).,1,0,0
10.1016/j.neuroimage.2022.119334,people.cas.sc.edu/rorden/mricro,"Regions were labeled using the SPM Anatomy Toolbox (Eickhoﬀet al., 2005) and the Brodmann area template of MRI-cro (people.cas.sc.edu/rorden/mricro).",1,0,0
10.1016/j.neuroimage.2022.118891,chronux.org,The Matlab toolboxes used for the EEG analysis are freely available online (EEGLAB: https://github.com/sccn/eeglab; Chronux: http://chronux.org/; Brainstorm: https://neuroimage.usc.,1,0,0
10.1016/j.neuroimage.2022.118891,github.com/sccn/eeglab,The Matlab toolboxes used for the EEG analysis are freely available online (EEGLAB: https://github.com/sccn/eeglab; Chronux: http://chronux.org/; Brainstorm: https://neuroimage.usc.,1,0,0
10.1016/j.neuroimage.2022.119285,github.com/hiroshiban/fs2bv,Data/code availability Any custom code used for analyses of data in this manuscript (in conjunction with built-in tools in BrainVoyager) are available at https://github.com/hiroshiban/FS2BV (shared with permission from Hiroshi Ban).,1,0,0
10.1016/j.neuroimage.2022.119750,applied-statistics.de/lst.html,"VOI deﬁnition In all study participants (HC and MS), lesions were segmented from FLAIR and MPRAGE data using the lesion growth algo-rithm (Schmidt et al., 2012) from the lesion segmentation tool (https://www.applied-statistics.de/lst.html) for SPM12.",1,0,0
10.1016/j.neuroimage.2022.119750,mriresearch.med.ubc.ca/news-projects/myelin-water-fraction/and,"For NNLS-based MWF calculation, the MATLAB scripts can be obtained from https://mriresearch.med.ubc.ca/news-projects/myelin-water-fraction/and the Decaes toolbox provides a Julia-based equiv-alent for data processing (https://github.com/jondeuce/DECAES.jl).",1,0,0
10.1016/j.neuroimage.2022.119750,github.com/mnagtegaal/spijn,A demo ver-sion is available via https://github.com/MNagtegaal/SPIJN.,1,0,0
10.1016/j.neuroimage.2022.119750,neurovault.org/images/1401,"Additionally, sev-eral anatomical regions from the ICBM-DTI-81 white-matter labels atlas (https://neurovault.org/images/1401/) (Mori et al., 2005) were com-bined into three WM regions (corpus callosum, corona radiata, inter-nal capsule) and registered to the participants’ MPRAGE data using the SPM12 “normalize ”module.",0,1,0
10.1016/j.neuroimage.2022.119750,nitrc.org/projects/noddi_toolbox,from https://www.nitrc.org/projects/noddi_toolbox.,0,0,1
10.1016/j.neuroimage.2022.119750,hmri.info,"The lat-est version of the hMRI toolbox, which was used for calculation of MTsat and PD parameter maps, is available from www.hMRI.info.",1,0,0
10.1016/j.neuroimage.2022.119750,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"MTsat and proton density (PD) parameter maps were generated from the 3D multi-echo gradient echo data using the hMRI toolbox (Tabelow et al., 2019) included in the SPM framework (SPM12, version v7771; www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).",1,0,0
10.1016/j.neuroimage.2022.119750,github.com/jondeuce/decaes.jl,"For NNLS-based MWF calculation, the MATLAB scripts can be obtained from https://mriresearch.med.ubc.ca/news-projects/myelin-water-fraction/and the Decaes toolbox provides a Julia-based equiv-alent for data processing (https://github.com/jondeuce/DECAES.jl).",1,0,0
10.1016/j.neuroimage.2022.119291,nitrc.org/projects/bnv,The modular architecture was visualized using BrainNet Viewer (www.nitrc.org/projects/bnv/).,1,0,0
10.1016/j.neuroimage.2022.119170,github.com/gkaguirrelab/eyetracktomeanalysis/tree/master/code,The code used to analyze the data is publicly available in three repos-itories: -Analysis of eye tracking videos: https://github.com/gkaguirrelab/eyeTrackTOMEAnalysis/tree/master/code.,1,0,0
10.1016/j.neuroimage.2022.119170,github.com/gkaguirrelab/transparenttrack,Measurement of eye position and the velocity of slow phase drift The IR videos of the eye were ﬁrst analyzed to extract eye fea-tures using open-source software (https://github.com/gkaguirrelab/transparentTrack).,1,0,0
10.1016/j.neuroimage.2022.119170,github.com/gkaguirrelab/mritomeanalysis/tree/master/code/innerearmodelandnystagmus,-Statistical analysis and plotting functions: https://github.com/gkaguirrelab/mriTOMEAnalysis/tree/master/code/innerEarModelAndNystagmus.,1,0,0
10.1016/j.neuroimage.2022.119170,youtube.com/channel/ucxojt8kd3bcsult_zawzwwq,youtube.com/channel/UCXojt8KD3bCsUlT_zaWzWWQ.,0,0,1
10.1016/j.neuroimage.2022.119170,osf.io/ervrh,Participants Forty-two participants from the University of Pennsylvania and the surrounding Philadelphia community were studied as part of a pre-registered project (https://osf.io/ervrh).,0,0,1
10.1016/j.neuroimage.2022.119279,humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage,Family structure and genetic information are re-stricted (https://www.humanconnectome.org/study/hcp-young-adult/document/quick-reference-open-access-vs-restricted-data) and available only to qualiﬁed inves-tigators who agree to HCP Restricted Data Use Terms (https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage).,0,0,1
10.1016/j.neuroimage.2022.119279,db.humanconnectome.org,Data availability The Human Connectome Project (HCP) data used in the current study are publicly available at https://db.humanconnectome.org.,0,1,0
10.1016/j.neuroimage.2022.119279,github.com/rayksyoo/c2c,Code availability MATLAB script for the connectome-to-connectome (C2C) model con-struction is available to download at https://github.com/rayksyoo/C2C.,1,0,0
10.1016/j.neuroimage.2022.119279,humanconnectome.org/study/hcp-young-adult/document/quick-reference-open-access-vs-restricted-data,Family structure and genetic information are re-stricted (https://www.humanconnectome.org/study/hcp-young-adult/document/quick-reference-open-access-vs-restricted-data) and available only to qualiﬁed inves-tigators who agree to HCP Restricted Data Use Terms (https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage).,0,0,1
10.1016/j.neuroimage.2022.118993,github.com/brain-networks/event,"Code for calculating edge time series, detecting events, clus-tering events, and predicting FC from event clusters is available here: https://github.com/brain-networks/event det ection.",1,0,0
10.1016/j.neuroimage.2022.118993,openneuro.org/datasets/ds000224/versions/00002,"This dataset was previously reported in (Gordon et al., 2017b ; Gratton et al., 2018) and is publicly available at https://openneuro.org/datasets/ds000224/versions/00002.",0,1,0
10.1016/j.neuroimage.2022.118993,openfmri.org/dataset/ds000224/and,"Data and code availability Midnight Scan Club raw data and derivatives are available here: https://openfmri.org/dataset/ds000224/and in processed, parcellated form here https://www.dropbox.com/sh/tb694nmpu2lbpnc/AABKU Mew7h-yjtAC4ObzG VaKa?dl = 0.",0,1,0
10.1016/j.neuroimage.2022.118993,dropbox.com/sh/tb694nmpu2lbpnc/aabku,"Data and code availability Midnight Scan Club raw data and derivatives are available here: https://openfmri.org/dataset/ds000224/and in processed, parcellated form here https://www.dropbox.com/sh/tb694nmpu2lbpnc/AABKU Mew7h-yjtAC4ObzG VaKa?dl = 0.",0,1,0
10.1016/j.neuroimage.2022.118993,myconnectome.org/wp/data-sharing,"MyConnectome dataset All data and cortical surface ﬁles are freely available and were obtained from the MyConnectome Project’s data-sharing webpage (http://myconnectome.org/wp/data-sharing/)., Processed and parcellated MyCon-nectome data is available here: http://myconnectome.org/wp/data-sharing/.",0,1,0
10.1016/j.neuroimage.2022.118993,netwiki.amath.unc.edu/genlouvain/genlouvain,"At each value, we use a generalized version of the Lou-vain algorithm to optimize the corresponding 𝑄 (𝛾) (Jutla et al., 2011) (http://netwiki.amath.unc.edu/GenLouvain/GenLouvain).",1,0,0
10.1016/j.neuroimage.2022.119463,neurovault.org/collections/12205,"Data/code availability statement The de-identiﬁed data, analysis scripts and materials for this study are available on DataverseNL and the MRI data are available on Neu-roVault (https://neurovault.org/collections/12205/).",0,1,0
10.1016/j.neuroimage.2022.119463,nwo.nl,Funding This study was supported by a personal research grant from the Netherlands Organization for Scientiﬁc Research (http://www.nwo.nl/) awarded to B.E.,0,0,1
10.1016/j.neuroimage.2022.119463,osf.io/54nky,"All study measures, hypotheses, and analyses were preregistered at Open Science Framework prior to data analyses (https://osf.io/54nky/)., All study mea-sures and hypotheses were preregistered at Open Science Framework prior to data analyses (https://osf.io/54nky/).",0,0,1
10.1016/j.neuroimage.2022.119712,github.com/junjypark/clean,An R package with code for conduct-ing statistical analyses using CLEAN-R is available on GitHub at https://github.com/junjypark/CLEAN.,1,0,0
10.1016/j.neuroimage.2022.119712,github.com/junjypark/clean,"Our method is inherently computationally eﬃcient and can be implemented using our publicly available software (http://www.github.com/junjypark/CLEAN)., Software and computational eﬃciency CLEAN-R can be implemented using software available at http://www.github.com/junjypark/CLEAN.",1,0,0
10.1016/j.neuroimage.2022.119254,pls.rotman-baycrest.on.ca/source,Matlab R2016a was used to perform the analy-sis along with the PLS package created by the Rotman Research Institute (http://pls.rotman-baycrest.on.ca/source).,1,0,0
10.1016/j.neuroimage.2022.119254,clinicaltrials.gov,"The Whitehall II Imaging Sub-study was supported by the UK Med-ical Research Council (MRC) grants “Predicting MRI abnormalities with longitudinal data of the Whitehall II Sub-study ” (G1001354; PI KPE; ClinicalTrials.gov Identiﬁer: NCT03335696), and “Adult De-terminants of Late Life Depression, Cognitive Decline and Physical Functioning -The Whitehall II Ageing Study ”(MR/K013351/1 ; PI: MK).",0,0,1
10.1016/j.neuroimage.2022.119254,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fdt,"DTIFit (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT) was used to generate maps of MD, FA, and RD for each subject.",1,0,0
10.1016/j.neuroimage.2022.119254,github.com/cobralab/minc-bpipe-library,"T1w images were preprocessed using the minc-bpipe-library (https://github.com/CoBrALab/minc-bpipe-library), including bias ﬁeld correction (Tustison et al., 2010), adaptive non-local means denoising (Manjón et al., 2010), head masking and brain extraction (Eskildsen et al., 2012) The resulting bias ﬁeld corrected, head-masked images and brain masks of each subject were input into the CIVET algo-rithm (Ad-Dab’bagh et al., 2006 ; Lerch & Evans, 2005) (version 2.1.0) in order to obtain cortical mid-surfaces and vertex wise measures of cortical thickness (CT) and surface area (SA), describing CT and SA esti-mates at a total of 81924 points across the cortical mid-surface.",1,0,0
10.1016/j.neuroimage.2022.119254,portal.dementiasplatform.uk,NeuroImage 257 (2022) 119254 (https://portal.dementiasplatform.uk/).,0,0,1
10.1016/j.neuroimage.2022.119254,mrc.ukri.org/research/policies-and-guidance-for-researchers/data-sharing,Data and Code Availability The study follows Medical Research Council data sharing policies (https://mrc.ukri.org/research/policies-and-guidance-for-researchers/data-sharing/).,0,1,0
10.1016/j.neuroimage.2022.119254,github.com/raihaan/micro-cog-nmf,Code used in this analysis is available at https://github.com/raihaan/micro-cog-nmf.,1,0,0
10.1016/j.neuroimage.2022.119728,github.com/gallantlab/tikreg,"Hyperparameter optimization For the ridge regression models, the reg-ularization hyperparameter 𝜆was optimized over a grid-search with 20 2 https://github.com/gallantlab/tikreg. 15 T.",1,0,0
10.1016/j.neuroimage.2022.119728,github.com/gallantlab/himalaya,"To understand this issue, note that solving ordinary least squares amounts to inverting the eigenvalues 𝜎𝑖 of the matrix 𝑋 ⊤𝑋. 1 https://github.com/gallantlab/himalaya. 2 T., Because banded ridge regression disentangles correlated feature spaces it clariﬁes which speciﬁc feature spaces best predict brain activity. 3 https://github.com/gallantlab/himalaya. 16 T., Code availability statement All methods described in this paper are implemented in an open-source Python package called Himalaya (https://github.com/gallantlab/himalaya).",1,0,0
10.1016/j.neuroimage.2022.119059,clinicaltrials.gov,"Sample size, power, and outcomes The study was preregistered at clinicaltrials.gov (id# NCT03035669).",0,0,1
10.1016/j.neuroimage.2022.119059,clinicatrials.gov,"To address these questions, a neuroimaging-based randomized controlled trial (RCT) was performed (n = 68), comparing mindfulness-based stress reduction (MBSR) with a reading/listening intervention (READ), using a novel dyadic paradigm for self and other emotion regulation under stress as primary outcome on behavior and brain levels and established empathy measures (clinicatrials.gov NCT03035669).",0,0,1
10.1016/j.neuroimage.2022.119059,fmrib.ox.ac.uk/fsl,"Data pre-processing and analysis were implemented with FSL 6.0 version (FMRIB’s Software Library, www.fmrib.ox.ac.uk/fsl).",1,0,0
10.1016/j.neuroimage.2022.119209,commdetect.weebly.com,"Retrieved from http://commdetect.weebly.com/), as described below.",0,0,1
10.1016/j.neuroimage.2022.119209,2.7.2.1,2.7.2.1.,0,0,1
10.1016/j.neuroimage.2022.119209,2.8.1.2,2.8.1.2.,0,0,1
10.1016/j.neuroimage.2022.119209,2.8.3.2,2.8.3.2.,0,0,1
10.1016/j.neuroimage.2022.119209,humanconnectome.org/study/hcp-lifespan-development,humanconnectome.org/study/hcp-lifespan-development ; https://www.,0,0,1
10.1016/j.neuroimage.2022.119209,2.1.2.1,2.1.2.1.,0,0,1
10.1016/j.neuroimage.2022.119209,hcp-aging.org,Participants The present research uses cross-sectional data preprocessed by the Lifespan HCP study team and downloaded in March 2021 as part of the 2.0 Data Release for the HCP-Development (www.hcp-development.org) and HCP-Aging (www.hcp-aging.org) studies.,0,1,0
10.1016/j.neuroimage.2022.119209,github.com/frantisekvasa/rotate_parcellation/commit/bb8b0ef10980f162793cc180cef371e83655c505,"In the gene-brain PLS analysis, to account for correlated gene ex-pression patterns based on anatomical proximity (Fornito et al., 2019 ; Markello & Misic, 2021) we used Vasa’s “rotate_parcellation ”f u n c -tion in Matlab (https://github.com/frantisekvasa/rotate_parcellation/commit/bb8b0ef10980f162793cc180cef371e83655c505) in order to generate 100,000 spatially constrained permutations of the Schaefer brain LV, as identiﬁed in behavioral PLS analysis 1.",1,0,0
10.1016/j.neuroimage.2022.119209,brain-map.org,"Micro-array gene expression data were obtained from six postmortem brains (1 female, ages 24.0–57.0, 42.50 +/-13.38) provided by the Allen Institute for Brain Science (https://www.brain-map.org/).",0,1,0
10.1016/j.neuroimage.2022.119209,fuma.ctglab.nl/browse,"The AD-relevant candidate risk loci had been mapped onto the corresponding genes by the original authors using the SNP2GENE tool in FUMA and made available via the Public Results tab (https://fuma.ctglab.nl/browse , Watanabe et al., 2017).",1,0,0
10.1016/j.neuroimage.2022.119209,github.com/thomasyeolab/cbig,"ROI deﬁnition Our main analyses were based on the Schaefer 300 parcel-functional atlas (Schaefer et al., 2018 ; Yeo et al., 2011), downloaded from https://github.com/ThomasYeoLab/CBIG.",0,1,0
10.1016/j.neuroimage.2022.119209,nitrc.org/projects/bnv,"Panel (a) depicts the Schaefer ROIs with robust loadings (absolute value BSR > 3) on the LV in panels (b, c) and visualized with the BrainNet Viewer (http://www.nitrc.org/projects/bnv/) (Xia et al., 2013)., The ROIs were visualized with the BrainNet Viewer (http://www.nitrc.org/projects/bnv/) (Xia et al., 2013)., The ROIs were visualized with the BrainNet Viewer (http://www.nitrc.org/projects/bnv/) (Xia et al., 2013).",1,0,0
10.1016/j.neuroimage.2022.119209,nda.nih.gov/ccf/lifespan-studies,Data statement The raw data are available at https://nda.nih.gov/ccf/lifespan-studies upon completion of the relevant data use agreements.,0,1,0
10.1016/j.neuroimage.2022.119209,2.5.3.2,2.5.3.2.,0,0,1
10.1016/j.neuroimage.2022.119209,humanconnectome.org/study/hcp-lifespan-aging),"humanconnectome.org/study/hcp-lifespan-aging), held in the NIMH Data Archive (NDA).",0,1,0
10.1016/j.neuroimage.2022.119209,2.8.3.1,2.8.3.1.,0,0,1
10.1016/j.neuroimage.2022.119209,2.8.1.1,2.8.1.1.,0,0,1
10.1016/j.neuroimage.2022.119209,github.com/netneurolab/abagen,The gene expression data was processed with abagen (https://github.com/netneurolab/abagen).,1,0,0
10.1016/j.neuroimage.2022.119209,2.8.1.3,2.8.1.3.,0,0,1
10.1016/j.neuroimage.2022.119209,2.8.3.3,2.8.3.3.,0,0,1
10.1016/j.neuroimage.2022.119209,rotman-baycrest.on.ca/index.php?section,"PLS was implemented using a series of Mat-lab scripts, which are available for download at https://www.rotman-baycrest.on.ca/index.php?section = 345.",1,0,0
10.1016/j.neuroimage.2022.119209,2.5.3.1,2.5.3.1.,0,0,1
10.1016/j.neuroimage.2022.119209,hcp-development.org,Participants The present research uses cross-sectional data preprocessed by the Lifespan HCP study team and downloaded in March 2021 as part of the 2.0 Data Release for the HCP-Development (www.hcp-development.org) and HCP-Aging (www.hcp-aging.org) studies.,0,1,0
10.1016/j.neuroimage.2022.119209,2.6.4.1,2.6.4.1.,0,0,1
10.1016/j.neuroimage.2022.119209,surfer.nmr.mgh.harvard.edu,"The FreeSurfer pipeline (http://surfer.nmr.mgh.harvard.edu/) generated the surface and vol-ume anatomical parcellations, as well as morphometric measurements of structure volumes and surface areas (cf.",1,0,0
10.1016/j.neuroimage.2022.119209,2.1.1.1,2.1.1.1.,0,0,1
10.1016/j.neuroimage.2022.119209,2.6.4.2,2.6.4.2.,0,0,1
10.1016/j.neuroimage.2022.119209,github.com/chrisﬁlo/alleninf,The MNI coordinates of tissue samples were updated to those gen-erated via non-linear registration using the Advanced Normalization Tools (ANTs; https://github.com/chrisﬁlo/alleninf).,1,0,0
10.1016/j.neuroimage.2021.118756,osf.io/fsqaj,"Data and code availability statement Behavioral data, scripts for data analysis is available via https://osf.io/fsqaj/, fMRI Data can be accessed from the ﬁrst au-thor on request upon a formal data usage agreement., Data and code availability statement Behavioral data, 3D datasets converted by the to3d program in AFNI, scripts for data analysis are available via https://osf.io/fsqaj.",0,1,0
10.1016/j.neuroimage.2021.118756,afni.nimh.nih.gov/afni,"Image preprocessing and GLM analysis Data were analyzed using the Analysis of Functional NeuroImages software (AFNI, http://afni.nimh.nih.gov/afni/) and in-house Matlab codes (Mathworks, MA).",1,0,0
10.1016/j.neuroimage.2021.118756,osf.io/fsqaj,"Data and code availability statement Behavioral data, scripts for data analysis is available via https://osf.io/fsqaj/, fMRI Data can be accessed from the ﬁrst au-thor on request upon a formal data usage agreement.",0,1,0
10.1016/j.neuroimage.2022.119638,github.com/mnagtegaal/spijn,The code for the SPIJN-MRF algorithm was previously pub-lished (10.1002/mrm.27947) and available at https://github.com/MNagtegaal/SPIJN Declaration of Competing Interest The authors declare that there is no conﬂict of interest.,1,0,0
10.1016/j.neuroimage.2022.118909,antsbrainextraction.sh,"The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workﬂow (from ANTs), using OA-SIS30ANTs as the target template.",1,0,0
10.1016/j.neuroimage.2022.118909,fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/file/mri/hbn_ru_protocol.pdf,Detailed scanner protocols are published on the Healthy Brain Network project website (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/File/mri/HBN_RU_Protocol.pdf).,0,0,1
10.1016/j.neuroimage.2022.118909,fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/index.html,Data and code availability Neuroimaging and phenotypic data can be collected fol-lowing directions on the Healthy Brain Network data portal (http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/index.html) after signing a data use agreement.,0,1,0
10.1016/j.neuroimage.2022.118909,advarra.com,"The Healthy Brain Network project was approved by the Chesapeake Institutional Review Board (now called Advarra, Inc.; https://www.advarra.com/).",0,0,1
10.1016/j.neuroimage.2022.119391,mrtrix.org,"In contrast, the LEMON DWI scans were obtained in raw format and underwent preprocessing through the dedicated pipeline included in the MRtrix3 software (https://www.mrtrix.org/).",1,0,0
10.1016/j.neuroimage.2022.119391,fcon_1000.projects.nitrc.org/indi/retro/mpi_lemon.html,"Validation dataset (LEMON) We obtained structural, diﬀusion and rs-fMRI data of 213 healthy subjects (males = 138, females = 75, age range 20-70 years) from the Leipzig Study for Mind-Body-Emotion Interactions (LEMON) dataset (http://fcon_1000.projects.nitrc.org/indi/retro/MPI_LEMON.html).",0,1,0
10.1016/j.neuroimage.2022.119391,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"Structural preprocessing All T1-weighted images were obtained in skull-stripped version (Babayan et al., 2019 ; Glasser et al., 2013) and were subsequently seg-mented into cortical and subcortical gray matter (GM), white matter (WM) and cerebrospinal ﬂuid (CSF) using FAST and FIRST FSL’s tools (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/).",1,0,0
10.1016/j.neuroimage.2022.119391,humanconnectome.org,"Primary dataset (HCP) Structural, diﬀusion and resting-state functional MRI data of 210 healthy subjects (males = 92, females = 118, age range 22-36 years) were retrieved from the HCP repository (https://humanconnectome.org).",0,1,0
10.1016/j.neuroimage.2021.118745,osf.io/c4m82/were,(2019) on https://osf.io/c4m82/were also considered.,0,0,1
10.1016/j.neuroimage.2021.118745,osf.io/kyp6s,"Pre-registration Hypotheses, data acquisition and parts of the data analysis were pre-registered online on https://osf.io/kyp6s/.",0,0,1
10.1016/j.neuroimage.2021.118745,osf.io/pfyt7,An experimen-tal checklist used by the experimenter during scanning is available on https://osf.io/pfyt7/.,0,0,1
10.1016/j.neuroimage.2022.119359,db.humanconnectome.org,NeuroImage 258 (2022) 119359 Data and materials availability All the data included in this study are publicly available in the Hu-man Connectome Project database at https://db.humanconnectome.org Declaration of Competing Interests The authors have no conﬂict of interests to disclose.,0,1,0
10.1016/j.neuroimage.2022.119298,2.2.2.2,2.2.2.2.,0,0,1
10.1016/j.neuroimage.2022.119298,2.2.1.1,2.2.1.1.,0,0,1
10.1016/j.neuroimage.2022.119298,2.2.2.5,2.2.2.5.,0,0,1
10.1016/j.neuroimage.2022.119298,2.2.1.2,2.2.1.2.,0,0,1
10.1016/j.neuroimage.2022.119298,2.2.2.4,2.2.2.4.,0,0,1
10.1016/j.neuroimage.2022.119298,2.2.2.1,Evaluating algorithms: beyond performance metrics 2.2.2.1.,0,0,1
10.1016/j.neuroimage.2022.119298,2.2.1.4,2.2.1.4.,0,0,1
10.1016/j.neuroimage.2022.119298,grand-challenge.org,"The international conference on Medi-cal Image Computing and Computer Assisted Interventions (MICCAI), the IEEE International Symposium on Biomedical Imaging (ISBI), the Grand Challenge website (https://grand-challenge.org/) and the Kag-gle platform are the leading organizers hosting inﬂuential annual benchmark challenges, with medical imaging modalities (MRI and CT) clearly dominating the nature of published benchmark datasets (Maier-Hein et al., 2018).",0,0,1
10.1016/j.neuroimage.2022.119298,2.2.1.3,2.2.1.3.,0,0,1
10.1016/j.neuroimage.2022.119298,2.2.2.3,2.2.2.3.,0,0,1
10.1016/j.neuroimage.2022.119327,irontract.mgh.harvard.edu,The IronTract Challenge was administered in two rounds (https://irontract.mgh.harvard.edu).,0,0,1
10.1016/j.neuroimage.2022.119327,github.com/chiaramﬀ/irontract,The post-processing scripts used in Round2 are available at https://github.com/chiaramﬀ/IronTract.,1,0,0
10.1016/j.neuroimage.2022.119327,qmenta.com/irontract-challenge,"The organizing team uploaded the data to the QMENTA platform (https://qmenta.com/irontract-challenge/) and the challenge teams could download them along with the tracer injection sites in the dMRI space., The IronTract Challenge remains open (https://qmenta.com/irontract-challenge/) and we plan to expand its scope in future iterations., Data availability The authors declare that the data supporting the ﬁndings of this study are available on the QMENTA platform (https://qmenta.com/irontract-challenge/).",0,1,0
10.1016/j.neuroimage.2022.119419,cran.r-project.org/web/packages/spamm,"To account for the spatial cor-relation in the analysis of remote regressions (uRQS), we used the spam package (https://cran.r-project.org/web/packages/spaMM/) that implement spatial Generalized Mixed Models (spaGLMMs, (Rousset and Ferdy, 2014)).",1,0,0
10.1016/j.neuroimage.2022.119419,rstudio.com/.,"RStudio, Inc., Boston, MA URL http://www.rstudio.com/., n.d.; The jamovi project (2020).",1,0,0
10.1016/j.neuroimage.2022.119419,gricad-gitlab.univ-grenoble-alpes.fr/harquels/public_codes,"The EEG data analysis function used in this work (using Matlab and Fieldtrip toolbox) can be found here: https://gricad-gitlab.univ-grenoble-alpes.fr/harquels/public_codes., The EEG data analysis function used in this work (using Matlab and Fieldtrip toolbox) can be found here: https://gricad-gitlab.univ-grenoble-alpes.fr/harquels/public_codes.",1,0,0
10.1016/j.neuroimage.2022.119419,clinicaltrials.gov,This study was approved by the ethical committee of Grenoble University Hospital (ID RCB: 2013-A01734-41) and registered on ClinicalTrials.gov (num-ber NCT02168413).,0,0,1
10.1016/j.neuroimage.2022.119419,jamovi.org,"Retrieved from https://www.jamovi.org, n.d.), and are detailed thereafter.",0,0,1
10.1016/j.neuroimage.2022.119372,osf.io/t492p,The review protocol was cre-ated using the International Prospective Register of Systematic Reviews (PROSPERO) template and preregistered on Open Science Framework (https://osf.io/t492p/).,1,0,0
10.1016/j.neuroimage.2022.118979,peiyin.xunfei.cn,"All disyllabic words were synthesized in-dependently using the iFLYTEK synthesizer (http://peiyin.xunfei.cn/; Mandarin Chinese; female voice, Xiaoying).",1,0,0
10.1016/j.neuroimage.2022.119522,osf.io/y6rc3,"The data together with deep gray matter parcellation maps are now publicly available via an open repository at https://osf.io/yfms7/, and the raw multi-orientation GRE data are also available at https://osf.io/y6rc3/., These data were publicly available via an open repository at https://osf.io/yfms7/, and the raw multi-orientation GRE data were also available at https://osf.io/y6rc3/.",0,1,0
10.1016/j.neuroimage.2022.119522,osf.io/yfms7/this,"Code: The source codes of quantitative evaluation metric calcula-tions were also downloadable at: https://osf.io/yfms7/This web page contains all necessary instructions, codes, and data to train, test and evaluate the newly proposed QSM reconstruction net-works.",1,0,0
10.1016/j.neuroimage.2022.119522,stnava.github.io/ants,"We registered the susceptibility template of the HybraPD at-las (Yu et al., 2021) to each subject using the ‘SyN’ approach in ANTs (http://stnava.github.io/ANTs) and the obtained transformation ma-trix was applied to the parcellation map.",1,0,0
10.1016/j.neuroimage.2022.119522,osf.io/yfms7,"The data together with deep gray matter parcellation maps are now publicly available via an open repository at https://osf.io/yfms7/, and the raw multi-orientation GRE data are also available at https://osf.io/y6rc3/., These data were publicly available via an open repository at https://osf.io/yfms7/, and the raw multi-orientation GRE data were also available at https://osf.io/y6rc3/., Code: The source codes of quantitative evaluation metric calcula-tions were also downloadable at: https://osf.io/yfms7/This web page contains all necessary instructions, codes, and data to train, test and evaluate the newly proposed QSM reconstruction net-works.",0,1,0
10.1016/j.neuroimage.2022.119522,people.eecs.berkeley.edu/∼chunlei,Phase processing steps were all implemented in STISuite toolbox (https://people.eecs.berkeley.edu/∼chunlei.,1,0,0
10.1016/j.neuroimage.2022.119375,ﬁl.ion.ucl.ac.uk/spm,fMRI data were pre-processed in SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119375,www2.warwick.ac.uk/fac/sci/statistics/staﬀ/academic-research/nichols/software,"For group-level inference, individual crossnobis distance maps were normalized and smoothed (using a Gaussian kernel with a full-width at half maximum of 8 mm) and then entered into a group level random-eﬀects analysis using permutation-based nonparametric statistics in SNPM (http://www2.warwick.ac.uk/fac/sci/statistics/staﬀ/academic-research/nichols/software).",1,0,0
10.1016/j.neuroimage.2022.119141,fsl.fmrib.ox.ac.uk,"We used the Diﬀusion MRI tools (FDT) in the FMRIB Software Library (FSL version 6.0.3; fsl.fmrib.ox.ac.uk/) to ﬁt a tensor model to the preprocessed dMRI data (DTFIT) and obtain diﬀusion metrics (FA, MD, AD and RD)., Fiber tracing The ﬁber tracing procedure was performed on the neonatal and adult samples analogously using the probtrackx2 function from the FDT toolbox in the FMRIB Software Library (FSL version 6.0.3; fsl.fmrib.ox.ac.uk/): For each brain hemisphere independently using each ROI of a given ﬁber tract both as seed and target (e.g., ASS and AUD)., We used the Diﬀusion MRI tools (FDT) in the FMRIB Software Library (FSL version 6.0.3; fsl.fmrib.ox.ac.uk/; retrieved November 30, 2020).",1,0,0
10.1016/j.neuroimage.2022.119141,fsl.fmrib.ox.ac.uk/):,"Fiber tracing The ﬁber tracing procedure was performed on the neonatal and adult samples analogously using the probtrackx2 function from the FDT toolbox in the FMRIB Software Library (FSL version 6.0.3; fsl.fmrib.ox.ac.uk/): For each brain hemisphere independently using each ROI of a given ﬁber tract both as seed and target (e.g., ASS and AUD).",1,0,0
10.1016/j.neuroimage.2022.119141,github.com/ecr05/dhcp_template_alignment,"Note that for the initial steps of our registration procedure, we used publicly available shell scripts (see https://github.com/ecr05/dHCP_template_alignment ; retrieved November 30, 2020).",1,0,0
10.1016/j.neuroimage.2022.119141,developingconnectome.org/project/and,Code availability statement Both preprocessed and unpreprocessed neonatal and adult data can be freely accessed via registration to the Developing Human Connec-tome Project (dHCP); http://www.developingconnectome.org/project/and Human Connectome Project (HCP) Young Adult Database (https://www.humanconnectome.org/study/hcp-young-adult).,0,1,0
10.1016/j.neuroimage.2022.119141,developingconnectome.org,"We performed probabilistic tractography and compared connective probabilities between a sample of term-born neonates (N = 311; the Developing Human Connectome Project (dHCP, http://www.developingconnectome.org) and young adults (N = 311 The Human Connectome Project; https://www.humanconnectome.org/) by means of a classiﬁcation algorithm., Further information regarding inclusion and exclusion criteria can be found on the dHCP webpage (retrieved November 30, 2020, from http://www.developingconnectome.org/)., Code availability statement Both preprocessed and unpreprocessed neonatal and adult data can be freely accessed via registration to the Developing Human Connec-tome Project (dHCP); http://www.developingconnectome.org/project/and Human Connectome Project (HCP) Young Adult Database (https://www.humanconnectome.org/study/hcp-young-adult)., Ethics statement The Research Ethics Committee reference number: 14/LO/1169 of the UK Health Research Authority approved the study (Developing Human Connectome Project (dHCP); http://www.developingconnectome.org/project/).",0,1,0
10.1016/j.neuroimage.2022.119141,osf.io/49sgu,Bash and R scripts used in this study to perform imaging and statistical processing are publicly available at OSF (https://osf.io/49sgu/).,1,0,0
10.1016/j.neuroimage.2022.119141,developingconnectome.org,"Further information regarding inclusion and exclusion criteria can be found on the dHCP webpage (retrieved November 30, 2020, from http://www.developingconnectome.org/)., Code availability statement Both preprocessed and unpreprocessed neonatal and adult data can be freely accessed via registration to the Developing Human Connec-tome Project (dHCP); http://www.developingconnectome.org/project/and Human Connectome Project (HCP) Young Adult Database (https://www.humanconnectome.org/study/hcp-young-adult)., Ethics statement The Research Ethics Committee reference number: 14/LO/1169 of the UK Health Research Authority approved the study (Developing Human Connectome Project (dHCP); http://www.developingconnectome.org/project/).",0,1,0
10.1016/j.neuroimage.2022.119141,developingconnectome.org/project,"developingconnectome.org/project/), which in total includes more than 500 neonatal MRI data sets and is publicly accessible after registration., Ethics statement The Research Ethics Committee reference number: 14/LO/1169 of the UK Health Research Authority approved the study (Developing Human Connectome Project (dHCP); http://www.developingconnectome.org/project/).",0,0,1
10.1016/j.neuroimage.2022.119141,humanconnectome.org/study/hcp-young-adult,"humanconnectome.org/study/hcp-young-adult) S1200, which is pub-licly available for scientiﬁc research (Van Essen et al., 2013)., Code availability statement Both preprocessed and unpreprocessed neonatal and adult data can be freely accessed via registration to the Developing Human Connec-tome Project (dHCP); http://www.developingconnectome.org/project/and Human Connectome Project (HCP) Young Adult Database (https://www.humanconnectome.org/study/hcp-young-adult)., Louis approved the study HCP (https://www.humanconnectome.org/study/hcp-young-adult).",1,0,0
10.1016/j.neuroimage.2022.119141,humanconnectome.org/study/hcp-young-adult,"Code availability statement Both preprocessed and unpreprocessed neonatal and adult data can be freely accessed via registration to the Developing Human Connec-tome Project (dHCP); http://www.developingconnectome.org/project/and Human Connectome Project (HCP) Young Adult Database (https://www.humanconnectome.org/study/hcp-young-adult)., Louis approved the study HCP (https://www.humanconnectome.org/study/hcp-young-adult).",0,1,0
10.1016/j.neuroimage.2022.119141,developingconnectome.org/project,"Code availability statement Both preprocessed and unpreprocessed neonatal and adult data can be freely accessed via registration to the Developing Human Connec-tome Project (dHCP); http://www.developingconnectome.org/project/and Human Connectome Project (HCP) Young Adult Database (https://www.humanconnectome.org/study/hcp-young-adult)., Ethics statement The Research Ethics Committee reference number: 14/LO/1169 of the UK Health Research Authority approved the study (Developing Human Connectome Project (dHCP); http://www.developingconnectome.org/project/).",0,1,0
10.1016/j.neuroimage.2022.119141,humanconnectome.org,"We performed probabilistic tractography and compared connective probabilities between a sample of term-born neonates (N = 311; the Developing Human Connectome Project (dHCP, http://www.developingconnectome.org) and young adults (N = 311 The Human Connectome Project; https://www.humanconnectome.org/) by means of a classiﬁcation algorithm., To this purpose, we employed the Con-nectome Workbench software (version 1.2.3; retrieved November 30, 2020 from https://www.humanconnectome.org/software/connectome-workbench ; Marcus et al., 2011)., Code availability statement Both preprocessed and unpreprocessed neonatal and adult data can be freely accessed via registration to the Developing Human Connec-tome Project (dHCP); http://www.developingconnectome.org/project/and Human Connectome Project (HCP) Young Adult Database (https://www.humanconnectome.org/study/hcp-young-adult)., Louis approved the study HCP (https://www.humanconnectome.org/study/hcp-young-adult).",1,0,0
10.1016/j.neuroimage.2022.119141,humanconnectome.org/software/connectome-workbench,"To this purpose, we employed the Con-nectome Workbench software (version 1.2.3; retrieved November 30, 2020 from https://www.humanconnectome.org/software/connectome-workbench ; Marcus et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119141,hearing4all.de/en/h4a,JFQ was supported through research funds of the School of Medicine and Health Sciences at the Carl von Ossietzky Uni-versität Oldenburg and by the Hearing for All Centre of Excellence (https://hearing4all.de/en/h4a/).,0,0,1
10.1016/j.neuroimage.2022.119141,mirtk.github.io/index.html,"Initially, a rigid registration in the MIRTK software (retrieved November 30, 2020 from https://mirtk.github.io/index.html) was performed between each participant’s native T2w scan and the 40-week volumetric template pro-vided by the dHCP.",1,0,0
10.1016/j.neuroimage.2022.119456,github.com/chassall/averagetaskvalue,Task and analysis scripts are available at https://github.com/chassall/averagetaskvalue.,1,0,0
10.1016/j.neuroimage.2022.118895,github.com/bernspitz/convolution,(2016) freely available at https://github.com/bernspitz/convolution –models –MEEG.,1,0,0
10.1016/j.neuroimage.2022.118895,translationalneuromodeling.org/tapas,"The HGF was implemented with the open-source software in TAPAS http://www.translationalneuromodeling.org/tapas , version 3.1.0).",1,0,0
10.1016/j.neuroimage.2022.118895,osf.io/b4qkp,Preprocessed EEG and behavioural data ﬁles are available in the Open Science Framework Data Repository: https://osf.io/b4qkp/.,0,1,0
10.1016/j.neuroimage.2022.118895,ﬁl.ion.ucl.ac.uk/spm/version,"Following preprocessing, EEG continuous data were converted to SPM 12 (http://www.ﬁl.ion.ucl.ac.uk/spm/version 7487) downsampled to 256 Hz and time-frequency analysis was performed (Litvak et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119051,openneuro.org/datasets/ds000030,"Imaging data & preprocessing We utilised the open source dataset from the healthy controls of the UCLA Consortium for Neuropsychiatric Phenomics LA5c Study (Poldrack et al., 2016) (v00016 openneuro.org/datasets/ds000030/)., Our work used resting state fMRI data that was presented in a recent publication: (2) where we used the open-source toolkit DiCER https://github.com/BMHLab/DiCER.git data acquired from the UCLA Consortium for Neuropsychiatric Phenomics LA5c Study (3) (v00016 openneuro.org/datasets/ds000030/).",0,1,0
10.1016/j.neuroimage.2022.119051,github.com/kevinaquino/modelling_comparisons/with,"The dynamical equations are solved with MATLAB, using an Euler-Maruyama integration scheme (with time step 0.01 ms with the code available at http://github.com/KevinAquino/modelling_comparisons/with the parameters described in Table 3.",1,0,0
10.1016/j.neuroimage.2022.119051,github.com/breakspear/bdtoolkit.git,"Within this code, we made use of the open-source brain dynamics toolkit for the BTF model located at: https://github.com/breakspear/bdtoolkit.git.",1,0,0
10.1016/j.neuroimage.2022.119051,github.com/bmhlab/dicer.git,"The code to run all of these analyses is located at https://github.com/BMHLab/DiCER.git and spe-ciﬁc details of the algorithms are detailed in Aquino et al., Our work used resting state fMRI data that was presented in a recent publication: (2) where we used the open-source toolkit DiCER https://github.com/BMHLab/DiCER.git data acquired from the UCLA Consortium for Neuropsychiatric Phenomics LA5c Study (3) (v00016 openneuro.org/datasets/ds000030/).",0,1,0
10.1016/j.neuroimage.2022.119051,github.com/kevinaquino/modelling_comparisons,"All anal-ysis code used here is provided as an open source toolbox available at http://github.com/KevinAquino/modelling_comparisons/. 4 K.M., The algorithmic implementation can be found at http://github.com/KevinAquino/modelling_comparisons/., The dynamical equations are solved with MATLAB, using an Euler-Maruyama integration scheme (with time step 0.01 ms with the code available at http://github.com/KevinAquino/modelling_comparisons/with the parameters described in Table 3.",1,0,0
10.1016/j.neuroimage.2022.119051,github.com/kevinaquino/modelling_comparisons.git,Data and code availability statement All code for implementing computational models and reproducing our results is available at https://github.com/KevinAquino/modelling_comparisons.git.,1,0,0
10.1016/j.neuroimage.2022.119051,massive.org.au,This work was supported by the MASSIVE HPC facility (www.massive.org.au).,0,0,1
10.1016/j.neuroimage.2022.119051,github.com/bmhlab/dicer,"The code to run all of these analyses is located at https://github.com/BMHLab/DiCER.git and spe-ciﬁc details of the algorithms are detailed in Aquino et al., NeuroImage 256 (2022) 119051 which can be used to identify gradients of such a correlation across vox-els; and (2) an ordering based on hierarchical clustering (CO), as im-plemented in https://github.com/BMHLab/DiCER , which can be used to reveal additional, more complex WSDs (see Aquino et al., 2020 for details)., Our work used resting state fMRI data that was presented in a recent publication: (2) where we used the open-source toolkit DiCER https://github.com/BMHLab/DiCER.git data acquired from the UCLA Consortium for Neuropsychiatric Phenomics LA5c Study (3) (v00016 openneuro.org/datasets/ds000030/).",1,0,0
10.1016/j.neuroimage.2022.119051,bmhlab.github.io/dicer_results,A large collection of over 500 subjects of these vi-sualisations for the UCLA and other cohorts is available at: https://bmhlab.github.io/DiCER_results/.,0,1,0
10.1016/j.neuroimage.2022.119222,2.19.1.1,"We ﬁtted models using Hamiltonian Markov Chain Monte Carlo as im-plemented in Stan (Carpenter et al., 2017) via the PyStan interface (Stan Development Team, 2018 , Version 2.19.1.1).",1,0,0
10.1016/j.neuroimage.2022.119722,qhqmx.com.cn,"The stimuli were ﬁrst presented on a PC screen, then translated into braille by a screen reader (NonVisual Desktop Access, https://www.nvaccess.org/), and ﬁnally presented on a braille display device (THDZ-40, https://www.qhqmx.com.cn).",0,0,1
10.1016/j.neuroimage.2022.119722,nvaccess.org,"The stimuli were ﬁrst presented on a PC screen, then translated into braille by a screen reader (NonVisual Desktop Access, https://www.nvaccess.org/), and ﬁnally presented on a braille display device (THDZ-40, https://www.qhqmx.com.cn).",0,0,1
10.1016/j.neuroimage.2022.119722,rfmri.org,"Imaging data preprocessing For each child, DPABI 4.3 (http://rfmri.org/; Yan et al., 2016) was applied to perform the preprocessing of rs-fMRI data., The volume values were estimated through DPABI (http://rfmri.org/) (Yan et al., 2016) and SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm) in the present study and through Tensor Based Morphometry or FSL (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL ; Jenkinson et al., 2012 ; 9 J.",1,0,0
10.1016/j.neuroimage.2022.119722,ﬁl.ion.ucl.ac.uk/spm,"The DPARSF toolbox-based software Statistical Parametric Mapping 12 (SPM 12; http://www.ﬁl.ion.ucl.ac.uk/spm) in DPABI 4.3 was used for VBM preprocessing and subsequent analyses., The volume values were estimated through DPABI (http://rfmri.org/) (Yan et al., 2016) and SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm) in the present study and through Tensor Based Morphometry or FSL (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL ; Jenkinson et al., 2012 ; 9 J.",1,0,0
10.1016/j.neuroimage.2022.119722,nitrc.org/projects/chn-pd,"3) New segmentation and the Diﬀeomorphic Anatomical Registration Through Exponentiated Lie (DARTEL) algorithm (Ashburner, 2007) were used to coregister the functional images with the T1 images and then spa-tially normalize the images to the Chinese pediatric (CHN-PD) atlas (https://www.nitrc.org/projects/chn-pd/; Zhao et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119722,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fsl,"The volume values were estimated through DPABI (http://rfmri.org/) (Yan et al., 2016) and SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm) in the present study and through Tensor Based Morphometry or FSL (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL ; Jenkinson et al., 2012 ; 9 J.",1,0,0
10.1016/j.neuroimage.2022.119437,osf.io/6ykqh,Data and code availability The data of this study can be downloaded on the Open Science Framework at https://osf.io/6ykqh/.,0,1,0
10.1016/j.neuroimage.2022.119529,github.com/hmri-group/hmri-toolbox/releases/tag/errormaps,"Map creation and spatial processing Map creation and spatial processing was performed using modules in SPM12 version v7771 (Friston et al., 2006) and a branch of the hMRI toolbox (Tabelow et al., 2019) available here: https://github.com/hMRI-group/hMRI-toolbox/releases/tag/errormaps., Code availability The code to generate error and SNR maps, as well as robust MTsat , R1 , and PD maps as they were used in this paper is avail-able within the hMRI toolbox here: https://github.com/hMRI-group/hMRI-toolbox/releases/tag/errormaps.",1,0,0
10.1016/j.neuroimage.2022.119529,github.com/hmri-group/hmri-toolbox/wiki,An instruction of where to ﬁnd the error maps and how to generate the robust combination is given in Sup-plementary Materials S2 and will become available on the hMRI wiki page (https://github.com/hMRI-group/hMRI-toolbox/wiki).,0,0,1
10.1016/j.neuroimage.2022.119529,hmri.info,"Details of the derivation and the formulae of the er-rors for each parameter (𝑑 𝑅 1 , 𝑑 𝑃 𝐷, 𝑑 𝑀𝑇) can be found in Background Supplementary Materials B1–B3 and their implementation in the hMRI toolbox can be found here: www.hMRI.info., Note, that the error and mSNR maps will be part of the oﬃcial hMRI toolbox (www.hMRI.info) upon pub-lication.",0,0,1
10.1016/j.neuroimage.2022.119027,quspin.com,"The triaxial sensor The triaxial sensor (www.quspin.com ; Osborne et al., (2020)) is a sin-gle self-contained unit (Fig. 2 a) comprising: 1) a glass cell (3x 3x 3 mm 3) housing a mixture of 87 Rb vapour and buﬀer gas (nitrogen), 2) a single laser diode tuned to the D1 transition of 87 Rb (795-nm wavelength), with associated optics to circularly polarise the light generated, 3) two independent photodetectors and 4) three orthogonal pairs of electro-magnetic coils to control magnetic ﬁeld inside the cell.",1,0,0
10.1016/j.neuroimage.2022.119218,github.com/djangraw/parseeyelinkascfiles,"The recording ﬁles were parsed into an accessible format using the proprietary software EDF Converter (SR Research, version 4.0) and a modiﬁed version of the ParseEyeLinkAsc module for Python (https://github.com/djangraw/ParseEyeLinkAscFiles , code based on version 7/4/19).",1,0,0
10.1016/j.neuroimage.2022.119218,biorxiv.org/content/10.1101/2021.09.18.460905,The manuscript was uploaded to a preprint server (https://www.biorxiv.org/content/10.1101/2021.09.18.460905).,0,0,1
10.1016/j.neuroimage.2022.119218,osf.io/8zctp,"The dataset for statistical analysis, as well as R and python scripts replicating the results, and python scripts used in data collection were made available in a public online repos-itory (https://osf.io/8zctp).",0,1,0
10.1016/j.neuroimage.2022.119218,mne.tools/stable/auto_tutorials/time-freq/50_ssvep.html,The code created for computing ASSR SNR was also made publicly available as part of a tutorial in the MNE-Python documentation; https://mne.tools/stable/auto_tutorials/time-freq/50_ssvep.html.,1,0,0
10.1016/j.neuroimage.2022.119218,1.11.0.0,"Connection between the experimenter PC and the EyeLink recording system was established via ethernet, and controlled using the PyLink Python module (SR Research Ltd., version 1.11.0.0) Epoched recordings were made for each trial, with a sampling frequency of 500 Hz.",1,0,0
10.1016/j.neuroimage.2022.119218,osf.io/bkep4,"Methods The experimental design and our hypotheses were preregistered (see https://osf.io/bkep4)., Data and code availability The experimental design and hypotheses reported here were preregistered (https://osf.io/bkep4).",1,0,0
10.1016/j.neuroimage.2022.119218,inkscape.org,"Local regression in Fig. 2 c was generated using locally weighted scat-terplot smoothing (LOWESS) as implemented in the seaborn module for Python (Waskom, 2021 , version 0.11.1) Visualization Data visualizations were generated using the seaborn Python mod-ule (Waskom, 2021 , version 0.11.1) with custom post-processing using Inkscape (https://inkscape.org).",1,0,0
10.1016/j.neuroimage.2022.119493,nimh.nih.gov,"nimh.nih.gov) (Cox, 1996), SUMA (AFNI surface mapper), FreeSurfer (https://surfer.nmr.mgh.harvard.edu/), and custom MATLAB scripts.",1,0,0
10.1016/j.neuroimage.2022.119493,surfer.nmr.mgh.harvard.edu,"nimh.nih.gov) (Cox, 1996), SUMA (AFNI surface mapper), FreeSurfer (https://surfer.nmr.mgh.harvard.edu/), and custom MATLAB scripts.",1,0,0
10.1016/j.neuroimage.2022.119335,datalad.org,Data and code availability statement The data of the present study will be made available in the data por-tal (https://www.datalad.org/) with a persistent data identiﬁer (DOI).,0,1,0
10.1016/j.neuroimage.2022.119220,math.mcgill.ca/keith/surfstat,"Group contrasts and behavioural associations were conducted on the rotated gradients using surface-based linear models, as implemented in the Surf-Stat toolbox (http://www.math.mcgill.ca/keith/surfstat ; Worsley et al., 2009).",1,0,0
10.1016/j.neuroimage.2022.119220,surfer.nmr.mgh.harvard.edu,Structural T1w images were processed using Freesurfer v5.3 (http://surfer.nmr.mgh.harvard.edu/).,1,0,0
10.1016/j.neuroimage.2022.119220,openneuro.org/datasets/ds003059/versions/1.0.0,Data is freely available at https://openneuro.org/datasets/ds003059/versions/1.0.0.,0,1,0
10.1016/j.neuroimage.2022.119220,github.com/mica-mni/brainspace,"Cortical gradients were computed using the BrainSpace toolbox (https://github.com/MICA-MNI/BrainSpace ; de Wael et al., 2020) as implemented in MATLAB.",1,0,0
10.1016/j.neuroimage.2022.119220,imperial.ac.uk/psychedelic-research-center,RLC –H is supported by the Alex Mosley Charitable Trust and supporters of the center for Psychedelic Re-search: https://www.imperial.ac.uk/psychedelic-research-center.,0,0,1
10.1016/j.neuroimage.2022.119206,search.kg.ebrains.eu/instances/cd4c0231-f9d6-4964-9763-54347029dd00,"Data availability statement Processed imaging data are available on the EBRAINS platform: https://search.kg.ebrains.eu/instances/cd4c0231-f9d6-4964-9763-54347029dd00., Data availability statement Processed imaging data are available on the EBRAINS platform: https://search.kg.ebrains.eu/instances/cd4c0231-f9d6-4964-9763-54347029dd00.",0,1,0
10.1016/j.neuroimage.2022.119206,ﬁl.ion.ucl.ac.uk/spm,"The functional data were preprocessed and analyzed with Sta-tistical Parametric Mapping software SPM12 (Wellcome Depart-ment of Imaging Neuroscience, University College London, UK; http://www.ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2021.118830,github.com/dmripreprocessing/neuroimage-review-2022,"As such, we endeavour to facilitate the navigation through the breadth of tools and software packages avail-able, as well as keeping an up-to-date overview of dMRI preprocessing developments (temporary link: https://github.com/dmripreprocessing/neuroimage-review-2022).",0,0,1
10.1016/j.neuroimage.2021.118830,gdpr-info.eu,"Likewise, making data publicly available for benchmarking is challenged by privacy con-siderations –in the European Union for example regulated by the Gen-eral Data Protection Regulation (GDPR, https://gdpr-info.eu/) –and the potential of data containing commercially sensitive information such as gradient system speciﬁcations (see e.g.",0,0,1
10.1016/j.neuroimage.2021.118830,na-mic.org/wiki/namic_wiki:dti:nrrd_format,"Nearly Raw Raster Data (NRRD, https://www.na-mic.org/wiki/NAMIC_Wiki:DTI:Nrrd_format) or the Neuroimaging In-formatics Technology Initiative (NIfTI, https://nifti.nimh.nih.gov/) ﬁle formats, of which the latter is the most commonly used.",0,1,0
10.1016/j.neuroimage.2021.118830,bitbucket.org/oriadev/qunex/wiki/home,"NeuroImage 249 (2022) 118830 connectome), dMRIPrep (https://github.com/nipreps/dmriprep), and QuNex (https://bitbucket.org/oriadev/qunex/wiki/Home)., BIDS Apps (Gorgolewski et al., 2017) are versioned container images that take BIDS formatted datasets as input and sev-eral BIDS-compatible tools have been developed for dMRI preprocess-ing, including Tractoﬂow (Theaud et al., 2020), QSIPrep (Cieslak et al., 2020), MRtrix3_connectome (https://github.com/BIDS-Apps/MRtrix3_connectome), dMRIPrep (https://github.com/nipreps/dmriprep), and QuNex (https://bitbucket.org/oriadev/qunex/wiki/Home).",0,0,1
10.1016/j.neuroimage.2021.118830,nifti.nimh.nih.gov,"Nearly Raw Raster Data (NRRD, https://www.na-mic.org/wiki/NAMIC_Wiki:DTI:Nrrd_format) or the Neuroimaging In-formatics Technology Initiative (NIfTI, https://nifti.nimh.nih.gov/) ﬁle formats, of which the latter is the most commonly used.",0,1,0
10.1016/j.neuroimage.2021.118830,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,The HCP provides test-retest data on 46 subjects (https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release).,0,1,0
10.1016/j.neuroimage.2021.118830,github.com/bids-apps/mrtrix3_connectome,"This can be in the form of Bash, Python, or MATLAB scripts, as for exam-ple adopted in the DESIGNER pipeline (Ades-Aron et al., 2018), Pre-Qual (Cai et al., 2021b), and MRtrix3_connectome (https://github.com/BIDS-Apps/MRtrix3_connectome)., BIDS Apps (Gorgolewski et al., 2017) are versioned container images that take BIDS formatted datasets as input and sev-eral BIDS-compatible tools have been developed for dMRI preprocess-ing, including Tractoﬂow (Theaud et al., 2020), QSIPrep (Cieslak et al., 2020), MRtrix3_connectome (https://github.com/BIDS-Apps/MRtrix3_connectome), dMRIPrep (https://github.com/nipreps/dmriprep), and QuNex (https://bitbucket.org/oriadev/qunex/wiki/Home).",1,0,0
10.1016/j.neuroimage.2021.118830,cai2r.net,"Research was performed as part of the Center of Advanced Imaging Innovation and Research (CAI2R, www.cai2r.net), an NIBIB Biomedical Technology Resource Center (NIH P41 EB017183) and was partially supported by the NINDS of the NIH (R01 NS088040).",0,0,1
10.1016/j.neuroimage.2021.118830,github.com/nipreps/dmriprep,"QSIPrep (Cieslak et al., 2020) and dMRIPrep (https://github.com/nipreps/dmriprep) are based on Nipype., NeuroImage 249 (2022) 118830 connectome), dMRIPrep (https://github.com/nipreps/dmriprep), and QuNex (https://bitbucket.org/oriadev/qunex/wiki/Home)., BIDS Apps (Gorgolewski et al., 2017) are versioned container images that take BIDS formatted datasets as input and sev-eral BIDS-compatible tools have been developed for dMRI preprocess-ing, including Tractoﬂow (Theaud et al., 2020), QSIPrep (Cieslak et al., 2020), MRtrix3_connectome (https://github.com/BIDS-Apps/MRtrix3_connectome), dMRIPrep (https://github.com/nipreps/dmriprep), and QuNex (https://bitbucket.org/oriadev/qunex/wiki/Home).",0,0,1
10.1016/j.neuroimage.2021.118830,github.com/bids-apps/mrtrix3_23,"Other frameworks have also been made available as container-ized version, such as PreQual (Cai et al., 2021b), QSIPrep (Cieslak et al., 2020), MRtrix3_connectome (https://github.com/BIDS-Apps/MRtrix3_23 C.M.W.",1,0,0
10.1016/j.neuroimage.2021.118848,ebrains.eu,"Such data driven eﬀorts in personalized computational full brain network modeling have been largely made possible by the availability of open source neuroinformatics platforms such as The Virtual Brain (TVB) (Sanz Leon et al., 2013) and the large-scale digital neuroscience research in-frastructure EBRAINS (https://www.ebrains.eu)., Patient-nonspeciﬁc eﬀorts rely on postmortem data includ-ing neurotransmitter distributions and high-resolution connectome data, which are integrated in a shared brain data space (see for instance Human Brain Project (htpps://www.humanbrainproject.eu) and EBRAINS (https://www.ebrains.eu)).",0,0,1
10.1016/j.neuroimage.2021.118848,htpps://humanbrainproject.eu,"Patient-nonspeciﬁc eﬀorts rely on postmortem data includ-ing neurotransmitter distributions and high-resolution connectome data, which are integrated in a shared brain data space (see for instance Human Brain Project (htpps://www.humanbrainproject.eu) and EBRAINS (https://www.ebrains.eu)).",0,1,0
10.1016/j.neuroimage.2021.118848,clinicaltrials.gov,Ethics statement Data were acquired using protocols approved by University of Cal-gary Ethics Review Board and all participants provided informed con-sent (Clinicaltrials.gov NCT01983904).,0,0,1
10.1016/j.neuroimage.2021.118848,github.com/ins-amu/an_2021_dbs_trd_sim,The source codes for brain network simulations can be found here: http://github.com/ins-amu/An_2021_DBS_TRD_sim.,1,0,0
10.1016/j.neuroimage.2021.118848,humanbrainproject.eu/en/follow-hbp/news/new-ebrains-enabled-tool-to-help-guide-surgery-in-drug-resistant-epilepsy-patients,"The inclusion of such data in virtual brain modeling eﬀorts has been demonstrated to improve the model’s individual predictive power in animal studies (using Allen connectome derived from tracer data (Melozzi et al., 2019)) and in early demonstrators of the Virtual Big Brain (i.e., high-resolution TVB) for epilepsy applications (Wang et al., 2021 ; see also https://www.humanbrainproject.eu/en/follow-hbp/news/new-ebrains-enabled-tool-to-help-guide-surgery-in-drug-resistant-epilepsy-patients/).",0,0,1
10.1016/j.neuroimage.2021.118812,github.com/huggingface/pytorch-pretrained-biggan,The Py-Torch version of the pretrained model can be publicly downloaded at https://github.com/huggingface/pytorch-pretrained-BigGAN.,1,0,0
10.1016/j.neuroimage.2021.118812,naturalscenesdataset.org,"Natural scenes data set We used the Natural Scenes Dataset (NSD; http://naturalscenesdataset.org (Allen, 2021) to train the encoding model., Data availability The NSD dataset is publicly available at http://naturalscenesdataset.org.",0,1,0
10.1016/j.neuroimage.2021.118812,github.com/zijin-gu/neurogen,Code availability Code is available at https://github.com/zijin-gu/NeuroGen.,1,0,0
10.1016/j.neuroimage.2022.119042,nitrc.org/projects/pythagoras,"Head position vectors (three rotation and three trans-lation), obtained using SPM12 motion correction, were transformed into a pair of nuisance regressors for head motion based on the Pythagorean Theorem, and ﬁrst order derivatives were calculated for head motion across BOLD contrast images (Kuchinsky et al., 2012 ; Wilke, 2012 ; http://www.nitrc.org/projects/pythagoras).",1,0,0
10.1016/j.neuroimage.2022.119148,ﬁl.ion.ucl.ac.uk,"VBM analysis Voxel-based morphometry was performed using CAT12 (Jena Uni-versity Hospital, Departments of Psychiatry and Neurology, Jena, Ger-many, www.neuro.uni-jena.de) and SPM12 toolbox (Wellcome Depart-ment of Cognitive Neurology, London www.ﬁl.ion.ucl.ac.uk), imple-mented in MATLAB 2016 (Mathworks Inc., Sherborn, MA, USA).",1,0,0
10.1016/j.neuroimage.2022.119148,processmacro.org,"PROCESS is an ordinary least squares path analysis modeling tool based on SPSS (www.processmacro.org), which can estimate indirect eﬀects and direct eﬀects in mediation mod-els.",1,0,0
10.1016/j.neuroimage.2022.119148,github.com/jdiedrichsen/suit,"To provide better localization of the observed cerebellar cluster, the Spatially Unbiased Infratentorial Toolbox (SUIT, version 3.4, github.com/jdiedrichsen/suit/) was used to isolate the cere-bellum from the rest of brain (Diedrichsen, 2006 ; Diedrichsen and Zo-tow, 2015).",1,0,0
10.1016/j.neuroimage.2022.119148,ac.uk/spm,"ac.uk/spm/) and ASLtbx (cfn.upenn.edu/perfusion/software.htm) (Wang et al., 2008).",1,0,0
10.1016/j.neuroimage.2022.119148,cfn.upenn.edu/perfusion/software.htm,"ac.uk/spm/) and ASLtbx (cfn.upenn.edu/perfusion/software.htm) (Wang et al., 2008).",1,0,0
10.1016/j.neuroimage.2022.119148,neuro.uni-jena.de,"VBM analysis Voxel-based morphometry was performed using CAT12 (Jena Uni-versity Hospital, Departments of Psychiatry and Neurology, Jena, Ger-many, www.neuro.uni-jena.de) and SPM12 toolbox (Wellcome Depart-ment of Cognitive Neurology, London www.ﬁl.ion.ucl.ac.uk), imple-mented in MATLAB 2016 (Mathworks Inc., Sherborn, MA, USA).",1,0,0
10.1016/j.neuroimage.2022.119148,ccs-lab.github.io/hbayesdm,"We estimated the model parameters at the individual level with the hierarchical Bayesian analysis implemented in the hBayesDM (version 1.0.2) R package (ccs-lab.github.io/hBayesDM/) for computational modeling of BART data (Ahn et al., 2017 ; Wallsten et al., 2005).",1,0,0
10.1016/j.neuroimage.2022.118894,solar-eclipse-genetics.org/index.html,Data and code availability All data and codes used in our study were obtained from the HCP website (http://www.humanconnectomeproject.org/) and SOLAR soft-ware website (http://solar-eclipse-genetics.org/index.html).,0,1,0
10.1016/j.neuroimage.2022.118894,healthmeasures.net/explore-measurement-systems/promis,"Moreover, the NIH Toolbox Emotion Battery shows a close relationship with other large NIH ini-tiatives, such as the Patient-Reported Outcomes Measurement Informa-tion System (PROMIS; www.healthmeasures.net/explore-measurement-systems/promis) and Quality of Life Outcomes in Neurological Disorders (www.neuro-qol.org).",0,0,1
10.1016/j.neuroimage.2022.118894,surfer.nmr.mgh.harvard.edu,Subcor-tical volume estimates were extracted using automatic segmentation in FreeSurfer 5.3.0 (https://surfer.nmr.mgh.harvard.edu/).,1,0,0
10.1016/j.neuroimage.2022.118894,neuro-qol.org,"Moreover, the NIH Toolbox Emotion Battery shows a close relationship with other large NIH ini-tiatives, such as the Patient-Reported Outcomes Measurement Informa-tion System (PROMIS; www.healthmeasures.net/explore-measurement-systems/promis) and Quality of Life Outcomes in Neurological Disorders (www.neuro-qol.org).",0,0,1
10.1016/j.neuroimage.2022.118894,humanconnectomeproject.org,"Participants We used the Human Connectome Project (HCP) S1200 release dataset (http://www.humanconnectomeproject.org/), which consists of 1206 healthy individuals (MZ = 298, DZ = 188, Not Twin = 720)., Data and code availability All data and codes used in our study were obtained from the HCP website (http://www.humanconnectomeproject.org/) and SOLAR soft-ware website (http://solar-eclipse-genetics.org/index.html).",0,1,0
10.1016/j.neuroimage.2022.118908,github.com/danieladamspencer/bayesglm_validation,"Mejia: Conceptualization, Method-ology, Software, Investigation, Resources, Data curation, Writing –orig-inal draft, Writing –review & editing, Supervision, Project administra-tion, Funding acquisition. 4 https://github.com/danieladamspencer/BayesGLM_Validation Acknowledgments Data were provided in part by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research ; and by the McDonnell Center for Systems Neuroscience at Washington University.",1,0,0
10.1016/j.neuroimage.2022.118908,humanconnectome.org/software/workbench-command/-cifti-smoothing,The 1 https://www.humanconnectome.org/software/connectome-workbench 2 https://www.humanconnectome.org/software/workbench-command/-cifti-smoothing 5 D.,1,0,0
10.1016/j.neuroimage.2022.118908,github.com/mandymejia/bayesfmri/tree/1.8.1,"Here, we assess the 3 https://github.com/mandymejia/BayesfMRI/tree/1.8.1 6 D.",1,0,0
10.1016/j.neuroimage.2022.118908,humanconnectome.org/software/connectome-workbench,The 1 https://www.humanconnectome.org/software/connectome-workbench 2 https://www.humanconnectome.org/software/workbench-command/-cifti-smoothing 5 D.,1,0,0
10.1016/j.neuroimage.2022.119007,github.com/donders-institute/multiecho,"This was done according to one of the three main methods of computing echo weights used within the Donders Center for Cognitive Neuroimaging (for more details, please ﬁnd all the details of the method ‘PAID’ at: https://github.com/Donders-Institute/multiecho).",1,0,0
10.1016/j.neuroimage.2022.119007,lotteries.as,"There was no eﬀect of sources of un-certainty: participants do not alter their transferred amounts between the Trust Games and the lotteries.As participants make repeated choices within each of the four conditions, the standard error bars in this Figure represent within-subject error bars and are calculated based on Morey (2008).",0,0,1
10.1016/j.neuroimage.2022.119680,github.com/wdika/mridc,"For testing generalizability, we used 1 https://github.com/wdika/mridc. 4 C.",1,0,0
10.1016/j.neuroimage.2022.119680,github.com/chaopingzhang/qrim,"For all net-works that are used in this work, the training lasted until their training loss was stabilized. 2 https://doi.org/10.34894/IHZGQM. 3 https://github.com/chaopingzhang/qRIM.",1,0,0
10.1016/j.neuroimage.2022.119644,biorender.com,Created with Biorender.com. 3 E.D.,1,0,0
10.1016/j.neuroimage.2022.119091,ftp://imaging.wustl.edu/pub/raichlab/4dfp_tools,"rs-fMRI data were preprocessed through a standard neonatal BOLD preprocessing pipeline using a combination of the 4dfp tool suite (ftp://imaging.wustl.edu/pub/raichlab/4dfp_tools/; Shulman et al., 2010) and FSL tools (Jenkinson et al., 2012)., Evaluation of synthetic images in fMRI analyses The 4dfp tool suite (ftp://imaging.wustl.edu/pub/raichlab/4dfp_tools/; Shulman et al., 2010) was used to compute linear registrations between BOLD and anatomical data (T2w and pseudo-T2w images) to 711–2N Talairach atlas space.",1,0,0
10.1016/j.neuroimage.2022.119595,j.va,J.vA.,0,0,1
10.1016/j.neuroimage.2022.119595,meguk.ac.uk/contact,Access to the MEG UK database can be requested at http://meguk.ac.uk/contact.,0,1,0
10.1016/j.neuroimage.2022.119595,github.com/ohba-analysis/hmm-mar,This is because the future value of a hidden logit is determined by a long sequence of 5 https://github.com/OHBA-analysis/HMM-MAR.,1,0,0
10.1016/j.neuroimage.2022.119595,github.com/ohba-analysis/osl-dynamics,An implementation of DyNeMo written in Python can be accessed here: https://github.com/OHBA-analysis/osl-dynamics.,1,0,0
10.1016/j.neuroimage.2022.119383,mek.oszk.hu,The sentences were selected from the Hungarian Electronic Library (https://mek.oszk.hu).,0,0,1
10.1016/j.neuroimage.2022.119217,2.3.3.2,2.3.3.2.,0,0,1
10.1016/j.neuroimage.2022.119217,audition.ens.fr/adc/noisetools,"EEG data were pre-processed using EEGLAB (Delorme and Makeig, 2004), FieldTrip (Oostenveld et al., 2011), NoiseTools (http://audition.ens.fr/adc/NoiseTools/), the mTRF Tool-box (Crosse et al., 2016) and custom scripts in MATLAB R2019a (The Mathworks, Inc).",1,0,0
10.1016/j.neuroimage.2022.119217,2.3.3.1,EEG measure 2.3.3.1.,0,0,1
10.1016/j.neuroimage.2022.119041,fsl.fmrib.ox.ac.uk/fsl/fslwiki/randomise,"The test was conducted with the FSL randomize package (version v2.9, http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Randomise) with 5000 random sign-ﬂips, and clusters with a size higher than 95% of the maximal supra-threshold clusters in the permutation distribution were then reported.",1,0,0
10.1016/j.neuroimage.2022.119041,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"fMRI preprocessing BOLD images of each scanning session were preprocessed inde-pendently using FSL FEAT (FMRIB’s Software Library, version 6.00, https://fsl.fmrib.ox.ac.uk/fsl/fslwiki ; Woolrich et al., 2001 , 2004).",1,0,0
10.1016/j.neuroimage.2022.119041,cran.r-project.org/web/packages/bayesfactor/index.html,"For Bayesian analyses, the median and 95% credible interval were also calculated from the posterior distribution generated by 10,000 iterations using the BayesFactor package in R (v0.9.12–4.3; available at: https://cran.r-project.org/web/packages/BayesFactor/index.html, see Supplementary Table.2).",1,0,0
10.1016/j.neuroimage.2022.119041,itksnap.org,"For ROI analysis, we manually delineated each of the medial temporal lobe (MTL) subareas (hippocampus HPC, parahippocampal cortex PHC, perirhinal cortex PRC, and entorhinal cortex ERC) on the partici-pant’s native space using established protocols (Insausti et al., 1998 ; Pruessner et al., 2000 , 2002 ; Duvernoy, 2005) as well as the delineating software ITK-SNAP (www.itksnap.org).",1,0,0
10.1016/j.neuroimage.2022.119041,mne.tools/stable/generated/mne.stats.spatio_temporal_cluster_1samp_test.html,"In the MEG contrast analyses, cluster-based permutation tests were performed using the built-in package of MNE (v0.24; available at: https://mne.tools/stable/generated/mne.stats.spatio_temporal_cluster_1samp_test.html).",1,0,0
10.1016/j.neuroimage.2022.119041,mne.tools/stable/index.html,"MEG preprocessing Raw signals from the MEG experiment were preprocessed and analyzed using the MNE Python toolbox (v0.19; available at: https://mne.tools/stable/index.html) (Gramfort et al., 2013 , 2014).",1,0,0
10.1016/j.neuroimage.2022.119041,scidb.cn/en/detail?,Data and code availability statement The data that support the ﬁndings of this study can be ac-cessed from the data repository https://www.scidb.cn/en/detail?,0,1,0
10.1016/j.neuroimage.2022.119041,github.co,"The code written in Python (version 3.3.8), Matlab (version 2019b), MNE (version 0.23), R (version 4.0.2), and BNU bash (version 3.2.57) that support the analysis of this study are available at https://github.co m/Z HANGneuro/Data-code-for-https-doi.org-10.1101–2020.07.22.215517.",1,0,0
10.1016/j.neuroimage.2022.119041,mixamo.com,"Three animated human characters (Mixamo, San Francisco, https://www.mixamo.com) 2 B.",0,0,1
10.1016/j.neuroimage.2022.118981,mne.tools/stable/index.html,"MNE-C and MNE-python (https://mne.tools/stable/index.html) (Gramfort et al., 2013 , 2014) were used for MEG data analysis.",1,0,0
10.1016/j.neuroimage.2022.118981,surfer.nmr.mgh.harvard.edu,fMRI data FS-FAST in FreeSurer (https://surfer.nmr.mgh.harvard.edu) was used for the fMRI image preprocessing and statistical analysis.,0,1,0
10.1016/j.neuroimage.2022.119123,gin.g-node.org/usz_nch/human_mtl_units_visual_wm/ethics,"Data and code availability All data and code needed to evaluate the conclu-sions in the paper are available in G-Node: https://gin.g-node.org/USZ_NCH/Human_MTL_units_visual_WM/Ethics statement The study was approved by the institutional ethics review board (Kantonale Ethikkommission Zürich, PB-2016-02055) and all partici-pants gave written informed consent.",0,1,0
10.1016/j.neuroimage.2022.119123,adtechmedical.com,"The participants were implanted in the MTL with commercially available depth electrodes (1.3 mm diameter, 8 contacts of 1.6 mm length, spacing between contact centers 5 mm, ADTech®, Racine, WI, www.adtechmedical.com).",0,0,1
10.1016/j.neuroimage.2022.119123,neuralynx.com,"Recordings were done at 32 kHz via the ATLAS record-ing system (1–8000 Hz passband, Neuralynx ®, Bozeman MT, USA, www.neuralynx.com).",1,0,0
10.1016/j.neuroimage.2022.119123,github.com/jniediek/combinato,"We performed spike sorting with the package Combinato (https://github.com/jniediek/combinato , Niediek et al.",1,0,0
10.1016/j.neuroimage.2022.118874,r-project.org,Statistical analyses All statistical analyses were completed in R (https://www.r-project.org).,1,0,0
10.1016/j.neuroimage.2022.119009,sites.google.com/site/bctnet/home,"All topological properties of the networks were calculated with the Brain Connectivity Toolbox (https://sites.google.com/site/bctnet/Home ; Rubinov & Sporns, 2010) in MATLAB R2014a. 4 H.",1,0,0
10.1016/j.neuroimage.2022.119009,harvard.edu,"harvard.edu/) in terms of the standard procedure described by the de-velopers (Dale, Fischl, & Sereno, 1999 ; Desikan et al., 2006 ; Fischl et al., 2002 ; Fischl, Sereno, & Dale, 1999).",0,0,1
10.1016/j.neuroimage.2022.119009,mathworks.com/matlabcentral/ﬁleexchange/14034-kernel-density-estimator,"This step was run with a kernel density estima-tor toolbox in MATLAB (https://www.mathworks.com/matlabcentral/ﬁleexchange/14034-kernel-density-estimator), which uses a Gaussian kernel by default and automatically chooses the optimal bandwidth (Botev et al., 2010).",1,0,0
10.1016/j.neuroimage.2022.119009,rotman-baycrest.on.ca/index.php?section,"A multivariate statistical method named PLSC (Krishnan et al., 2011 ; McIntosh & Lobaugh, 2004), which has been applied in numerous neuroimaging studies (Jessen et al., 2019 ; McIntosh & Lobaugh, 2004 ; Spreng et al., 2019 ; Sullivan, Anderson, Turner, Spreng, & aging, 2019), was performed using a PLS toolbox (https://www.rotman-baycrest.on.ca/index.php?section = 84) in MAT-LAB R2014a.",1,0,0
10.1016/j.neuroimage.2021.118851,humanconnectome.org/study/hcp-young-adult/document/500-subjects-data-release,"Moreover, the EEG and MEG working memory and motor imagery datasets that we used for as-sessing the generalizability of the methods are available from Collab-orative Research in Computational Neuroscience (CRCNS) data shar-ing (http://crcns.org/data-sets/pfc/pfc-5/about-pfc-5), Human Connec-tome Project (https://www.humanconnectome.org/study/hcp-young-adult/document/500-subjects-data-release), and Berlin Brain-Computer Interface websites (http://www.bbci.de/competition/iv/desc_1.html) respectively.",0,1,0
10.1016/j.neuroimage.2021.118851,crcns.org/data-sets/pfc/pfc-5/about-pfc-5,"Moreover, the EEG and MEG working memory and motor imagery datasets that we used for as-sessing the generalizability of the methods are available from Collab-orative Research in Computational Neuroscience (CRCNS) data shar-ing (http://crcns.org/data-sets/pfc/pfc-5/about-pfc-5), Human Connec-tome Project (https://www.humanconnectome.org/study/hcp-young-adult/document/500-subjects-data-release), and Berlin Brain-Computer Interface websites (http://www.bbci.de/competition/iv/desc_1.html) respectively.",0,1,0
10.1016/j.neuroimage.2021.118851,bbci.de/competition/iv/desc_1.html,"Moreover, the EEG and MEG working memory and motor imagery datasets that we used for as-sessing the generalizability of the methods are available from Collab-orative Research in Computational Neuroscience (CRCNS) data shar-ing (http://crcns.org/data-sets/pfc/pfc-5/about-pfc-5), Human Connec-tome Project (https://www.humanconnectome.org/study/hcp-young-adult/document/500-subjects-data-release), and Berlin Brain-Computer Interface websites (http://www.bbci.de/competition/iv/desc_1.html) respectively.",0,1,0
10.1016/j.neuroimage.2021.118847,jmrui.eu,"Spectral analysis of metabolites Spectral preprocessing, analysis, and relative metabolite quantiﬁca-tion was performed with the Java-based Magnetic Resonance User Inter-face (jMRUI) version 5.0 (http://www.jmrui.eu/; Naressi et al., 2001).",1,0,0
10.1016/j.neuroimage.2022.119444,github.com/mungomeng/registration-aan,"(9)) 1 https://github.com/MungoMeng/Registration-AAN. 6 M., The code associated with this study is available at the GitHub repos-itory: https://github.com/MungoMeng/Registration-AAN.",1,0,0
10.1016/j.neuroimage.2022.119444,nitrc.org/projects/ibsr,"Available at https://www.nitrc.org/projects/ibsr. 6 Klein, A., Tourville, J., 2012. 101 labeled brain images and a consis-tent human cortical labeling protocol.",0,0,1
10.1016/j.neuroimage.2022.119444,brain-development.org/ixi-dataset,Available at https://brain-development.org/ixi-dataset/. 5 Internet Brain Segmentation Repository (IBSR).,0,1,0
10.1016/j.neuroimage.2021.118742,nordicneurolab.com,"Following each TS, subjects had a total of 20 s to rate their perceived pain of the TS and CS (10 s each) on a NRS projected on the NordicNeuroLab 32 ″ screen (NordicNeuroLab, Norway and USA, https://www.nordicneurolab.com) using a manual response unit placed in their dominant hand., During this time, sub-jects were instructed to relax, remain awake with eyes ﬁxating on a mo-tionless cross, projected on a NordicNeuroLab 32 ″ screen (NordicNeuro-Lab, Norway and USA, https://www.nordicneurolab.com).",0,0,1
10.1016/j.neuroimage.2021.118742,clinicaltrial.gov,"The study has been approved by the lo-cal ethics board ‘Kantonale Ethikkommission Zürich, KEK’ (EK-04/2006, PB_2016–02,051, clinicaltrial.gov number: NCT02138344)., Ethics approval All procedures described in this study were in accordance with the Declaration of Helsinki and have been approved by the local ethics board ‘Kantonale Ethikkommission Zürich, KEK’ (EK-04/2006, PB_2016–02051, clinicaltrial.gov number: NCT02138344).",0,0,1
10.1016/j.neuroimage.2021.118742,nitrc.org/projects/conn)(whitﬁeld-gabrieli,"Seed-to-voxel resting-state functional connectivity analysis Resting-state functional MRI data was analysed with the CONN tool-box (CONN 18b; www.nitrc.org/projects/conn)(Whitﬁeld-Gabrieli and Nieto-Castanon, 2012).",1,0,0
10.1016/j.neuroimage.2021.118742,ﬁl.ion.ucl.ac.uk/spm,"Pre-processing for neuroimaging analysis Both raw structural T1w images and resting-state functional images were pre-processed using Statistical Parametric Mapping (SPM12) software (Statistical parametric mapping; Wellcome De-partment of Imaging Neuroscience, London, United Kingdom: (http://www.ﬁl.ion.ucl.ac.uk/spm/) implemented in MATLAB 2017a (The Mathworks, Inc, Natick, MA).",1,0,0
10.1016/j.neuroimage.2022.119155,neurosynth.org,"Although it is possible to use existing databases to derive re-gion of interest predictions (e.g., neurosynth.org, NiMARE, and Neuro-Query), these databases are not speciﬁc to social processes and social psychological terms may not be well speciﬁed.",0,1,0
10.1016/j.neuroimage.2022.119155,osf.io/pydbk,Aggregated data and analysis scripts that support the ﬁndings have been deposited in OSF at https://osf.io/pydbk/.,1,0,0
10.1016/j.neuroimage.2022.119613,nitrc.org/projects/mricrogl,The brain map was visualized us-ing MRIcroGL (https://www.nitrc.org/projects/mricrogl/) and Mango (ric.uthscsa.edu/mango).,1,0,0
10.1016/j.neuroimage.2022.119613,nih.gov,nih.gov/).,0,0,1
10.1016/j.neuroimage.2022.119613,ric.uthscsa.edu/mango,The brain map was visualized us-ing MRIcroGL (https://www.nitrc.org/projects/mricrogl/) and Mango (ric.uthscsa.edu/mango).,1,0,0
10.1016/j.neuroimage.2022.119613,neurosynth.org,"Furthermore, using meta-analytic maps generated by Neurosynth (https://neurosynth.org), we were able to compare the average response consistency for dif-ferent functional networks across same-task and diﬀerence-task con-ditions.",1,0,0
10.1016/j.neuroimage.2022.119613,neurosynth.org,"Meta-Analysis We performed automatic meta-analyses using Neurosynth (Yarkoni et al., 2011 , www.neurosynth.org) to further explore the cognitive functions of the cortical regions and networks identiﬁed in our fMRI experiment.",1,0,0
10.1016/j.neuroimage.2022.119026,db.humanconnectome.org,"The Group ICA parcellations and cor-responding time series are publicly available from the HCP repository (https://db.humanconnectome.org)., Data and code availability statement The real dataset used in this study has been made pub-licly available by the WU-Minn Consortium in ConnectomeDB https://db.humanconnectome.org/.",0,1,0
10.1016/j.neuroimage.2022.119026,github.com/ohba-analysis/hmm-mar,"We implemented the model using the HMM-MAR toolbox avail-able at https://github.com/OHBA-analysis/HMM-MAR in MATLAB (The Mathworks Inc, 2016)., The HMM-MAR toolbox used to run the HMMs is available under https://github.com/OHBA-analysis/HMM-MAR.",1,0,0
10.1016/j.neuroimage.2022.119026,github.com/ahrends/mixing,Code to reproduce the simulated dataset and to replicate the analyses of this study is available under https://github.com/ahrends/mixing.,1,0,0
10.1016/j.neuroimage.2022.119026,db.humanconnectome.org,Data and code availability statement The real dataset used in this study has been made pub-licly available by the WU-Minn Consortium in ConnectomeDB https://db.humanconnectome.org/.,0,1,0
10.1016/j.neuroimage.2022.119202,osf.io/bjzpa/?view_only,"Model ﬁtting and analysis code is available at https://github.com/DijunQuant/HMM_public Feature time series for hidden Markov Model ﬁtting, derived from raw EEG data is available at https://osf.io/bjzpa/?view_only = bc59c1c2aec44a68a584c668c234c55e Declaration of Competing Interest The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to inﬂu-ence the work reported in this paper.",0,0,1
10.1016/j.neuroimage.2022.119202,github.com/dijunquant/hmm_public,"Model ﬁtting and analysis code is available at https://github.com/DijunQuant/HMM_public Feature time series for hidden Markov Model ﬁtting, derived from raw EEG data is available at https://osf.io/bjzpa/?view_only = bc59c1c2aec44a68a584c668c234c55e Declaration of Competing Interest The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to inﬂu-ence the work reported in this paper.",0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/nplpk,https://balsa.wustl.edu/nplpK.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/g767v,https://balsa.wustl.edu/g767V. 14 M.F.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/prbrk,https://balsa.wustl.edu/PrBrK.,0,0,1
10.1016/j.neuroimage.2022.119360,github.com/washington-university/hcppipelines,"The methods presented in this study will be released as an HCP Pipeline (https://github.com/Washington-University/HCPpipelines)., The code will be made available as a part of the HCP Pipelines (https://github.com/Washington-University/HCPpipelines) when the various analysis streams presented here (currently existing as 3 separate pipelines) are integrated into a single multi-functional pipeline to make this approach easier for users to use.",1,0,0
10.1016/j.neuroimage.2022.119360,db.humanconnectome.org,The raw HCP-YA data are available in ConnectomeDB (https://db.humanconnectome.org/) or Amazon Public Datasets (https://registry.opendata.aws/hcp-openaccess ; https://wiki.humanconnectome.org/display/PublicData/How+To+Connect+to+Connectome+Data+via+AWS).,0,1,0
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/6vjvk,https://balsa.wustl.edu/6VjVk. 11 M.F.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/5xqxp,https://balsa.wustl.edu/5XqXP.,0,0,1
10.1016/j.neuroimage.2022.119360,registry.opendata.aws/hcp-openaccess,The raw HCP-YA data are available in ConnectomeDB (https://db.humanconnectome.org/) or Amazon Public Datasets (https://registry.opendata.aws/hcp-openaccess ; https://wiki.humanconnectome.org/display/PublicData/How+To+Connect+to+Connectome+Data+via+AWS).,0,1,0
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/mxnx8,https://balsa.wustl.edu/Mxnx8.,0,0,1
10.1016/j.neuroimage.2022.119360,wiki.humanconnectome.org/display/publicdata/how+to+connect+to+connectome+data+via+aws,The raw HCP-YA data are available in ConnectomeDB (https://db.humanconnectome.org/) or Amazon Public Datasets (https://registry.opendata.aws/hcp-openaccess ; https://wiki.humanconnectome.org/display/PublicData/How+To+Connect+to+Connectome+Data+via+AWS).,0,1,0
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/llmlp,https://balsa.wustl.edu/lLMLp.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/mdbd3,https://balsa.wustl.edu/mDBD3.,0,0,1
10.1016/j.neuroimage.2022.119360,nda.nih.gov/general-query.html?q,The raw HCD and HCA data are available in the NDA (https://nda.nih.gov/general-query.html?q = query = featured-datasets:HCP%20Aging%20and %20Development).,0,1,0
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/study/show/mdbp0,"The data for this study are available at the BALSA neuroimaging study results database (https://balsa.wustl.edu/study/show/mDBP0 ; Van Essen et al., 2017), and a link to each ﬁgure’s speciﬁc data is provided in the legend., The study results data from this manuscript are already available in the BALSA neuroimaging study results database: https://balsa.wustl.edu/study/show/mDBP0.",0,1,0
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/4mlml,"Methods sections 2.2 and 2.5 describe the participants and data, and preprocessing is described in methods sections 2.6 and 2.9 https://balsa.wustl.edu/4mlml.",0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/1bgbp,https://balsa.wustl.edu/1BgBP.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/7qwq3,https://balsa.wustl.edu/7qwq3.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/b494v,https://balsa.wustl.edu/B494V. 15 M.F.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/wnznz,https://balsa.wustl.edu/wNzNZ.,0,0,1
10.1016/j.neuroimage.2022.119360,balsa.wustl.edu/qnwnz,https://balsa.wustl.edu/qNwNZ. 16 M.F.,0,0,1
10.1016/j.neuroimage.2022.119228,knightadrc.wustl.edu/research/resourcerequest.htm,Data availability The data and code used in this study are available by request through the Knight ADRC (https://knightadrc.wustl.edu/research/resourcerequest.htm).,0,1,0
10.1016/j.neuroimage.2022.119597,pexels.com,"All object images were selected from three websites (http://unsplash.com , http://pixabay.com , http://www.pexels.com) that pro-vide freely usable, non-copyrighted images.",0,0,1
10.1016/j.neuroimage.2022.119597,unsplash.com,"All object images were selected from three websites (http://unsplash.com , http://pixabay.com , http://www.pexels.com) that pro-vide freely usable, non-copyrighted images.",0,0,1
10.1016/j.neuroimage.2022.119597,pixabay.com,"All object images were selected from three websites (http://unsplash.com , http://pixabay.com , http://www.pexels.com) that pro-vide freely usable, non-copyrighted images.",0,0,1
10.1016/j.neuroimage.2022.119597,memory.kaist.ac.kr/codes,The code used in this study is available at this link (https://memory.kaist.ac.kr/codes/).,1,0,0
10.1016/j.neuroimage.2022.119597,afni.nimh.nih.gov,"fMRI data analysis Data analysis was conducted using AFNI (http://afni.nimh.nih.gov , RRID:SCR_005927) version 19.0.09, SUMA (AFNI surface mapper), FreeSurfer (RRID:SCR_001847) version 6.0.0, and custom MATLAB scripts.",1,0,0
10.1016/j.neuroimage.2022.119398,github.com/dvs-lab/dmn-reward,Analysis code can be found on https://github.com/DVS-Lab/dmn-reward.,1,0,0
10.1016/j.neuroimage.2022.119398,"neurovault.org/collections/10,921","NeuroVault: https://neurovault.org/collections/10,921/.",0,0,1
10.1016/j.neuroimage.2022.119398,humanconnectome.org,"Participants We obtained behavioral and neuroimaging data of 495 ran-domly selected subjects from the Human Connectome Project (HCP; www.humanconnectome.org), an open-access database aimed at col-lecting healthy participant data from over 1200 people., Data availability statement Behavioral and neuroimaging data for this manuscript was obtained from the Human Connectome Project (HCP; www.humanconnectome.org), an open-access database aimed at collecting healthy participant data from over 1200 people.",0,1,0
10.1016/j.neuroimage.2022.119398,fmrib.ox.ac.uk/fsl,fMRI preprocessing Our data preprocessing and analysis was carried out using FMRIB Software Library tools (v6.00; www.fmrib.ox.ac.uk/fsl).,1,0,0
10.1016/j.neuroimage.2022.119491,osf.io/4rmex/?view_only,Data and code availability statement The code that supports the ﬁndings of this study and the underly-ing numerical data for each ﬁgure are available on the Open Science Framework at the following link: https://osf.io/4rmex/?view_only = 7d17c2334ace4a0d83087bddf2a64a68.,0,1,0
10.1016/j.neuroimage.2021.118824,statistic.compare,end The 𝑛 𝑃 returned values constitute the null distribution of the ﬁrst cluster depth (𝐷 depth: 1) statistic.Compare (𝐷 depth: 1) to the observed statistics at the ﬁrst cluster depth of the 𝐾 observed clusters to produce 𝐾 𝑝 -values.,1,0,0
10.1016/j.neuroimage.2021.118824,values.compare,"practical approach Compare the multivariate distribution to the observed statistics of the 𝐾 clusters in order to produce ∑𝑘 𝐽 𝑘 𝑝 -values from the head, which correspond to one 𝑝 -value for each time point within the 𝐾 clusters, as follows: for each 𝑘 = {1 , …, 𝐾} clusters do When 𝐽 𝑘 < 𝐽 𝐷 , ﬁll the statistics of the 𝑘 th cluster with 0 to obtain 𝐽 𝐷 values.Compare the 𝐽 𝐷 values to the multivariate null distribution  𝐷 𝐻 depth: 1 …𝐷 𝐻 depth: 𝐽 𝐷  using the algorithm proposed byTroendle(1995).Keep the 𝐽 𝑘 corrected 𝑝 -values from the head corresponding to each time point within the 𝑘 th cluster.",1,0,0
10.1016/j.neuroimage.2022.119082,help.brain-map.org/download/attachments/2818169/mouseccf.pdf,"Separate right and left hippocampus masks were created us-ing the CA1, CA2, CA3 and DG ROIs as deﬁned in the Allen Common Coordinate Framework (CCF) (V3, http://help.brain-map.org/download/attachments/2818169/MouseCCF.pdf).",0,0,1
10.1016/j.neuroimage.2022.119082,picsl.upenn.edu/ants,"Normalisation was performed through linear aﬃne and nonlinear greedy SyN diﬀeomorphic transformation metric map-ping using ANTs v2.1 (picsl.upenn.edu/ANTS)., The GCE and seed voxel maps were normalised to the Allen Mouse Brain Common Coordinate Framework (CCFv3) (Wang et al., 2020), using linear aﬃne and nonlinear greedy SyN diﬀeomorphic transfor-mation metric mapping using ANTs v2.1 (picsl.upenn.edu/ANTS).",1,0,0
10.1016/j.neuroimage.2022.119082,netneurotools.readthedocs.io/en/latest/index.html,The test returns p-values assessing the two-sided test for whether the absolute diﬀerence between the mean of the correlation and zero is greater than the absolute value of the permuted diﬀerences https://netneurotools.readthedocs.io/en/latest/index.html (https://netneurotools.readthedocs.io/en/latest/auto_examples/plot_perm_pvals.html#sphx-glr-auto-examples-plot-perm-pvals-py).,1,0,0
10.1016/j.neuroimage.2022.119082,ox.ac.uk/fsl/fslwiki/othersoftware,ox.ac.uk/fsl/fslwiki/OtherSoftware.,1,0,0
10.1016/j.neuroimage.2022.119082,mouse.brain-map.org/agea,"We used the Anatomical Gene Ex-pression Atlas (AGEA; https://mouse.brain-map.org/agea) correlation maps quantifying GCE., The GCE data are openly available through the AGEA web-site (https://mouse.brain-map.org/agea).",0,1,0
10.1016/j.neuroimage.2022.119082,github.com/koenhaak/congrads,"Next, similarity matrices were computed, averaging across individuals to create a group similarity matrix, and gradient analysis was conducted using scripts available at https://github.com/koenhaak/congrads.",1,0,0
10.1016/j.neuroimage.2022.119082,github.com/netneurolab/netneurotools,"That is, for a given correlation (i.e., FC for an edge) between the time series for a speciﬁc gradient cluster and the time series for a speciﬁc CCF ROI, we used the one-sample permutation test from netneurotools (https://github.com/netneurolab/netneurotools) to com-pute the signiﬁcance of FC, across mice.",1,0,0
10.1016/j.neuroimage.2022.119082,github.com/brynjagunnars/fc_ge_mouse_hippocampus,"FC gradients and maps, GCE maps, code and ROIs used for this study are available through github (https://github.com/BrynjaGunnars/FC_GE_mouse_Hippocampus).",1,0,0
10.1016/j.neuroimage.2022.119082,github.com/murraylab/brainsmash,"The surrogate maps were created using the open-access python package, brainSMASH (https://github.com/murraylab/brainsmash), which is an implementa-tion of the generative model put forward by Burt et al.",1,0,0
10.1016/j.neuroimage.2022.119082,biorender.com,Figs. 1 –3 were created with BioRender.com.,1,0,0
10.1016/j.neuroimage.2022.119082,netneurotools.readthedocs.io/en/latest/auto_examples/plot_perm_pvals.html#sphx-glr-auto-examples-plot-perm-pvals-py,The test returns p-values assessing the two-sided test for whether the absolute diﬀerence between the mean of the correlation and zero is greater than the absolute value of the permuted diﬀerences https://netneurotools.readthedocs.io/en/latest/index.html (https://netneurotools.readthedocs.io/en/latest/auto_examples/plot_perm_pvals.html#sphx-glr-auto-examples-plot-perm-pvals-py).,1,0,0
10.1016/j.neuroimage.2022.119082,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fix),"This pipeline includes independent component analysis (ICA) based artefact removal (FSL’s FIX https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FIX), band-pass ﬁltering (0.01–0.25 Hz), skull-stripping and normalisation to the Allen Mouse Brain Common Coordinate Framework (CCFv3) (Wang et al., 2020).",1,0,0
10.1016/j.neuroimage.2022.119645,leoliuf.github.io/mrilab,"The Bloch simulation was conducted on MRi-Lab (https://leoliuf.github.io/MRiLab/), an open-source numerical MRI simulation package (Liu et al., 2016), using the same MQMOLED se-quence and parameters as those conﬁgured in the experiments. 3 L.",1,0,0
10.1016/j.neuroimage.2022.119645,ﬁl.ion.ucl.ac.uk/spm,"All the images from the real scans were spatially registered using Statistical Parametric Mapping 12 (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) before parameter estimations.",1,0,0
10.1016/j.neuroimage.2022.119093,fcon_1000.projects.nitrc.org/indi/retro/mpi_lemon.html,The EEG data is available at: http://fcon_1000.projects.nitrc.org/indi/retro/MPI_LEMON.html.,0,1,0
10.1016/j.neuroimage.2022.119093,hdl.handle.net/11633/di.dccn.dsc_3011020.09_236,The MEG data is available at: http://hdl.handle.net/11633/di.dccn.DSC_3011020.09_236.,0,1,0
10.1016/j.neuroimage.2022.119093,github.com/nschawor/meg-eeg-leadﬁeld-mixing,"The analysis code needed to reproduce the analysis and ﬁgures is available here: https://github.com/nschawor/meg-eeg-leadﬁeld-mixing., Code underlying all ﬁgure generation and analysis is available at: https://github.com/nschawor/meg-eeg-leadﬁeld-mixing.",1,0,0
10.1016/j.neuroimage.2022.119663,github.com/fangq/iso2mesh,"Ashburner and Fris-ton, 2005 The head model included scalp, skull, cerebrospinal ﬂuid (CSF), grey matter (GM), and white matter (WM) tissue layers, which were converted into a high-resolution tetrahedral mesh using Iso2mesh (github.com/fangq/iso2mesh), a MATLAB mesh generation and pro-cessing toolbox.",1,0,0
10.1016/j.neuroimage.2022.119663,homer-fnirs.org,"Signal processing and HD-DOT image reconstruction Each step in the data pre-processing pipeline was undertaken us-ing functions from the Homer2 fNIRS processing package (www.homer-fnirs.org), or modiﬁed versions thereof.",1,0,0
10.1016/j.neuroimage.2022.119699,mc-stan.org/users/interfaces/.the,"The software package Stan (Stan Development Team, 2020b) is available at: https://www.mc-stan.org/users/interfaces/.The software package R (R Core Team, 2020) is available at: https://www.r-project.org.",1,0,0
10.1016/j.neuroimage.2022.119699,r-project.org,"The software package Stan (Stan Development Team, 2020b) is available at: https://www.mc-stan.org/users/interfaces/.The software package R (R Core Team, 2020) is available at: https://www.r-project.org.",1,0,0
10.1016/j.neuroimage.2022.119699,cran.r-project.org/web/packages/rstan/index.html,"The R-package rstan (Stan Development Team, 2020a) is available at: https://www.cran.r-project.org/web/packages/rstan/index.html.",1,0,0
10.1016/j.neuroimage.2022.119699,github.com/likeajumprope/bayesian_normative_models,"The Stan code for the HBLM, the HBGPM and the simple Bayesian linear model without site as covariate can be found at https://www.github.com/likeajumprope/Bayesian_normative_models., The Stan code for the HBLM, HBGPM and simple Bayesian linear model are available at: https://www.github.com/likeajumprope/Bayesian_normative_models. 14 J.M.M.",1,0,0
10.1016/j.neuroimage.2022.119699,github.com/topepo/caret,"Splitting the ABIDE data set into training and test sets To evaluate the performance of the models, we split the healthy control data set into a training set (70% of data, n = 391) and a test set (30% of data, n = 168) using the R pack-age caret (https://www.github.com/topepo/caret/) and splitstackshape (http://www.github.com/mrdwab/splitstackshape) , while the distribu-tion of age, sex and site was preserved between sets.",1,0,0
10.1016/j.neuroimage.2022.119699,ukbiobank.ac.uk/explore-your-participation/contribute-further/imaging-study,"When launched in 2006, it aimed to scan 100,000 individuals at four diﬀer-ent scanning locations  https://www.ukbiobank.ac.uk/explore-your-participation/contribute-further/imaging-study (Miller et al., 2016).",0,0,1
10.1016/j.neuroimage.2022.119699,github.com/mrdwab/splitstackshape,"Splitting the ABIDE data set into training and test sets To evaluate the performance of the models, we split the healthy control data set into a training set (70% of data, n = 391) and a test set (30% of data, n = 168) using the R pack-age caret (https://www.github.com/topepo/caret/) and splitstackshape (http://www.github.com/mrdwab/splitstackshape) , while the distribu-tion of age, sex and site was preserved between sets.",1,0,0
10.1016/j.neuroimage.2022.119699,preprocessed-connectomes-project.org/abide,"In present paper, we ﬁrst use the healthy control sample from the ABIDE (autism brain imaging data exchange, http://www.preprocessed-connectomes-project.org/abide/) (Di Martino et al., 2014) data set to compare a non-linear, Gaussian process version of the normative model, to a linear hierarchical Bayesian version accounting for site eﬀects that does not include the Gaussian Process term., ABIDE Data set The ABIDE consortium (http://www.preprocessed-connectomes-project.org/abide/) was founded to facilitate research and collabora-tion on autism spectrum disorders by data aggregation and sharing., preprocessed-connectomes-project.org/abide/.",0,1,0
10.1016/j.neuroimage.2022.119699,preprocessed-connectomes-project.org/abide,"In present paper, we ﬁrst use the healthy control sample from the ABIDE (autism brain imaging data exchange, http://www.preprocessed-connectomes-project.org/abide/) (Di Martino et al., 2014) data set to compare a non-linear, Gaussian process version of the normative model, to a linear hierarchical Bayesian version accounting for site eﬀects that does not include the Gaussian Process term., ABIDE Data set The ABIDE consortium (http://www.preprocessed-connectomes-project.org/abide/) was founded to facilitate research and collabora-tion on autism spectrum disorders by data aggregation and sharing.",0,1,0
10.1016/j.neuroimage.2022.119201,c4science.ch/source/icaps,"Matlab code for the application of the whole framework can be found at https://www.c4science.ch/source/iCAPs., Mat-lab code for the application of the iCAP framework can be found at https://www.c4science.ch/source/iCAPs.",1,0,0
10.1016/j.neuroimage.2022.119201,cnda.wustl.edu/app/template/login,The full set of neuroimaging data (along with behavioral data) are available at http://cnda.wustl.edu/app/template/Login.,0,1,0
10.1016/j.neuroimage.2022.119201,ﬁl.ion.ucl.ac.uk/spm,"Tarun et al., 2020 ; Zöller et al., 2019 ; Zoeller et al., 2019) that used SPM8 (http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119201,github.com/danizoeller/mypls,Matlab code for the ap-plication of the PLSC analysis can be found at https://github.com/danizoeller/myPLS.,1,0,0
10.1016/j.neuroimage.2022.119449,neurobs.com,The stimuli were presented by the software ‘Pre-sentation’ (www.neurobs.com).,1,0,0
10.1016/j.neuroimage.2022.118959,mrtrix.org,"All these steps, and the following tractography, were performed using the MRtrix3 software (http://www.mrtrix.org/) (Tournier et al., 2012). 2 D.",1,0,0
10.1016/j.neuroimage.2022.118959,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,"Data were provided by the Hu-man Connectome Project, WU-Minn Consortium (Principal Investiga-tors: David Van Essen and Kamil Ugurbil; 1U54MH091657) and are publicly available at https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release.",0,1,0
10.1016/j.neuroimage.2022.118959,github.com/brainmappinglab,Both code and subthalamic nucleus maps will be also made available online soon after publication of the article at https://github.com/BrainMappingLab.,0,0,1
10.1016/j.neuroimage.2022.119365,osf.io/ue3tg,Data and code are available at osf.io/ue3tg Declaration of Competing Interest Hartwig R.,1,0,0
10.1016/j.neuroimage.2022.119558,stnava.github.io/ants,"To convert them to individual subject space, we ﬁrst performed a non-linear registration of a 1mm 3 MNI template using Advanced Normalization Tools (ANTs, http://stnava.github.io/ANTs/).",1,0,0
10.1016/j.neuroimage.2022.119558,mgh.harvard.edu,mgh.harvard.edu/) was used to perform cortical/subcortical segmenta-tion and parcellation.,1,0,0
10.1016/j.neuroimage.2022.119558,github.com/garikoitz/rtp-pipeline/wiki/parameter-recommendations,We will provide support to researchers interested in using this protocol (https://github.com/garikoitz/RTP-pipeline/wiki/Parameter-recommendations).,0,0,1
10.1016/j.neuroimage.2022.119558,github.com/vistalab/vistasoft,"Also, it builds on previous well-validated tools including the ﬁrst probabilistic atlas of the thalamus based on combin-ing high-resolution ex vivo MRI and histology (Iglesias et al., 2018) and the reproducible-tract-proﬁles (RTP2) containerized tool which is based on state-of-the-art techniques implemented on top of Vistasoft’s code, which have been tested and used in many publications over the last 15 years (https://www.github.com/vistalab/vistasoft ; Lerma-Usabiaga et al., 2022).",1,0,0
10.1016/j.neuroimage.2022.119558,github.com/scilus/scilpy,"The measurements used to examine computational reproducibility and rest-retest reproducibility were computed using the package scilpy (see details in Schilling et al., 2021 and https://github.com/scilus/scilpy).",1,0,0
10.1016/j.neuroimage.2022.119558,github.com/mengxingliu/thatract-paper,The code and parameters are available through GitHub (github.com/MengxingLiu/Thatract-paper) and Docker Hub (https://hub.docker.com/u/garikoitz).,1,0,0
10.1016/j.neuroimage.2022.119558,github.com/noahbenson/neuropythy,"To parcellate the visual cortex we ran the Neuropythy (Benson & Winawer, 2018 ; https://github.com/noahbenson/neuropythy) tool on the Freesurfer results.",1,0,0
10.1016/j.neuroimage.2022.119558,hub.docker.com/u/garikoitz,The code and parameters are available through GitHub (github.com/MengxingLiu/Thatract-paper) and Docker Hub (https://hub.docker.com/u/garikoitz).,1,0,0
10.1016/j.neuroimage.2021.118781,ﬁl.ion.ucl.ac.uk/spm,fMRI preprocessing and GLM analysis We used SPM12 on MATLAB 2015b (www.ﬁl.ion.ucl.ac.uk/spm) for the preprocessing and analysis of both fMRI datasets.,1,0,0
10.1016/j.neuroimage.2022.119487,11.4.2.0,"Statistical analysis All data were analyzed by the SPSS software (IBM SPSS Statistics, Version 25.0) or MedCalc statistical software (Version 11.4.2.0).",1,0,0
10.1016/j.neuroimage.2022.119487,oasis-brains.org,"3D VOI for stereotactically normalized images were automatically segmented on both brain hemispheres by using the neuromorphomet-rics atlas with maximum probability tissue labels, originating from the OASIS project (http://www.oasis-brains.org/).",1,0,0
10.1016/j.neuroimage.2022.119487,ﬁl.ion.ucl.ac.uk/spm,"Image analysis All reconstructed brain 18 F-FDG PET images were spatially nor-malized into the default brain template of tissue probability map (TPM) embedded in statistical parametric mapping 12 (SPM12) soft-ware package (Welcome Trust center for Neuroimaging, London, UK; http://www.ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2021.118768,mne.tools/stable/generated/mne.stats.bonferroni_correction.html,"We also found marginally signiﬁcant task diﬀer-ences at later latencies in PVA (p-value < 0.075) at 300 ms, and IFG (p-2 https://mne.tools/stable/generated/mne.stats.fdr_correction.html 3 https://mne.tools/stable/generated/mne.stats.bonferroni_correction.html 6 S.",0,0,1
10.1016/j.neuroimage.2021.118768,github.com/setareh10/semnet-project,"Data and code availability Codes used for this study are available from the following public repository: https://github.com/setareh10/semnet-project Credit author statement Setareh Rahimi: Conceptualization, Methodology, Software, For-mal analysis, Investigation, Visualization, Writing – original draft, Writing– review & editing, Project administration.",1,0,0
10.1016/j.neuroimage.2021.118768,biorxiv.org/cgi/content/short/2021.06.28.450126v1,"Stimuli The stimulus set included in our MEG analysis consisted of 250 unin-ﬂected words, including three categories of concrete words with strong visual, auditory and hand-action attributes (50 words per category), as well as two categories of emotional and neutral abstract words (50 1 A previous version of our manuscript has been published as a pre-print: https://biorxiv.org/cgi/content/short/2021.06.28.450126v1 2 S.",0,0,1
10.1016/j.neuroimage.2021.118768,mne.tools/stable/generated/mne.stats.fdr_correction.html,"We also found marginally signiﬁcant task diﬀer-ences at later latencies in PVA (p-value < 0.075) at 300 ms, and IFG (p-2 https://mne.tools/stable/generated/mne.stats.fdr_correction.html 3 https://mne.tools/stable/generated/mne.stats.bonferroni_correction.html 6 S.",0,0,1
10.1016/j.neuroimage.2022.119670,github.com/koch-means-cook/damson,Data and Code availability All code of the involved analyses will be published and made openly available at https://github.com/koch-means-cook/damson.,1,0,0
10.1016/j.neuroimage.2022.119670,python.org,"fMRI analyses Classiﬁcation of walking direction All classiﬁcation of walking direction was performed in Python (Python Software Foundation; Python Language Refer-ence, version 3.7.8; available at http://www.python.org) and re-lied on scikit-learn (Pedregosa et al., 2011) and nilearn (Abraham et al., 2014).",1,0,0
10.1016/j.neuroimage.2022.119670,antsbrainextraction.sh,"The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workﬂow (from ANTs), using OASIS30ANTs as target template.",1,0,0
10.1016/j.neuroimage.2022.119434,github.com/mandymejia,"The code for tem-plate estimation and template ICA model ﬁtting is available for Matlab (https://github.com/mandymejia/templateICA) and R (https://github., Esti-mation and removal of subject-and session-speciﬁc nuisance ICs, some of which represent head motion, was performed as part of the tem-plate ICA package to minimize the impact of motion and other sources of variability of non-interest on the estimation of the thalamic spa-tial maps (https://github.com/mandymejia/templateICA/blob/master/templateICA.m)., Code: Most of the code that supports this study is available via freely available online repositories: https://github.com/KKI-CNIR/CNIR-fmri_preproc_toolbox -https://github.com/mandymejia If any code is not easily accessible at these locations, code can be shared upon reasonable request: Andrew Gaddis-gaddis@jhmi.edu and Frederick Barrett fbar-rett@jhmi.edu Acknowledgements This work was supported by a grant from the Heﬀter Research Insti-tute (R.R.G.) and by the ﬁnancial support of the Johns Hopkins Center for Psychedelic and Consciousness Research which was funded by the Steven and Alexandra Cohen Foundation , Tim Ferriss, Matt Mullenweg, Blake Mycoskie, and Craig Nerenberg.",1,0,0
10.1016/j.neuroimage.2022.119434,trendscenter.org/software/gift,"Group ICA of fMRI Tool-box (GIFT v3.0b: https://trendscenter.org/software/gift/; Medical Im-age Analysis Lab, Albuquerque, New Mexico) (Calhoun et al., 2001 ; Erhardt et al., 2011) was then used to decompose intrathalamic vox-els into temporally coherent, spatially independent components.",1,0,0
10.1016/j.neuroimage.2022.119434,github.com/mandymejia/templateica/blob/master/templateica.m,"Esti-mation and removal of subject-and session-speciﬁc nuisance ICs, some of which represent head motion, was performed as part of the tem-plate ICA package to minimize the impact of motion and other sources of variability of non-interest on the estimation of the thalamic spa-tial maps (https://github.com/mandymejia/templateICA/blob/master/templateICA.m).",1,0,0
10.1016/j.neuroimage.2022.119434,2.4.1.1,Motion and nuisance regression 2.4.1.1.,0,0,1
10.1016/j.neuroimage.2022.119434,clinicaltrials.gov,This study was con-ducted as part of a clinical trial that was registered at ClinicalTrials.gov (NCT02145091).,0,0,1
10.1016/j.neuroimage.2022.119434,2.4.1.2,2.4.1.2.,0,0,1
10.1016/j.neuroimage.2022.119434,github.com/mandymejia/templateica,"The code for tem-plate estimation and template ICA model ﬁtting is available for Matlab (https://github.com/mandymejia/templateICA) and R (https://github., Esti-mation and removal of subject-and session-speciﬁc nuisance ICs, some of which represent head motion, was performed as part of the tem-plate ICA package to minimize the impact of motion and other sources of variability of non-interest on the estimation of the thalamic spa-tial maps (https://github.com/mandymejia/templateICA/blob/master/templateICA.m).",1,0,0
10.1016/j.neuroimage.2022.119434,afni.nimh.nih.gov/afni,"The resulting timecourses were despiked to remove the inﬂuence of outlier timepoints (Analysis of Functional Neuroimages: http://afni.nimh.nih.gov/afni ; NIMH Scientiﬁc and Statistical Com-puting Core, Bethesda, Maryland). 4 Thalamocortical connectivity matrix: Subject-and session-speciﬁc par-tial correlation matrices were generated using ridge regression (rho = 1) (Lombardo et al., 2019), converted from r to z -statistics using Fisher’s transformation, and compared between sessions using paired two-sided t-tests to examine between-session diﬀerences in both intra-thalamic, thalamocortical, and intra-cortical connectivity (FDR = 0.05). 5 Associations with Subjective-Eﬀects: Between-session changes in func-tional connectivity were associated with between-session changes in subjective-eﬀects (see Section 2.4.3) using Spearman rank cor-relations.",1,0,0
10.1016/j.neuroimage.2022.119434,ﬁl.ion.ucl.ac.uk/spm,fMRI minimal preprocessing for template map generation The fMRI datasets were minimally preprocessed using SPM 12 (https://www.ﬁl.ion.ucl.ac.uk/spm/) and custom code written in MATLAB (https://github.com/KKI-CNIR/CNIR-fmri_preproc_toolbox).,1,0,0
10.1016/j.neuroimage.2022.119434,github.com/kki-cnir/cnir-fmri_preproc_toolbox,"fMRI minimal preprocessing for template map generation The fMRI datasets were minimally preprocessed using SPM 12 (https://www.ﬁl.ion.ucl.ac.uk/spm/) and custom code written in MATLAB (https://github.com/KKI-CNIR/CNIR-fmri_preproc_toolbox)., Code: Most of the code that supports this study is available via freely available online repositories: https://github.com/KKI-CNIR/CNIR-fmri_preproc_toolbox -https://github.com/mandymejia If any code is not easily accessible at these locations, code can be shared upon reasonable request: Andrew Gaddis-gaddis@jhmi.edu and Frederick Barrett fbar-rett@jhmi.edu Acknowledgements This work was supported by a grant from the Heﬀter Research Insti-tute (R.R.G.) and by the ﬁnancial support of the Johns Hopkins Center for Psychedelic and Consciousness Research which was funded by the Steven and Alexandra Cohen Foundation , Tim Ferriss, Matt Mullenweg, Blake Mycoskie, and Craig Nerenberg.",1,0,0
10.1016/j.neuroimage.2021.118787,mni.mcgill.ca,A leadﬁeld was generated us-ing a realistic three-shell boundary-element volume conduction model based on the MNI standard brain (MNI; http://www.mni.mcgill.ca) for each grid point in the brain on a regular 10 mm grid.,1,0,0
10.1016/j.neuroimage.2021.118787,ru.nl/fcdonders/ﬁeldtrip,"EEG data preprocessing and data analysis were conducted in MATLAB (MathWorks, Natick, MA, USA) using EEGLAB (http://www.sccn.ucsd.edu/eeglab) (Delorme and Makeig, 2004), FieldTrip (http://www.ru.nl/fcdonders/ﬁeldtrip) (Oostenveld et al., 2010) and customized scripts.",1,0,0
10.1016/j.neuroimage.2021.118787,sccn.ucsd.edu/eeglab,"EEG data preprocessing and data analysis were conducted in MATLAB (MathWorks, Natick, MA, USA) using EEGLAB (http://www.sccn.ucsd.edu/eeglab) (Delorme and Makeig, 2004), FieldTrip (http://www.ru.nl/fcdonders/ﬁeldtrip) (Oostenveld et al., 2010) and customized scripts.",1,0,0
10.1016/j.neuroimage.2022.119098,nitrc.org/projects/conn,"Functional connectivity analyzes Functional connectivity was examined using ROI-to-ROI connectivity maps implemented in the CONN v.18b (www.nitrc.org/projects/conn) toolbox (Whitﬁeld-Gabrieli and Nieto-Castanon, 2012).",1,0,0
10.1016/j.neuroimage.2022.119098,openneuro.org,Data availability statement Data will be publicly available via Open Neuro (https://openneuro.org/) upon publication.,0,1,0
10.1016/j.neuroimage.2022.119448,2.6.2.5,2.6.2.5.,0,0,1
10.1016/j.neuroimage.2022.119448,2.6.2.4,2.6.2.4.,0,0,1
10.1016/j.neuroimage.2022.119448,2.6.3.1,Group-level analysis (time-lagged INS) 2.6.3.1.,0,0,1
10.1016/j.neuroimage.2022.119448,2.6.3.2,2.6.3.2.,0,0,1
10.1016/j.neuroimage.2022.119448,2.6.2.3,2.6.2.3.,0,0,1
10.1016/j.neuroimage.2022.119448,github.com/cran/fiar,"Fourth, an R package (FIAR; down-load from https://github.com/cran/FIAR) was used to calculate the par-tial multivariate Granger causalities in two directions (Roelstraete and Rosseel, 2011): from the leaders to the followers and from the followers to the leaders.",1,0,0
10.1016/j.neuroimage.2022.119448,2.6.2.1,Group-level analysis (time-aligned INS) 2.6.2.1.,0,0,1
10.1016/j.neuroimage.2022.119448,2.6.2.2,2.6.2.2.,0,0,1
10.1016/j.neuroimage.2021.118751,sites.google.com/view/pinstudy,"Furthermore, the defaced neuroimaging data for a subset of participants who provided consent has been made publicly available for researchers to download (https://sites.google.com/view/pinstudy)., Demographic and structural MRI data for a subset of 24 participants is publicly avail-able to download at https://sites.google.com/view/pinstudy., Further, we have made the raw, defaced MRI data for a subset of participants publicly available to other researchers (https://sites.google.com/view/pinstudy)., Data availability statement Data is available from https://sites.google.com/view/pinstudy for a subset of participants who provided consent for their demographic and defaced neuroimaging data to be made publicly available.",0,1,0
10.1016/j.neuroimage.2021.118751,adni.loni.usc.edu,"For MRI acquisition, we chose to use the third generation of the Alzheimer’s Disease Neuroimaging Inia-tive (ADNI) protocol (http://adni.loni.usc.edu/) as the ADNI-3 sequence was speciﬁcally developed for longitudinal multi-centre studies (Jack Jr et al., 2008).",0,0,1
10.1016/j.neuroimage.2021.118751,surfer.nmr.mgh.harvard.edu,"FreeSurfer is an open-source automated segmentation software pack-age (Martinos Center for Biomedical Imaging, Harvard-MIT, Boston; https://surfer.nmr.mgh.harvard.edu/) that has been widely used in https://doi.org/10.1016/j.neuroimage.2021.118751.",1,0,0
10.1016/j.neuroimage.2022.119583,nora-imaging.org,"For visualization, the orientation-ally weighted streamline densities enable simple streamline tractogra-phy to be performed on the average orientation ﬁelds to visualize the bundles on a group level, using streamline tractography as implemented in NORA (www.nora-imaging.org).",1,0,0
10.1016/j.neuroimage.2022.119583,ida.loni.usc.edu/login.jsp,Participants and preprocessing The dMRI data analysed in this study are part of the Human Con-nectome Project (3T HCP) (https://ida.loni.usc.edu/login.jsp).,0,1,0
10.1016/j.neuroimage.2022.119583,brain-development.org,Results are displayed on an average T1 image and aver-age surface created in VBM8 from 550 healthy participants from the IXI-database (http://www.brain-development.org) and normalized into MNI standard space.,0,1,0
10.1016/j.neuroimage.2022.118962,github.com/brain-modulation-lab/bml,"Electrophysiological data preprocessing Data processing was performed using custom code based on the FieldTrip toolbox (Oostenveld et al., 2011) implemented in Matlab, available at (github.com/Brain-Modulation-Lab/bml).",1,0,0
10.1016/j.neuroimage.2022.119555,humanconnectome.org/study/amish-connectome-project,d.) with no history of psychiatric diagnoses from the ACP (https://www.humanconnectome.org/study/amish-connectome-project).,0,0,1
10.1016/j.neuroimage.2021.118850,mne.tools/stable/index.html,"Forward models were then computed on this source space using the one-layer boundary element method (BEM) for MEG and the three-layer BEM with default conductivity values for EEG (as used and dis-cussed in (Coquelet et al., 2020a)) implemented in the MNE-C suite ((Gramfort et al., 2014); MNE-C v2.7.3, Martinos Center for Biomedi-cal Imaging, Massachusetts, USA; https://mne.tools/stable/index.html).",1,0,0
10.1016/j.neuroimage.2021.118850,thomaskoenig.ch/index.php/software/microstates-in-eeglab,"Microstate clustering Microstate inference from EEG data followed stan-dard steps (for reviews, see, e.g., (Khanna et al., 2015 ; Michel et al., 2009 ; Michel and Koenig, 2018)) and was per-formed using the EEGLAB plugin for microstate analysis (v1.1, http://www.thomaskoenig.ch/index.php/software/microstates-in-eeglab).",1,0,0
10.1016/j.neuroimage.2021.118850,ﬁl.ion.ucl.ac.uk/spm,"The source space was built by placing three orthogonal current dipoles at each point of a grid derived from a regular 5-mm grid cropped within the Montreal Neurological Institute (MNI) template MRI volume and non-linearly deformed onto each participant’s MRI with the Statistical Para-metric Mapping software ((Friston et al., 2007); SPM12, Wellcome cen-tre for Neuroimaging, London, UK; https://www.ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2021.118850,github.com/ohba-analysis/glean,"Hidden Markov modeling of power envelopes The HMM was inferred from power envelope time courses esti-mated by Hilbert transformation of wideband ﬁltered (4–30 Hz) sig-nals, using the GLEAN toolbox (GLEAN0.3, https://github.com/OHBA-analysis/GLEAN) originally developed for and applied to MEG source power (Baker et al., 2014).",1,0,0
10.1016/j.neuroimage.2021.118850,surfer.nmr.mgh.harvard.edu,"Separate forward models for MEG and EEG were computed based on the participants’ MRI, segmented beforehand using the FreeSurfer software ((Fischl, 2012); FreeSurfer v6.0; Martinos Center for Biomedi-cal Imaging, Massachusetts, USA; https://surfer.nmr.mgh.harvard.edu, freesurfer-x86_64-linux-gnu-stable6–20,170,118).",1,0,0
10.1016/j.neuroimage.2021.118850,sccn.ucsd.edu/eeglab/index.php,"Remnant bad channels were then automatically detected and removed using ar-tifact subspace reconstruction (Kothe and Makeig, 2013) as imple-mented in EEGLAB ((Delorme and Makeig, 2004); EEGLAB v2019.0, https://sccn.ucsd.edu/eeglab/index.php) (number of bad channels: 11 ± 4 out of 172, range: 4–21).",1,0,0
10.1016/j.neuroimage.2022.119474,brain-development.org/ixi-dataset,"Lastly, to evaluate the ability of SynthStrip to adapt to imaging modalities beyond MR, we 1 Acquired from http://brain-development.org/ixi-dataset.",0,1,0
10.1016/j.neuroimage.2022.119474,w3id.org/synthstrip,"Our method and labeled evaluation data are available at https://w3id.org/synthstrip., We implement SynthStrip in Python, using the open-source PyTorch (Paszke et al., 2019) and Neurite (Dalca et al., 2018) libraries, and make our tool and associated code available in the open-source FreeSurfer package (https://w3id.org/synthstrip)., To facilitate further development and testing of robust skull-stripping tools, we also make our evaluation data and ground-truth labels available at https://w3id.org/synthstrip.",1,0,0
10.1016/j.neuroimage.2022.119474,masslifesciences.com,The research project beneﬁtted from computational hardware generously provided by the Massachusetts Life Sciences Cen-ter (https://www.masslifesciences.com).,1,0,0
10.1016/j.neuroimage.2022.119221,openneuro.org/datasets/ds004056,"Data availability statement The data that support the ﬁndings of this study are openly available in OpenNeuro at https://openneuro.org/datasets/ds004056, doi:10.18112/openneuro.ds004056.",0,1,0
10.1016/j.neuroimage.2022.119221,people.cas.sc.edu/rorden/mricron/index.html,We used MRIcron (http://people.cas.sc.edu/rorden/mricron/index.html) to display the activation patterns on T1w MRI and volume-rendered im-ages.,1,0,0
10.1016/j.neuroimage.2022.119050,audition.ens.fr/adc/noisetools,"ECG removal, epoching, and trial rejection EEG analysis used FieldTrip (Oostenveld et al., 2011), Noise-Tools (De Cheveigne and Parra, 2014 ; http://audition.ens.fr/adc/NoiseTools/), and custom-written scripts in Matlab.",1,0,0
10.1016/j.neuroimage.2022.119050,zenodo.org,"Data and code availability Raw EEG data from all healthy individuals, as well as Matlab code, are publicly available on zenodo.org (doi: 10.5281/zenodo.6110595)., Data and code availability Raw EEG data from all healthy individuals, as well as Matlab code, are publicly available on zenodo.org (doi: 10.5281/zenodo.6110595).",0,1,0
10.1016/j.neuroimage.2022.119036,osf.io/6jm2r,"Behavioral and EEG data are available at https://osf.io/6jm2r., Data availability Behavioral and EEG data are available at https://osf.io/6jm2r.",0,1,0
10.1016/j.neuroimage.2022.119521,github.com/coﬀeine-labs/coﬀeine,"One of them is Mother of all BCI 3 https://github.com/coﬀeine-labs/coﬀeine 4 https://braindecode.org Benchmarks (MOABB) (Jayaram and Barachant 2018), which allows for convenient EEG-data fetching, MNE (Gramfort et al. 2013 , 2014), imple-ments well established data structures, preprocessing functionality, and more.",0,1,0
10.1016/j.neuroimage.2022.119521,braindecode.org,"One of them is Mother of all BCI 3 https://github.com/coﬀeine-labs/coﬀeine 4 https://braindecode.org Benchmarks (MOABB) (Jayaram and Barachant 2018), which allows for convenient EEG-data fetching, MNE (Gramfort et al. 2013 , 2014), imple-ments well established data structures, preprocessing functionality, and more.",0,1,0
10.1016/j.neuroimage.2022.119521,meeg-ml-benchmarks.github.io/brain-age-benchmark-paper,"2005), the line length (Esteller, Echauz, 1 https://github.com/meeg-ml-benchmarks/meeg-brain-age-benchmark-paper 2 http://meeg-ml-benchmarks.github.io/brain-age-benchmark-paper et al.",0,0,1
10.1016/j.neuroimage.2022.119521,github.com/mne-tools/mne-bids-pipeline,"Apart from preprocessing, we also made use of the MNE-BIDS-Pipeline to generate forward solutions and inverse operators for the source localization ap-proach based on template MRI (see section Covariance-based ﬁlterbank approaches for detailed explanations). 5 https://github.com/mne-tools/mne-bids-pipeline Each model of the benchmark is based on features extracted from clean epochs.",1,0,0
10.1016/j.neuroimage.2022.119521,github.com/meeg-ml-benchmarks/meeg-brain-age-benchmark-paper,"2005), the line length (Esteller, Echauz, 1 https://github.com/meeg-ml-benchmarks/meeg-brain-age-benchmark-paper 2 http://meeg-ml-benchmarks.github.io/brain-age-benchmark-paper et al., For a detailed description consider the main text and the open-source code repository supporting this article (https://github.com/meeg-ml-benchmarks/meeg-brain-age-benchmark-paper).",0,0,1
10.1016/j.neuroimage.2022.119029,3.1.2.1,Commissural ﬁber bundles 3.1.2.1.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.3,3.1.1.3.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.2.2,3.1.2.2.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.2,3.1.1.2.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.9,3.1.1.9.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.1,"Association ﬁber bundles 3.1.1.1., 3.1.1.10.",0,0,1
10.1016/j.neuroimage.2022.119029,3.1.3.4,3.1.3.4.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.4.2,3.1.4.2.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.10,3.1.1.10.,0,0,1
10.1016/j.neuroimage.2022.119029,github.com/kul-radneuron/kul_fwt.git,"The automated workﬂows can be found at (https://github.com/KUL-Radneuron/KUL_FWT.git)., The FWT workﬂows can be downloaded from (https://github.com/KUL-Radneuron/KUL_FWT.git).",0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.4,3.1.1.4.,0,0,1
10.1016/j.neuroimage.2022.119029,massive-data.org,"MASSIVE data The MASSIVE dataset (Froeling et al., 2017) (http://www.massive-data.org), comprises multiple scans of the same healthy individual (female, 25 years old) using various b-values and diﬀusion sampling schemes.",0,1,0
10.1016/j.neuroimage.2022.119029,3.1.1.5,3.1.1.5.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.4.1,Cerebellar bundles 3.1.4.1.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.3.1,Projection ﬁber bundles 3.1.3.1.,0,0,1
10.1016/j.neuroimage.2022.119029,osf.io/snq2d,"Finally, the Open Science Framework repository (https://osf.io/snq2d/) contains addi-tional screenshots of single subject bundles, bundle heatmaps, original and symmetric versions of the HCP-template atlas bundles, and addi-tional tractograms.",0,1,0
10.1016/j.neuroimage.2022.119029,db.humanconnectome.org/data/projects/hcp_retest,"HCP test ‐retest data The test-retest HCP dataset (Van Essen et al., 2012) (https://db.humanconnectome.org/data/projects/HCP_Retest) consists of 2 scans acquired at 1 -11 months apart using the same scanning pro-tocol on the same 3-Tesla Siemens Skyra scanner (Siemens Healthineers, Erlangen, Germany) using a 32-channel phased array receive head coil.",0,1,0
10.1016/j.neuroimage.2022.119029,3.1.1.8,3.1.1.8.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.4.3,3.1.4.3.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.6,3.1.1.6.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.3.3,3.1.3.3.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.3.5,3.1.3.5.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.1.7,3.1.1.7.,0,0,1
10.1016/j.neuroimage.2022.119029,3.1.3.2,3.1.3.2.,0,0,1
10.1016/j.neuroimage.2022.119083,mrbrains18.isi.uu.nl,"Whereas complementar-ity is a strength providing insight on a broad range of questions, it also limits the validation of results between challenges. 10 mrbrains18.isi.uu.nl. 11 node21.grand-challenge.org. 12 www.docker.com. 8 E.E.",0,0,1
10.1016/j.neuroimage.2022.119083,docker.com,"Whereas complementar-ity is a strength providing insight on a broad range of questions, it also limits the validation of results between challenges. 10 mrbrains18.isi.uu.nl. 11 node21.grand-challenge.org. 12 www.docker.com. 8 E.E.",0,0,1
10.1016/j.neuroimage.2022.119083,node21.grand-challenge.org,"Whereas complementar-ity is a strength providing insight on a broad range of questions, it also limits the validation of results between challenges. 10 mrbrains18.isi.uu.nl. 11 node21.grand-challenge.org. 12 www.docker.com. 8 E.E.",0,0,1
10.1016/j.neuroimage.2022.119083,tadpole-share.github.io,"In addi-tion, future challenge should strive for a challenge design that includes 9 tadpole-share.github.io.",1,0,0
10.1016/j.neuroimage.2022.119083,codalab.org/competitions/1471,"On the other hand, methods for avoiding cheating, uncer-2 synapse.org/##!Synapse:syn2290704. 3 caddementia.grand-challenge.org. 4 codalab.org/competitions/1471. 5 kaggle.com/c/mci-prediction. 6 tadpole.grand-challenge.org. 7 frontiersin.org/research-topics/13501.",0,0,1
10.1016/j.neuroimage.2022.119083,adni.loni.usc.edu,"The neuroimaging data was used in diﬀerent ways; some challenges provided raw imaging data (CADDementia, PAC), whereas other provided pre-computed imaging measurements (MLC, MCI-NI) or allowed for both (DREAM, TADPOLE). 8 adni.loni.usc.edu. 4 E.E.",0,0,1
10.1016/j.neuroimage.2022.119083,kaggle.com/c/mci-prediction,"On the other hand, methods for avoiding cheating, uncer-2 synapse.org/##!Synapse:syn2290704. 3 caddementia.grand-challenge.org. 4 codalab.org/competitions/1471. 5 kaggle.com/c/mci-prediction. 6 tadpole.grand-challenge.org. 7 frontiersin.org/research-topics/13501.",0,0,1
10.1016/j.neuroimage.2022.119083,caddementia.grand-challenge.org,"On the other hand, methods for avoiding cheating, uncer-2 synapse.org/##!Synapse:syn2290704. 3 caddementia.grand-challenge.org. 4 codalab.org/competitions/1471. 5 kaggle.com/c/mci-prediction. 6 tadpole.grand-challenge.org. 7 frontiersin.org/research-topics/13501.",0,0,1
10.1016/j.neuroimage.2022.119083,aicrowd.com/challenges/addi-alzheimers-detection-challenge,"Based on this, we evaluated seven grand challenges: •Minimal Interval Resonance Imaging in Alzheimer’s Disease atrophy challenge (MIRIAD, 2012): the challenge task was to quantify vol-1 aicrowd.com/challenges/addi-alzheimers-detection-challenge. 2 E.E.",0,0,1
10.1016/j.neuroimage.2022.119083,synapse.org/##!synapse:syn2290704,"On the other hand, methods for avoiding cheating, uncer-2 synapse.org/##!Synapse:syn2290704. 3 caddementia.grand-challenge.org. 4 codalab.org/competitions/1471. 5 kaggle.com/c/mci-prediction. 6 tadpole.grand-challenge.org. 7 frontiersin.org/research-topics/13501.",0,0,1
10.1016/j.neuroimage.2022.119083,grand-challenge.org,"All chal-lenges had a challenge website, mostly connected to a challenge plat-form (Grand-challenge.org: 2, Kaggle: 1, Codalab: 1, Synapse: 1).",0,0,1
10.1016/j.neuroimage.2022.119083,sciencedirect.com/journal/journal-of-neuroscience-methods/vol/302,"An ensemble of lightweight convolutional neural networks pretrained on UK Biobank data achieved the best performance (MAE = 2.90 years, which slightly increased after bias correction to MAE = 2.95 years.) a sciencedirect.com/journal/journal-of-neuroscience-methods/vol/302.",0,0,1
10.1016/j.neuroimage.2022.119083,grand-challenge.org,"Challenges were found by searching chal-lenges listed by grand-challenge.org and Maier-Hein et al., On the other hand, methods for avoiding cheating, uncer-2 synapse.org/##!Synapse:syn2290704. 3 caddementia.grand-challenge.org. 4 codalab.org/competitions/1471. 5 kaggle.com/c/mci-prediction. 6 tadpole.grand-challenge.org. 7 frontiersin.org/research-topics/13501., Platforms such as grand-challenge.org provide an infrastructure that can be used by challenges without having to implement a custom evaluation system., Whereas complementar-ity is a strength providing insight on a broad range of questions, it also limits the validation of results between challenges. 10 mrbrains18.isi.uu.nl. 11 node21.grand-challenge.org. 12 www.docker.com. 8 E.E.",0,0,1
10.1016/j.neuroimage.2022.119083,tadpole.grand-challenge.org,"On the other hand, methods for avoiding cheating, uncer-2 synapse.org/##!Synapse:syn2290704. 3 caddementia.grand-challenge.org. 4 codalab.org/competitions/1471. 5 kaggle.com/c/mci-prediction. 6 tadpole.grand-challenge.org. 7 frontiersin.org/research-topics/13501.",0,0,1
10.1016/j.neuroimage.2022.119083,frontiersin.org/research-topics/13501,"On the other hand, methods for avoiding cheating, uncer-2 synapse.org/##!Synapse:syn2290704. 3 caddementia.grand-challenge.org. 4 codalab.org/competitions/1471. 5 kaggle.com/c/mci-prediction. 6 tadpole.grand-challenge.org. 7 frontiersin.org/research-topics/13501.",0,0,1
10.1016/j.neuroimage.2022.119147,clinicaltrials.gov,"Altogether, the Percept TM PC, AlphaDBS, G102RS and Picostim systems are the corner-stone of current and future aDBS basic research and clinical trials (e.g., ClinicalTrials.gov reference NCT04547712).",0,0,1
10.1016/j.neuroimage.2022.119147,clinicaltrials.gov/ct2/show/nct04547712,participate in the ADAPT-PD trial https://clinicaltrials.gov/ct2/show/NCT04547712 sponsored by Medtronic.,0,0,1
10.1016/j.neuroimage.2022.119667,osf.io/rmhpe,"GJ was funded by the fed-eral state of Saxony-Anhalt and the “European Regional Development Fund ”(ERDF 2014-2020), Project: Center for Behavioral Brain Sciences (CBBS), FKZ: ZS/2016/04/78113 Data and code availability The data of this study can be downloaded upon accep-tance on the Open Science Framework at https://osf.io/rmhpe/.",0,1,0
10.1016/j.neuroimage.2022.118933,brain-development.org/ixi-dataset,"The data has been collected at three diﬀer-ent sites in London, UK on one GE (1.5T) and two Philips (1.5T and 3T) scanners and is available online (https://brain-development.org/ixi-dataset/) under Creative Commons License BY-NC-ND 3.0 (https://creativecommons.org/licenses/by-sa/3.0/legalcode).",0,1,0
10.1016/j.neuroimage.2022.118933,miriad.drc.ion.ucl.ac.uk,"MIRIAD The Minimal Interval Resonance Imaging in Alzheimer’s Dis-ease (Malone et al., 2013) is a publicly available longitudinal study with focus on neurodegeneration (see http://miriad.drc.ion.ucl.ac.uk/).",1,0,0
10.1016/j.neuroimage.2022.118933,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.118933,github.com/deep-mi/fastsurfer,Access can be provided to scientists in accordance with the Rhineland Study’s Data Use and Access 8 https://github.com/Deep-MI/FastSurfer.,0,0,1
10.1016/j.neuroimage.2022.118933,adni.loni.usc.edu,"Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf., The ADNI database has > 2000 participants and is available online at http://adni.loni.usc.edu.",0,0,1
10.1016/j.neuroimage.2022.118933,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.,0,0,1
10.1016/j.neuroimage.2022.118933,github.com/reuter-lab/fastsurfer,The source code of FastSurferVINN will be made publicly available on Github (https://github.com/reuter-lab/FastSurfer) upon acceptance.,1,0,0
10.1016/j.neuroimage.2022.118933,adni-info.org,"The primary goal of ADNI has been to test whether serial MRI, positron emission tomography, other biolog-ical markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment and early Alzheimer’s disease (see www.adni-info.org for up-to-date information).",0,0,1
10.1016/j.neuroimage.2022.118933,osf.io/nhtur,"Mindboggle-101 The largest manually corrected set of free, pub-licly accessible (https://osf.io/nhtur/) labeled brain images based on a consistent human cortical labeling protocol (DKTatlas) Klein and Tourville (2012).",0,1,0
10.1016/j.neuroimage.2022.118933,adni.loni.usc.edu,"A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf., The ADNI database has > 2000 participants and is available online at http://adni.loni.usc.edu.",0,0,1
10.1016/j.neuroimage.2022.118933,openfmri.org/dataset/ds000030,This data was obtained from the OpenfMRI database (https://openfmri.org/dataset/ds000030/).,0,1,0
10.1016/j.neuroimage.2022.118933,humanconnectome.org/study/hcp-young-adult,"The 0.8 mm isotropic T1-weighted MRI data is acquired on a 3T Siemens Magnetom Prisma scanner using a multi-echo MPRAGE (ME-MPRAGE) sequence with TR 3 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html. 4 https://www.humanconnectome.org/study/hcp-young-adult. 5 https://www.humanconnectome.org/study-hcp-lifespan-pilot. 2560 ms, 10 TEs (1.68 ms, 3.29 ms, 4.90 ms, 6.51 ms, 6 ×5. 0 ms), TI 1100 ms, and ﬂip angle 7 ◦.",1,0,0
10.1016/j.neuroimage.2022.118933,oasis-brains.org,"Oasis-1 Marcus et al.(2007) and Oasis-2 Marcus et al.(2010) The Open Access Series of Imaging Studies 1 and 2, are publicly avail-able (https://www.oasis-brains.org/) cross-sectional (Oasis-1) and lon-gitudinal (Oasis-1) studies covering non-demented and demented indi-viduals with very mild to moderate Alzheimer’s disease.",0,1,0
10.1016/j.neuroimage.2022.118933,fcon_1000.projects.nitrc.org/indi/abide/abide_ii.html,"The 0.8 mm isotropic T1-weighted MRI data is acquired on a 3T Siemens Magnetom Prisma scanner using a multi-echo MPRAGE (ME-MPRAGE) sequence with TR 3 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html. 4 https://www.humanconnectome.org/study/hcp-young-adult. 5 https://www.humanconnectome.org/study-hcp-lifespan-pilot. 2560 ms, 10 TEs (1.68 ms, 3.29 ms, 4.90 ms, 6.51 ms, 6 ×5. 0 ms), TI 1100 ms, and ﬂip angle 7 ◦.",1,0,0
10.1016/j.neuroimage.2022.118933,nitrc.org,"Since GPU mem-ory limitations render full volume 3D models impractical speciﬁcally for higher number of feature channels and output classes, top performing methods process the volume in slices (QuickNat, FastSurfer (Henschel et al., 2020; Roy et al., 2019)) or in large patches (DeepNat, SLANT, 2 www.nitrc.org. 3 L.",1,0,0
10.1016/j.neuroimage.2022.118933,humanconnectome.org/disease-studies,"As the only diﬀerence in the training corpus is the increased image resolution of 60 subjects, the gain 7 https://humanconnectome.org/disease-studies. 13 L.",0,1,0
10.1016/j.neuroimage.2022.118933,surfer.nmr.mgh.harvard.edu,"Thus, variable 6 http://surfer.nmr.mgh.harvard.edu/. 5 L.",0,0,1
10.1016/j.neuroimage.2022.118933,humanconnectome.org/study-hcp-lifespan-pilot,"The 0.8 mm isotropic T1-weighted MRI data is acquired on a 3T Siemens Magnetom Prisma scanner using a multi-echo MPRAGE (ME-MPRAGE) sequence with TR 3 http://fcon_1000.projects.nitrc.org/indi/abide/abide_II.html. 4 https://www.humanconnectome.org/study/hcp-young-adult. 5 https://www.humanconnectome.org/study-hcp-lifespan-pilot. 2560 ms, 10 TEs (1.68 ms, 3.29 ms, 4.90 ms, 6.51 ms, 6 ×5. 0 ms), TI 1100 ms, and ﬂip angle 7 ◦.",1,0,0
10.1016/j.neuroimage.2022.118933,neuromorphometrics.com,(http://Neuromorphometrics.com/)).,0,0,1
10.1016/j.neuroimage.2022.118933,fcon_1000.projects.nitrc.org/indi/abide/abide_i.html,Scanner and sequence param-eters vary depending on the site and can be accessed on the ABIDE web-site (https://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html).,0,0,1
10.1016/j.neuroimage.2022.118933,github.com/deepmi/fastsurfer,A note on reproducibility: Authors wishing to compare their methods to Fast-SurferVINN and FastSurferCNN are strongly encouraged to download our code from our repository (github.com/DeepMI/FastSurfer).,1,0,0
10.1016/j.neuroimage.2022.119019,3.1.9.4,"Before the experiment, the GPower 3.1.9.4 was used to calculate the approximate sample size in advance.",1,0,0
10.1016/j.neuroimage.2022.119415,bic.mni.mcgill.ca/servicessoftware/minc,"Im-ages were then ﬁrst linearly (using a nine-parameter rigid registra-tion) and then nonlinearly registered to an average brain template (MNI ICBM152–2009c) (Manera et al., 2020) using MNI MINC tools (http://www.bic.mni.mcgill.ca/ServicesSoftware/MINC) and Advanced Normalization Tools (ANTS) software (http://stnava.github.io/ANTs/), respectively.",1,0,0
10.1016/j.neuroimage.2022.119415,camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/pdfs/camcan700_mr_params.pdf,For detailed acquisition param-eters see: https://camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/pdfs/CAMCAN700_MR_params.pdf.,0,0,1
10.1016/j.neuroimage.2022.119415,nist.mni.mcgill.ca/?p,"(Xiao et al., 2019), including 11 subcortical regions in each hemisphere (http://nist.mni.mcgill.ca/?p = 1209).",0,0,1
10.1016/j.neuroimage.2022.119415,stnava.github.io/ants,"Im-ages were then ﬁrst linearly (using a nine-parameter rigid registra-tion) and then nonlinearly registered to an average brain template (MNI ICBM152–2009c) (Manera et al., 2020) using MNI MINC tools (http://www.bic.mni.mcgill.ca/ServicesSoftware/MINC) and Advanced Normalization Tools (ANTS) software (http://stnava.github.io/ANTs/), respectively.",1,0,0
10.1016/j.neuroimage.2022.119415,computecanada.ca/home,Authors thank Compute Canada (https://www.computecanada.ca/home) for the usage of the computing resources in the current work.,0,0,1
10.1016/j.neuroimage.2022.119415,cam-can.org/index.php?content,"1) Training dataset Data used to train the brain age prediction model included par-ticipants with T1-weighted MRI data available from the second stage of the Cambridge Centre for Ageing and Neuroscience (Cam-CAN, https://www.cam-can.org/index.php?content = dataset) dataset, described in Shafto et al.",0,1,0
10.1016/j.neuroimage.2022.118889,bobspunt.com/bspmview,"The coordinates of signiﬁcant eﬀects were reported in the Mon-treal Neurological Institute (MNI) space and labeled according to Automated Anatomical Labeling (AAL2) (Rolls et al., 2015b) atlas with the use of bspmview (http://www.bobspunt.com/bspmview).",1,0,0
10.1016/j.neuroimage.2022.118889,exp.lobi.nencki.gov.pl,"Each dot represents a single word, blue – disgust, red – fear, yellow –n e u t r a l ; (dataset available as an interactive browser: http://exp.lobi.nencki.gov.pl/nawl-analysis)., After the second session and a short break, the third ex-perimental session started during which subjective aﬀective ratings were collected through a web application running on a local server (http://exp.lobi.nencki.gov.pl/).",0,1,0
10.1016/j.neuroimage.2022.118889,exp.lobi.nencki.gov.pl/nawl-analysis,"Using available aﬀective ratings and a novel method of classiﬁcation based on Euclidean distances (Wierzba et al., 2015), a total of 140 words eliciting disgust, 140 words eliciting fear, and 140 neutral words was selected through an interac-tive browser available at https://exp.lobi.nencki.gov.pl/nawl-analysis and custom-made algorithms.",0,0,1
10.1016/j.neuroimage.2022.118889,tools.robjellis.net,"Because individual brain masks not always included all voxels comprising anatomically deﬁned (based on the MNI coordinates) PRC region, we used the Imcalc tool (http://tools.robjellis.net/) for the SPM toolbox and constructed indi-vidual masks for the purpose of the analyses presented here.",1,0,0
10.1016/j.neuroimage.2022.118889,osf.io/5fmpk/?view_only,nencki.gov.pl/research/18/All data analysed and presented in the current manuscript are pub-licly available: https://osf.io/5fmpk/?view_only = 0db2f4f7c230455c 8ﬀ84c5346a7400c.,0,1,0
10.1016/j.neuroimage.2022.118889,mccauslandcenter.sc.edu/mricrogl/home,Results were visualized with the use of MRIcroGL (http://www.mccauslandcenter.sc.edu/mricrogl/home).,1,0,0
10.1016/j.neuroimage.2022.118889,nencki.gov.pl/research/18/all,nencki.gov.pl/research/18/All data analysed and presented in the current manuscript are pub-licly available: https://osf.io/5fmpk/?view_only = 0db2f4f7c230455c 8ﬀ84c5346a7400c.,0,1,0
10.1016/j.neuroimage.2022.118889,exp.lobi.nencki.gov.pl/nawl-analysis,"Each dot represents a single word, blue – disgust, red – fear, yellow –n e u t r a l ; (dataset available as an interactive browser: http://exp.lobi.nencki.gov.pl/nawl-analysis).",0,1,0
10.1016/j.neuroimage.2022.119003,github.com/fraimondo/lg_sleep_paper,All scripts used for the analysis after pre-processing the data are available at https://github.com/fraimondo/lg_sleep_paper.,1,0,0
10.1016/j.neuroimage.2022.119167,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,Data and code availability statements All MRI data (DTI and T1) is available from the Human Connectome Project (HCP) 1200 Subjects release image and be-havioral data (https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release).,0,1,0
10.1016/j.neuroimage.2022.119167,fmrib.ox.ac.uk/fsl,"To construct a structural connectivity matrix, we followed a conventional pipeline for probabilistic ﬁber tracking using the FSL toolbox (https://www.fmrib.ox.ac.uk/fsl) in the individual diﬀusion MRI space (Behrens et al., 2007).",1,0,0
10.1016/j.neuroimage.2022.119167,ﬁl.ion.ucl.ac.uk/spm/software/spm12,All voxels within the white matter (after segmentation using SPM12 available at https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12) were used as seed voxels.,1,0,0
10.1016/j.neuroimage.2022.119267,neurovault.org/collections/10477,"All seed regions were func-tionally deﬁned from our whole-brain analyses, and can be found on Neurovault (https://neurovault.org/collections/10477).",0,0,1
10.1016/j.neuroimage.2022.119267,github.com/dvs-lab/srndna-trustgame,Analysis code can be found on https://github.com/DVS-Lab/srndna-trustgame.,1,0,0
10.1016/j.neuroimage.2022.119267,antsbrainextraction.sh,"The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workﬂow (from ANTs), using OASIS30ANTs as target template.",1,0,0
10.1016/j.neuroimage.2022.119267,openneuro.org/datasets/ds003745,"Smith: Conceptualization, Methodology, Software, Validation, Formal Analysis, Writing-Original Draft, Writing-Review & Editing, Funding Acquisition, Project adminis-tration, Supervision, Visualization, Data Curation, Resources Data and code availability Data can be found on https://openneuro.org/datasets/ds003745.",0,1,0
10.1016/j.neuroimage.2022.119267,openneuro.org,"In addition, we also collected T2-weighted structural images (TR: 3.2 s; TE: 567 ms; matrix 192 ×192; voxel size: 1.0 mm 3 ; 192 slices; ﬂip angle: 120°); these images are in-cluded with our data on OpenNeuro.org, but we did not use them in our preprocessing or analyses.",0,1,0
10.1016/j.neuroimage.2022.119267,aspredicted.org/mvz_odi,"This sample size was pre-registered (https://aspredicted.org/MVZ_ODI), determined a priori before data collection, and limited largely by available funding for data collection for this project.",0,0,1
10.1016/j.neuroimage.2022.119133,librow.com/articles/article-13,"R-peaks were then identiﬁed using an online sample software package (http://www.librow.com/articles/article-13 ; Petzschner et al., 2019) and then data were down-sampled to 512 Hz. 3 L.",1,0,0
10.1016/j.neuroimage.2022.119319,data.developingconnectome.org/structural,"Data availability The imaging and collateral data from the dHCP can be downloaded by registering at https://data.developingconnectome.org/Structural connectivity networks and code used to predict age at birth and age at scan are available in https://github.com/CoDe-Neuro/Predicting-age-and-clinical-risk-from-the-neonatal-connectome 3., Data and code availability statement The imaging and collateral data from the dHCP can be downloaded by registering at https://data.developingconnectome.org/Structural connectivity networks and code used to predict age at birth and age at scan are available in https://github.com/CoDe-Neuro/Predicting-age-and-clinical-risk-from-the-neonatal-connectome Credit authorship contribution statement Yassine Taoudi-Benchekroun: Formal analysis, Investigation, Methodology, Software, Writing – original draft, Writing –r e v i e w & editing.",0,1,0
10.1016/j.neuroimage.2022.119319,github.com/code-neuro/predicting-age-and-clinical-risk-from-the-neonatal-connectome,"Data availability The imaging and collateral data from the dHCP can be downloaded by registering at https://data.developingconnectome.org/Structural connectivity networks and code used to predict age at birth and age at scan are available in https://github.com/CoDe-Neuro/Predicting-age-and-clinical-risk-from-the-neonatal-connectome 3., To enable this, our predictive algorithms have been made publicly available (https://github.com/CoDe-Neuro/Predicting-age-and-clinical-risk-from-the-neonatal-connectome)., Data and code availability statement The imaging and collateral data from the dHCP can be downloaded by registering at https://data.developingconnectome.org/Structural connectivity networks and code used to predict age at birth and age at scan are available in https://github.com/CoDe-Neuro/Predicting-age-and-clinical-risk-from-the-neonatal-connectome Credit authorship contribution statement Yassine Taoudi-Benchekroun: Formal analysis, Investigation, Methodology, Software, Writing – original draft, Writing –r e v i e w & editing.",1,0,0
10.1016/j.neuroimage.2022.119623,clinicaltrials.gov,"The three most frequently used platforms are: (1) OSF, a platform that can also be used to share additional information about the study/project (such as data and code), with multiple templates and forms for diﬀerent types of pre-registration, in addition to extensive resources about pre-registration and other open science practices; (2) aspredicted.org, a simpliﬁed form for pre-registration (Simmons et al., 2021); and (3) clinicaltrials.gov, which is used for registration of clinical trials in the U.S.",0,0,1
10.1016/j.neuroimage.2022.119623,brainlife.io/datasets,"Indeed, DataLad and Brain-life interact nicely with one another and all published datasets retrieved by DataLad are readily accessible at brainlife.io/datasets.",0,1,0
10.1016/j.neuroimage.2022.119623,brainlife.io/apps,"To ensure long-term preservation of the shared code, we suggest us-ing version control systems such as Git, and social coding platforms such as GitHub in combination with an archival database for assign-ing permanent DOI to code served for research, for instance, Zenodo (Troupin et al., 2018), brainlife.io/apps (Avesani et al., 2019) or Soft-ware Heritage (Di Cosmo, 2018).",0,1,0
10.1016/j.neuroimage.2022.119623,oreoni.github.io,"An online version of this resource can be found at https://oreoni.github.io., In addition, the con-tent is available online as a Jupyter Book at https://oreoni.github.io (https://doi.org/10.5281/zenodo.7083031).",0,0,1
10.1016/j.neuroimage.2022.119623,ohbm-environment.org,"Acquiring and analyzing data also has a substantial environmental cost, which can be minimized when research data and products are shared and reused 1 https://ohbm-environment.org/., Although these issues require ongoing deep introspection and cannot be solved solely by adopting the 1 https://ohbm-environment.org/. 14 G.",0,0,1
10.1016/j.neuroimage.2022.119623,aspredicted.org,"The three most frequently used platforms are: (1) OSF, a platform that can also be used to share additional information about the study/project (such as data and code), with multiple templates and forms for diﬀerent types of pre-registration, in addition to extensive resources about pre-registration and other open science practices; (2) aspredicted.org, a simpliﬁed form for pre-registration (Simmons et al., 2021); and (3) clinicaltrials.gov, which is used for registration of clinical trials in the U.S.",0,0,1
10.1016/j.neuroimage.2022.119623,protocols.io,"Public SOPs can serve as examples, as can protocols published on specialized sites (e.g., Protocol Exchange, protocols.io, Nature Protocols; see Table S1).",0,0,1
10.1016/j.neuroimage.2022.118929,osf.io/f8jqd,"The code and simu-lated signals are freely provided at https://osf.io/f8jqd/., Data and Code Availability Statement The code and simulated signals are freely provided at https://osf.io/f8jqd/.",1,0,0
10.1016/j.neuroimage.2022.119726,ox.ac.uk/spet4877/ihcpy,ox.ac.uk/spet4877/ihcpy.,0,0,1
10.1016/j.neuroimage.2022.119726,open.win.ox.ac.uk/digitalbrainbank,The data will soon be available on https://open.win.ox.ac.uk/DigitalBrainBank/.,0,1,0
10.1016/j.neuroimage.2021.118820,fmrib.ox.ac.uk/fsl,"Task-based fMRI data processing was carried out using FEAT (FMRI Expert Analysis Tool) Version 6.00, part of FSL (FMRIB’s Software Library, www.fmrib.ox.ac.uk/fsl).",1,0,0
10.1016/j.neuroimage.2022.119079,r-project.org,"The mean of the relative diﬀerences was calculated from the individual |CMRgl c 𝐼 𝐷𝐼 𝐹 − CMRgl c 𝐴𝐼𝐹 |CMRgl c 𝐴𝐼𝐹 Analyses were performed in R statistical software (R Core Team, 2017; R Foundation for Statistical Computing, Vienna, Austria; https://www.R-project.org).",1,0,0
10.1016/j.neuroimage.2022.119139,qunex.yale.edu,"Neuroimaging processing Structural and functional MRI data were pre-processed using HCP minimal preprocessing pipelines (Glasser et al., 2013) im-plemented through our multi-modal neuroimaging platform called Quantitative Neuroimaging Environment and Toolbox (Qu|Nex, https://qunex.yale.edu).",1,0,0
10.1016/j.neuroimage.2022.119139,balsa.wustl.edu,"Reward and Loss Incentives Improve Spatial Work-ing Memory by Shaping Trial-by-Trial Posterior Frontoparietal Signals Neuroimaging data will be uploaded for public sharing on BALSA, https://balsa.wustl.edu.",0,1,0
10.1016/j.neuroimage.2022.119015,projecttemplate.net/index.html,"Method and materials To address increasing calls for transparency and reproducibility of neuroimaging researches, pipelines of data analysis and correspond-ing scripts used in this study were documented in a standardized fash-ion by using the “ProjectTemplate ”p a c k a g e implemented in R Studio (http://projecttemplate.net/index.html).",1,0,0
10.1016/j.neuroimage.2022.119015,freesurfer.net,"Subsequently, resampling was exerted using “fsaverage5 ”f u n c t i o n of FreeSurfer (http://www.freesurfer.net).",1,0,0
10.1016/j.neuroimage.2022.119015,rfmri.org/dpabi,"fMRIPrep (V1.3.0) pipeline embodied in DAPBI toolbox (http://rfmri.org/dpabi) was leveraged for robustly self-adjusted pre-processes of functional fMRI data (Esteban et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119015,osf.io/zcahs,"Considering the increasing concerns regarding p-hacking and test-retest reliabilities, we adhered to recommendation of APA (American Psychological Association) by providing pre-registration for major aims and purposes of this study in Open Framework Science (OFS, welcome to visit: https://osf.io/zcahs/) repository., Ethics statements Data availability All of raw data, resultant ﬁles and interactive NFITI images have been submitted at the OSF repository (https://osf.io/zcahs/) for open accesses., Data and code availability statement Study protocols and hypotheses were pre-registered on the Open Sci-ence Framework (OSF) (https://osf.io/zcahs/)., Raw data, protocols and analysis scripts are available openly at the OSF (https://osf.io/zcahs/).",0,1,0
10.1016/j.neuroimage.2022.119015,chinageoss.org/dsp/home,"Drawing upon shared data for Satellite Remote Sensing Image (S-RSI) of Charge-coupled Device (CCD) in Chinese government (http://www.chinageoss.org/dsp/home/), the present study also took disparities of ecological environment for participants into account (see SI Method).",0,0,1
10.1016/j.neuroimage.2022.118954,ski.clps.brown.edu/hddm_docs/howto.html#outliers,"We adopted this criterion because percentages from 0.01% to 10% are suﬃcient to capture outliers for the DDM analysis (http://ski.clps.brown.edu/hddm_docs/howto.html#outliers , Ratcliﬀ and Tuerlinckx, 2002).",0,0,1
10.1016/j.neuroimage.2022.118954,marsbar.sourceforge.net,The contrast values were extracted from the ROI using MarsBaR (http://marsbar.sourceforge.net).,1,0,0
10.1016/j.neuroimage.2022.118954,github.com,The code used to analyze the data are available at github.com (https://github.com/Tianyugao526/CultureDiﬀerence_ChineseDanish_Belive).,1,0,0
10.1016/j.neuroimage.2022.118954,osf.io/zp6kv,Data/code availability statement Behavioral and imaging data that support the ﬁndings of this study are available at osf (https://osf.io/zp6kv/).,0,1,0
10.1016/j.neuroimage.2022.118954,github.com/tianyugao526/culturediﬀerence_chinesedanish_belive,The code used to analyze the data are available at github.com (https://github.com/Tianyugao526/CultureDiﬀerence_ChineseDanish_Belive).,1,0,0
10.1016/j.neuroimage.2022.119748,nitrc.org/projects/wfu_pickatlas,"Three thalamic ROIs (the LGN, PUL, and MD) were anatomically deﬁned using the Wake Forest Uni-versity Pick Atlas (https://www.nitrc.org/projects/wfu_pickatlas).",1,0,0
10.1016/j.neuroimage.2022.119748,ion.ucl.ac.uk/spm/software/spm12,"ion.ucl.ac.uk/spm/software/spm12) and MRIQC (https://www.mriqc., Thalamocortical functional activation Functional activation of the fusion and non-fusion conditions was ex-amined using a standard general linear model (GLM) pipeline of SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12).",1,0,0
10.1016/j.neuroimage.2022.119748,web.mit.edu/swg/software.htm,Beta weights were aver-aged separately within these 12 ROIs and extracted for further analy-ses (temporal dynamics) using the region of interest extraction toolbox (http://web.mit.edu/swg/software.htm).,1,0,0
10.1016/j.neuroimage.2022.119748,github.com/cogmind1/2022ni_code,The analysis codes have been made publicly available on Github (https://github.com/cogmind1/2022NI_code).,1,0,0
10.1016/j.neuroimage.2022.119748,dbm.neuro.uni-jena.de/tfce,"To correct for multiple comparisons, a permutation-based voxel-wise non-parametric test was performed using the threshold-free cluster enhance-ment (TFCE) toolbox (http://dbm.neuro.uni-jena.de/tfce) (Smith and Nichols, 2009).",1,0,0
10.1016/j.neuroimage.2022.119748,readthedocs.io,readthedocs.io).,0,0,1
10.1016/j.neuroimage.2022.119748,ﬁl.ion.ucl.ac.uk/spm/software/spm12,Thalamocortical functional activation Functional activation of the fusion and non-fusion conditions was ex-amined using a standard general linear model (GLM) pipeline of SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12).,1,0,0
10.1016/j.neuroimage.2022.119194,brainmap.org,"Meta-analytic co-activation of regions conjointly activated by reappraisal and empathic perspective-taking To better understand the shared co-activation maps related to reappraisal and empathic perspective-taking and their respec-tive goals/perspectives, a large database of fMRI studies was used (BrainMap: http://www.brainmap.org/) and MACM was applied (Eickhoﬀet al., 2011 ; Kohn et al., 2014).",0,1,0
10.1016/j.neuroimage.2022.119274,registry.opendata.aws/hcp-openaccess,Data availability HCP datasets are available via (https://db.humanconnectome.org) and also https://registry.opendata.aws/hcp-openaccess/.,0,1,0
10.1016/j.neuroimage.2022.119274,db.humanconnectome.org,Data availability HCP datasets are available via (https://db.humanconnectome.org) and also https://registry.opendata.aws/hcp-openaccess/.,0,1,0
10.1016/j.neuroimage.2022.119274,github.com/ohba-analysis/hmm-mar,HMM codes are publicly available from HMM-MAR (multivariate autoregressive) toolbox (https://github.com/OHBA-analysis/HMM-MAR).,1,0,0
10.1016/j.neuroimage.2022.119274,cran.r-project.org/web/packages/mets/index.html,"This method was implemented in the R package mets (http://cran.r-project.org/web/packages/mets/index.html), adjusting for age and head motion.",1,0,0
10.1016/j.neuroimage.2022.118928,gauss-centre.eu,(www.gauss-centre.eu) for supporting this project by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercom-puter JUWELS at Jülich Supercomputing Centre (JSC).,0,0,1
10.1016/j.neuroimage.2022.118928,github.com/brainmodes/review_dynamicprimitives,Data and code availability The data and code used in this article can be downloaded from https://github.com/BrainModes/Review_DynamicPrimitives.,0,1,0
10.1016/j.neuroimage.2021.118786,hpc.nih.gov,This work utilized the computational resources of the NIH HPC Biowulf clus-ter (https://hpc.nih.gov).,1,0,0
10.1016/j.neuroimage.2022.119399,brain.labsolver.org/diﬀusion-mri-templates/tractography,(3) the regional synchronies were compared between the two functional states using the HCP tractography atlas of WM available on the website: http://brain.labsolver.org/diﬀusion-mri-templates/tractography.,1,0,0
10.1016/j.neuroimage.2022.119399,github.com/yuzhaomri/faiw_pca,Code and data availability An implementation of the methods proposed in this work wil be made available as a MATLAB package on GitHub at https://github.com/YuZhaoMRI/FAIW_PCA.,1,0,0
10.1016/j.neuroimage.2022.119399,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"GLM-based activation mapping was performed with the preprocessed task fMRI data that underwent identical spatial smoothing and regres-sions of nuisance signals including cerebrospinal ﬂuid and head mo-tion; the GLM based activation mapping was implemented using SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12), where the task conditions were convolved with a canonical HRF to generate ﬁve pre-dictors that were included in the motor model –right hand, left hand, right foot, left foot, and tongue; each predictor covered the duration of 10 movements trials (12 s); the 3 s cue period prior to each motor block was modeled separately to account for visual activation related to the cue word presented on the screen at the beginning of each block; lin-ear contrasts were computed to estimate activation for each movement type versus baseline and versus all other movement type; on a single subject level, conditions were contrasted against each other to create a parametric image that reﬂected the percent signal changes invoked by the tasks; on the group level, a one sample t -test was applied to the parametric images across all subjects to create a map of the brain acti-vation; ﬁnally, activated voxels were reported at a threshold P < 0.05 with family-wise error rate (FWE) correction.",1,0,0
10.1016/j.neuroimage.2022.119053,github.com/harmonic-minimization,"The codes of Har-moni, simulating toy examples, as well as analyzing the simulated EEG and real data are available at github.com/harmonic-minimization.",1,0,0
10.1016/j.neuroimage.2022.119096,openneuro.org/datasets/ds001875/versions/1.0.3,"https://openneuro.org/datasets/ds001875/versions/1.0.3 Shen, K., Bezgin, G., Schirner, M., Ritter, P., Everling, S., and McIn-tosh, A.",0,0,1
10.1016/j.neuroimage.2022.119096,humanconnectome.org/study/hcp-young-adult,For detailed recruitment information and for a full list of procedures see: https://www.humanconnectome.org/study/hcp-young-adult.,0,0,1
10.1016/j.neuroimage.2022.119096,github.com/el-suri/analyse-monkey-brain-tracts,"All in-house scripts used for pre-processing with a step-by-step guide can be found online at https://github.com/El-Suri/Analyse-Monkey-Brain-Tracts., https://github.com/El-Suri/Analyse-Monkey-Brain-Tracts 14 S.C.",1,0,0
10.1016/j.neuroimage.2022.119096,neurovault.org/collections/3245,"The BNST mask was downloaded from the NeuroVault website (https://neurovault.org/collections/3245), origi-nally uploaded by Theiss et al (Mai et al., 2015 ; Theiss et al., 2017 ; Tillman et al., 2018).",0,0,1
10.1016/j.neuroimage.2022.119096,db.humanconnectome.org,"https://db.humanconnectome.org/, Van Essen, D.",0,0,1
10.1016/j.neuroimage.2022.119096,brainder.org/2016/08/01/three-hcp-utilities,"The pedigree ﬁle was cre-ated using the HCP2Solar MATLAB function, a tool speciﬁcally designed for the HCP participants (https://brainder.org/2016/08/01/three-hcp-utilities) (Winkler et al., 2015).",1,0,0
10.1016/j.neuroimage.2022.119096,github.com/washington-university/hcppipelines,Full pre-processing steps and the code to run the HCP pre-processing pipeline can be found at https://github.com/Washington-University/HCPpipelines.,1,0,0
10.1016/j.neuroimage.2022.119096,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,"NeuroImage 253 (2022) 119096 subject reference manual https://humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release (Glasser et al., 2013 ; Sotiropoulos et al., 2013 ; Van Essen et al., 2012).",0,0,1
10.1016/j.neuroimage.2022.119736,osf.io/vr6qj/?,Data and code availability Derived data and analysis code for this project are accessi-ble at: https://osf.io/vr6qj/?,1,0,0
10.1016/j.neuroimage.2022.119736,medizin.uni-tuebingen.de/kinder/en/research/neuroimaging/software,The LI-tool toolbox is freely available at: http://www.medizin.uni-tuebingen.de/kinder/en/research/neuroimaging/software/.,1,0,0
10.1016/j.neuroimage.2022.119364,github.com/kangjoo/arousal-rsfmripupil-hub,Several additional MATLAB scripts for SPARK adapted for this study are available on https://github.com/Kangjoo/arousal-rsfMRIpupil-hub.,1,0,0
10.1016/j.neuroimage.2022.119364,github.com/multifunkim/spark-matlab,The MATLAB scripts for SPARK im-plementation are available on https://github.com/multifunkim/spark-matlab.,1,0,0
10.1016/j.neuroimage.2022.119364,github.io/webapp/index.html,github.io/webapp/index.html).,0,0,1
10.1016/j.neuroimage.2022.119364,openneuro.org/datasets/ds003673,Imaging and pupillometry data are freely available on https://openneuro.org/datasets/ds003673.,0,1,0
10.1016/j.neuroimage.2022.119364,sites.google.com/site/bctnet,"The Brain Connectivity toolbox (https://sites.google.com/site/bctnet/) was used to compute two net-work measures for each node: within-module degree z-score and par-ticipant coeﬃcient (Rubinov and Sporns, 2010).",1,0,0
10.1016/j.neuroimage.2022.119631,github.com/gallantlab,"The Filter model was created by ﬁrst computing the Gabor ﬁlter outputs for the last high-contrast frame of the videos (with the Com-puteStaticGabors function; https://github.com/gallantlab/; Nishimoto, Vu, Naselaris, Benjamini, Yu, & Gallant, 2011), and by correlating the ﬁlter outputs across stimuli and subtracting the correlation matrix from one.",1,0,0
10.1016/j.neuroimage.2022.119631,cmusatyalab.github.io/openface,"The OpenFace model was calculated based on the OpenFace algorithm (https://cmusatyalab.github.io/openface/; Amos, Ludwiczuk, & Satya-narayanan, 2016), which is trained to separate diﬀerent face identities from each other.",1,0,0
10.1016/j.neuroimage.2022.118990,osf.io/28u6b/•,"Data/code availability statement • The MATLAB code for MNS delivery is available on OSF https://osf.io/28u6b/• The MATLAB code for analysis will be made available on OSF https://osf.io/28u6b/• As the MRI data used during analysis were not collected by the re-searchers involved, we do not have ethical approval to share these.",1,0,0
10.1016/j.neuroimage.2022.119753,bic.mni.mcgill.ca/servicessoftware/civet,"We estimated cortical thickness based on the estab-lished CIVET pipeline which is a sequential image processing pipeline (CIVET, http://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET) (Ad-Dab’bagh et al., 2006).",1,0,0
10.1016/j.neuroimage.2022.119753,github.com/shyook83/eeg-bai,"Source code for the proposed brain age prediction model is available at github link (https://github.com/shyook83/EEG-BAI)., Code availability Source code for the proposed brain age prediction model is available at github link (https://github.com/shyook83/EEG-BAI).",1,0,0
10.1016/j.neuroimage.2022.119052,kaken.nii.ac.jp/en/grant/kakenhi-project-20k19867,"Acknowledgements This research was funded by the JSPS KAKENHI grant (20K19867, https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-20K19867/) from the Japan Society for the Promotion of Science.",0,0,1
10.1016/j.neuroimage.2022.119052,github.com/mygit-yokoyamahiroshi/changedetectsim,"The Python script of our proposed method is pro-vided in the following GitHub repository: https://github.com/myGit-YokoyamaHiroshi/ChangeDetectSim 2.3., The sample code of all numerical simulation described in this paper (both in the main manuscript and Supplementary Materials) is available at Github: https://github.com/myGit-YokoyamaHiroshi/ChangeDetectSim Procedure of the simulation In this simulation, we applied our proposed method to the synthetic data generated by a phase-coupled oscillator model with three oscilla-tors to clarify whether our method can sequentially detect changes in the network couplings., The sample code of the numerical simulation described in this paper is available at Github: https://github.com/myGit-YokoyamaHiroshi/ChangeDetectSim Credit authorship contribution statement Hiroshi Yokoyama: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Resources, Data curation, Writing – original draft, Writing –r e v i e w & editing, Visualization, Funding acquisition.",1,0,0
10.1016/j.neuroimage.2022.119119,cometstool.com,"The MATLAB toolbox Comets2 was used to estimate the current density distribution (http://www.cometstool.com) (Lee et al., 2017) produced with our montage, stimulating electrode size and mean intensity applied for mu-tACS (0.35 mA). 3 A.",1,0,0
10.1016/j.neuroimage.2022.119422,mrt.uni-jena.de/simbio,"Open-source FEM solvers are available for MEG forward solution computation like DUNEuro (Schrader et al., 2021) and SimBio (https://www.mrt.uni-jena.de/simbio).",1,0,0
10.1016/j.neuroimage.2022.119014,fmrib.ox.ac.uk/fsl,"Linear registration of quantitative maps Before creating the corresponding atlases for both methods, the acquired intra-subject quantitative water content maps were reg-istered to subject-speciﬁc T 1 -weighted MPRAGE using FSL-FLIRT (http://www.fmrib.ox.ac.uk/fsl) with 12-parameter aﬃne transforma-tion (Collins, 1995 ; Jenkinson and Smith, 2001).",1,0,0
10.1016/j.neuroimage.2022.119014,nitrc.org/projects/art,"Figure 3 a shows an aligned transverse slice of the T 1 -weighted MPRAGE scan and constitutes the input registered to the correspond-ing slice from the MNI template (3b) using the “3dwarper ”p r o -gram, which is part of the Automatic Registration Toolbox (ART) (http://www.nitrc.org/projects/art) (Ardekani et al., 2005).",1,0,0
10.1016/j.neuroimage.2022.119010,pstnet.com/eprime.cfm,"1) was presented in an event related de-sign via E-prime 2.0 (http://www.pstnet.com/eprime.cfm , Psychology Software Tools, Pittsburgh, Pennsylvania, USA).",1,0,0
10.1016/j.neuroimage.2022.119010,ﬁl.ion.ucl.ac.uk/spm,"Preprocessing was conducted using standard procedures in SPM12 (Statistical Parametric Mapping, http://www.ﬁl.ion.ucl.ac.uk/spm/) implemented in MATLAB 2014a (MathWorks, Inc., USA), including the following steps: (1) discarding the ﬁrst 10 vol of each run to allow MRI equilibration and active noise cancelling, (2) slice time correction, (3) head motion correction using a six-parameter rigid body algorithm, (4) tissue segmentation and skull-stripped bias-correction for the high-resolution structural images, (5) co-registration of mean functional im-ages to structural images, (6) normalization (resampled at 3 ×3 ×3 mm) to Montreal Neurological Institute (MNI) space, and (7) spatial smooth-ing with an 8 mm full-width at half maximum (FWHM) Gaussian ker-nel., Analyses were conducted using standard processing scripts in SPM12 (Statistical Parametric Mapping, http://www.ﬁl.ion.ucl.ac.uk/spm/) implemented in MATLAB 2014a (MathWorks, Inc., USA), and SPSS 24 (IBM SPSS Statistics for Windows, Version 24.0.",1,0,0
10.1016/j.neuroimage.2022.119010,osf.io/yzhbk,Data and code availability statement Behavioral data and statistical parametric maps at the group level from all fMRI analyses are available via the Open Science Framework (https://osf.io/yzhbk/).,0,1,0
10.1016/j.neuroimage.2022.119010,optoacoustics.com,OptoActive MRI headphones (http://www.optoacoustics.com/) were used to reduce acoustic noise exposure for the participants during MRI data acquisition.,0,0,1
10.1016/j.neuroimage.2022.119010,atlas.brainnetome.org,"The signiﬁcant clusters were further localized according to the Brainnetome Atlas which includes a whole-brain coverage at a functionally and ﬁne-grained deﬁned level including several subdivisions (http://atlas.brainnetome.org ; Fan et al., 2016)., Accord-ing to the Brainnetome Atlas (http://atlas.brainnetome.org ; Fan et al., 2016), the cluster is located at the left IFG subregion of opercular 6 L.",0,0,1
10.1016/j.neuroimage.2022.119156,sites.google.com/site/cartool,•Krzanowski-Lai Index: A measure of within-clusters dispersion The mean/meta-criterion calculation is implemented in the free academic software Cartool (https://sites.google.com/site/cartool community/).,1,0,0
10.1016/j.neuroimage.2022.119156,repository.cam.ac.uk/handle/1810/252736,Three recent works in literature used the EEG microstate ap-proach to analyze the University of Cambridge data repository https://www.repository.cam.ac.uk/handle/1810/252736 and study the eﬀects of mild to moderate sedation induced by anesthetics.,0,1,0
10.1016/j.neuroimage.2021.118827,neurovault.org,"Imaging data are available at neurovault.org (https://www.neurovault.org/collections/5879/)., Imaging data are available at neurovault.org (https://www.neurovault.org/collections/5879/).",0,1,0
10.1016/j.neuroimage.2021.118827,github.com,"Data and code availability Behavioral data and scripts are available at github.com (https://github.com/AnneSaulin/complex_motivations)., Code and data availability Behavioral data and scripts are available at github.com (https://github.com/AnneSaulin/complex_motivations).",0,1,0
10.1016/j.neuroimage.2021.118827,neurovault.org/collections/5879,"Imaging data are available at neurovault.org (https://www.neurovault.org/collections/5879/)., Imaging data are available at neurovault.org (https://www.neurovault.org/collections/5879/).",0,1,0
10.1016/j.neuroimage.2021.118827,1.0.0.34,"The projectile was shot against the cuticle of the left index ﬁnger using air pressure (Impact Stimulator, Labortechnik Franken, Release 1.0.0.34).",1,0,0
10.1016/j.neuroimage.2021.118827,github.com/annesaulin/complex_motivations,"Data and code availability Behavioral data and scripts are available at github.com (https://github.com/AnneSaulin/complex_motivations)., Code and data availability Behavioral data and scripts are available at github.com (https://github.com/AnneSaulin/complex_motivations).",0,1,0
10.1016/j.neuroimage.2022.119501,parralab.org/roast,"Current ﬂow modelling E-ﬁeld modelling for tDCS was performed using Realistic vOlumet-ric Approach to Simulate Transcranial Electric Stimulation (ROAST) v3.0 software package (https://www.parralab.org/roast/) (Huang et al., 2019 a).",1,0,0
10.1016/j.neuroimage.2022.119501,getdp.info,"The FEM is then solved for current distribution using getDP FEM solver (https://getdp.info/) (Dular et al., 1998).",1,0,0
10.1016/j.neuroimage.2022.119501,iso2mesh.sourceforge.net/cgi-bin/index.cgi,"To generate the ﬁnite element model (FEM), ROAST creates a volumetric mesh from 3D multi-domain images using iso2mesh toolbox (http://iso2mesh.sourceforge.net/cgi-bin/index.cgi) (Fang and Boas, 2009).",1,0,0
10.1016/j.neuroimage.2022.119501,surfer.nmr.mgh.harvard.edu,"HCP extracted pial and white matter surfaces using FreeSurfer 5.1 software (http://surfer.nmr.mgh.harvard.edu/) plus customised steps to improve surface accuracy (for more detail see (Glasser et al., 2013)).",1,0,0
10.1016/j.neuroimage.2022.119501,github.com/caryse/tdcs_currentdirection,"Code for extracting E-ﬁeld at cortical surface is available here: https://github.com/caryse/tdcs_currentdirection/., The code used to extract E-ﬁeld data from these scans is available here https://github.com/caryse/tdcs_currentdirection/.",1,0,0
10.1016/j.neuroimage.2022.119501,ida.loni.usc.edu/login/jsp,"Structural MRIs Fifty T1-weighted structural MRIs of healthy adults (aged 22–35, 21 males, 29 females) were randomly selected from the Human Connec-tome Project (HCP) database (http://ida.loni.usc.edu/login/jsp).",0,1,0
10.1016/j.neuroimage.2022.119501,ﬁl.ion.ucl.ac.uk/spm,"MR images are transformed into RAS space and seg-mented into grey matter, white matter, cerebrospinal ﬂuid (CSF), bone, skin, and air cavities using SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119727,clinicaltrials.gov,"In small studies of preterm infants with DWMA at ✩ ClinicalTrials.gov Identiﬁer: NCT03345069 ∗ Corresponding author at: Professor of Pediatrics, Cincinnati Children’s Hospital, 3333 Burnet Ave, MLC 7009, Cincinnati, OH 45229.",0,0,1
10.1016/j.neuroimage.2022.119727,ac.uk/fsl/fslwiki,"ac.uk/fsl/fslwiki/, version 5.0.11).",1,0,0
10.1016/j.neuroimage.2022.119727,github.com/jekline2/dwma.git,Data and code availability statement Parameter maps used in this study can be found on GitHub (https://github.com/Jekline2/DWMA.git).,1,0,0
10.1016/j.neuroimage.2022.119727,mrtrix3.org,"CSD model We processed all b2000 diﬀusion-weighted data in MRtrix3 (www.mrtrix3.org), a CSD-enabled software (Tournier et al., 2019), using our published methods for preprocessing (PCA denoising, cor-rection for Gibbs ringing; motion artifacts; eddy currents; and the susceptibility-induced oﬀ-resonance ﬁeld, global intensity normaliza-tion, and upsampling to 1.3 mm isotropic resolution) (Chandwani et al., 2022).",1,0,0
10.1016/j.neuroimage.2022.119727,nitrc.org/projects/noddi_toolbox,"These combined multi-shell data were processed using the NODDI tool-box (https://www.nitrc.org/projects/noddi_toolbox/), version 1.04.",1,0,0
10.1016/j.neuroimage.2022.119304,resolution.in,"Introduction Optical coherence tomography (OCT) is an imaging technique that uses low temporal coherence light to obtain cross sectional images of an object at 1 − 20 μm resolution.In addition, block-face imaging of OCT enables large volumetric reconstruction (tens of cubic centimeters) of ex vivo human brain tissues, by combining with serial sectioning using a vibratome (Magnain et al., 2014).",0,0,1
10.1016/j.neuroimage.2022.119045,fmri.wfubmgc.edu/cms/software,"We computed correlations (element-wise multiplication) between each component’s sample-speciﬁc IC map (converted to a binary mask) and a-priori binary mask maps of gray matter, white matter, and cere-brospinal ﬂuid as provided by the WFU Pickatlas (Maldjian et al., 2003 ; http://fmri.wfubmgc.edu/cms/software).",1,0,0
10.1016/j.neuroimage.2022.119045,www0.cs.ucl.ac.uk/staﬀ/gridgway/vbm/get_totals.m,Whole-brain gray matter volume was calculated using the MATLAB script “get_totals ”provided by Ridg-way (http://www0.cs.ucl.ac.uk/staﬀ/gridgway/vbm/get_totals.m).,1,0,0
10.1016/j.neuroimage.2022.119045,icatb.sourcefttkorge.net,"First, group spatial ICA was conducted across all 85 participants using Group ICA of the fMRI Toolbox (GIFT ; http://icatb.sourcefttkorge.net/, version v4.0b; (Calhoun et al., 2009).",1,0,0
10.1016/j.neuroimage.2022.119045,alivelearn.net/xjview,We used xjView (http://www.alivelearn.net/xjview) to visualize the results.,1,0,0
10.1016/j.neuroimage.2022.119045,ﬁl.ion.ucl.ac.uk/spm,"MRI data preprocessing Resting-state MRI images were preprocessed using CONN functional connectivity toolbox (v.18.b; Whitﬁeld-Gabrieli and Nieto-Castanon, 2012) in conjunction with SPM12 software (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) on MATLAB R2018a (MathWorks Inc., Natick, MA, USA)., We preprocessed structural MRI images using SPM8 soft-ware (https://www.ﬁl.ion.ucl.ac.uk/spm/) and the VBM8 toolbox (http://dbm.neuro.uni-jena.de/vbm/) following the standard prepro-cessing pipeline (Liu and Feng, 2017).",1,0,0
10.1016/j.neuroimage.2022.119045,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"MRI data preprocessing Resting-state MRI images were preprocessed using CONN functional connectivity toolbox (v.18.b; Whitﬁeld-Gabrieli and Nieto-Castanon, 2012) in conjunction with SPM12 software (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) on MATLAB R2018a (MathWorks Inc., Natick, MA, USA).",1,0,0
10.1016/j.neuroimage.2022.119045,dbm.neuro.uni-jena.de/vbm,"We preprocessed structural MRI images using SPM8 soft-ware (https://www.ﬁl.ion.ucl.ac.uk/spm/) and the VBM8 toolbox (http://dbm.neuro.uni-jena.de/vbm/) following the standard prepro-cessing pipeline (Liu and Feng, 2017).",1,0,0
10.1016/j.neuroimage.2022.119553,computecanada.ca,This research was enabled in part by the support provided by Compute Ontario (www.computeontario.ca) and Compute Canada (www.computecanada.ca).,0,0,1
10.1016/j.neuroimage.2022.119553,humanconnectome.org/study/hcp-young-adult,"FreeSurfer 1 https://humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release 2 J., NeuroImage 262 (2022) 119553 Data Availability The data used in this study are available as part of the pub-licly available Human Connectome Project S1200 release (https://humanconnectome.org/study/hcp-young-adult).",0,1,0
10.1016/j.neuroimage.2022.119553,computeontario.ca,This research was enabled in part by the support provided by Compute Ontario (www.computeontario.ca) and Compute Canada (www.computecanada.ca).,0,0,1
10.1016/j.neuroimage.2022.119553,humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,FreeSurfer 1 https://humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release 2 J.,1,0,0
10.1016/j.neuroimage.2022.118871,docs.monai.io/en/latest/_modules/monai/networks/nets/densenet.html,"Our DenseNet brain-age models were adapted from the im-plementation available at Project MONAI (https://docs.monai.io/en/latest/_modules/monai/networks/nets/densenet.html), and all modelling was performed with PyTorch 1.7.1 (Paszke et al., 2019) using two NVIDIA RTX 2080 11 GB graphics processing units (GPU).",1,0,0
10.1016/j.neuroimage.2022.118871,github.com/midiconsortium/radreports,"To enable readers to generate large training datasets for brain-age model development us-ing data from their own institution, we have made our neuroradiology report classiﬁer training scripts, as well as a dedicated labelling ‘app’, available at https://github.com/MIDIconsortium/RadReports., A dedicated ‘labelling ‘app’ is made avail-able at https://github.com/MIDIconsortium/RadReports to enable read-ers to label their own neuroradiology report datasets for natural lan-guage processing model development.",1,0,0
10.1016/j.neuroimage.2022.118871,brain-development.org/ixi-dataset,"The scans were acquired at three diﬀerent London institu-tions between 2005 –2008 (Hammersmith Hospital, using a Phillips 3T system; Guy’s Hospital, using a Phillips 1.5T system; Institute of Psychia-try, using a GE 1.5T system), and can be downloaded from https://brain-development.org/ixi-dataset/.",0,0,1
10.1016/j.neuroimage.2022.118871,2.1.1.1,2.1.1.1.,0,0,1
10.1016/j.neuroimage.2022.118871,github.com/midiconsortium/brainage,"Scripts to enable readers to run our trained brain-age models using their own scans are available at https://github.com/MIDIconsortium/BrainAge., Data and code availability Scripts to enable readers to run our trained brain-age mod-els using their own scans are available at https://github.com/MIDIconsortium/BrainAge.",1,0,0
10.1016/j.neuroimage.2022.118871,2.1.1.2,"A subset of these examinations was identiﬁed as ‘radiologically normal for age’ (Section 2.1.1.2) and included for model training and testing., 2.1.1.2.",0,0,1
10.1016/j.neuroimage.2022.118871,2.1.1.3,A separate subset of examinations that were reported as having atrophy ‘excessive for age’ was also identiﬁed (section 2.1.1.3) and included for additional model testing.,0,1,0
10.1016/j.neuroimage.2022.118871,github.com/mic-dkfz/hd-bet,"NeuroImage 249 (2022) 118871 move non-brain tissue from all axial T2-weighted scans from GSTT (n scans = 13,806). 2 Further information about this open-access tool is available at https://github.com/MIC-DKFZ/HD-BET.",1,0,0
10.1016/j.neuroimage.2022.119099,eegstudy.org,"Data for this arti-cle have been documented according to the EEG Study Scheme standard (www.eegstudy.org), and containerized using the BIDS format (https://bids.neuroimaging.io/).",0,1,0
10.1016/j.neuroimage.2022.119099,bit.ly/31p2mrd,"The same Ultra-book was used for collecting the data, using the LiveAmpConnector (ver-sion 1.16, bit.ly/31P2mrd) and LSL (Lab Streaming Layer; Swartz Center for Computational Neuroscience and Kothe, 2015) LabRecorder (version 1.13, bit.ly/2ULAFhb) software to time-synchronously collect both the event markers from NBS Presentation and EEG channel data.",1,0,0
10.1016/j.neuroimage.2022.119099,bit.ly/2ulafhb,"The same Ultra-book was used for collecting the data, using the LiveAmpConnector (ver-sion 1.16, bit.ly/31P2mrd) and LSL (Lab Streaming Layer; Swartz Center for Computational Neuroscience and Kothe, 2015) LabRecorder (version 1.13, bit.ly/2ULAFhb) software to time-synchronously collect both the event markers from NBS Presentation and EEG channel data.",1,0,0
10.1016/j.neuroimage.2022.119099,openneuro.org/datasets/ds004033,"Datasets analyzed dur-ing the current study are available in the OpenNeuro repository, https://openneuro.org/datasets/ds004033.",0,1,0
10.1016/j.neuroimage.2022.119099,github.com/jscanlon275/jos_sync2,Data and code accessibility statement All MATLAB code for the project is available at https://github.com/jscanlon275/jos_sync2.,1,0,0
10.1016/j.neuroimage.2022.119099,github.com/raincloudplots/raincloudplots/tree/master/tutorial_matlab,Frequency lock-ing percentage is demonstrated using raincloud plots in Fig. 2 B (using MATLAB functions ksdensity and raincloud_plot: https://github.com/RainCloudPlots/RainCloudPlots/tree/master/tutorial_matlab; modiﬁed by authors).,1,0,0
10.1016/j.neuroimage.2022.119099,bids.neuroimaging.io,"Data for this arti-cle have been documented according to the EEG Study Scheme standard (www.eegstudy.org), and containerized using the BIDS format (https://bids.neuroimaging.io/).",0,1,0
10.1016/j.neuroimage.2022.119099,labeling.ucsd.edu,"Removed channels were then interpolated, and ICLabel (Pion-Tonachini et al., 2019 ; see labeling.ucsd.edu) was then used to identify and remove any IC’s classiﬁed with less than 70% probability of being due to brain activity.",1,0,0
10.1016/j.neuroimage.2021.118799,neobrains12.isi.uu.nl,"LINKS is an in-house developed learning-based multi-source integration framework for brain tissue segmenta-tion, which has been validated on MICCAI Neonatal Brain Segmentation (NeoBrainS12, https://neobrains12.isi.uu.nl/) and 6-month infant brain MRI Segmentation (iSeg-2017, https://iseg2017.web.unc.edu/) Grand Challenges, achieving state-of-the-art performance.",1,0,0
10.1016/j.neuroimage.2021.118799,lpbr.cn,The generated longitudinal cynomolgus macaque atlases will be publicly available on the website of Yunnan key laboratory of primate biomedical research (www.lpbr.cn) and NITRC (https://www.nitrc.org/projects/cyno_4d_atlas/) website to greatly fa-cilitate early brain development studies.,0,0,1
10.1016/j.neuroimage.2021.118799,stnava.github.io/ants,"After initial quality control, N4 bias correction (Tustison et al., 2010) in ANTS (version 2.1; http://stnava.github.io/ANTs/) was used to perform initial image inho-mogeneity correction.",1,0,0
10.1016/j.neuroimage.2021.118799,fmrib.ox.ac.uk/fsl,"FMRIB’s Linear Image Registration Tool (FLIRT) in FSL (version 5.0; http://www.fmrib.ox.ac.uk/fsl/) (Jenkinson et al., 2002; 2012) was used to rigidly align each T2w image to its correspond-ing T1w image, followed by being resampled to 0.5 mm isotropic reso-lution.",1,0,0
10.1016/j.neuroimage.2021.118799,nitrc.org/projects/cyno_4d_atlas,The generated longitudinal cynomolgus macaque atlases will be publicly available on the website of Yunnan key laboratory of primate biomedical research (www.lpbr.cn) and NITRC (https://www.nitrc.org/projects/cyno_4d_atlas/) website to greatly fa-cilitate early brain development studies.,0,0,1
10.1016/j.neuroimage.2021.118799,iseg2017.web.unc.edu,"LINKS is an in-house developed learning-based multi-source integration framework for brain tissue segmenta-tion, which has been validated on MICCAI Neonatal Brain Segmentation (NeoBrainS12, https://neobrains12.isi.uu.nl/) and 6-month infant brain MRI Segmentation (iSeg-2017, https://iseg2017.web.unc.edu/) Grand Challenges, achieving state-of-the-art performance.",1,0,0
10.1016/j.neuroimage.2022.119530,translationalneuromodeling.org/tapas,"The precision-weighted prediction error predictor (pwPE) was derived using the freely available HGF toolbox (Mathys, 2011 ; Stefanics et al., 2018 ; Weber et al., 2020), which can be downloaded from http://www.translationalneuromodeling.org/tapas.",1,0,0
10.1016/j.neuroimage.2022.119530,osf.io/bjna4,Data and code availability statement Data and code are available on the Open Science Framework acces-sible via https://osf.io/bjna4/.,0,1,0
10.1016/j.neuroimage.2022.119530,biosemi.com/faq/cms&drl.htm,"Instead of ground and reference, the BioSemi EEG sys-tem uses a CMS/DRL feedback loop with two additional electrodes (for more information see: http://www.biosemi.com/faq/cms&drl.htm).",0,0,1
10.1016/j.neuroimage.2022.118937,letswave.cn,"Software Matlab 2018b & Letswave7 (Letswave.cn) Band-pass ﬁltering Butterworth ﬁlter, 0.01–200 Hz, 4th order, 24 dB/octave, zero-phase Notch ﬁltering Butterworth ﬁlter, 49–51 Hz, 4th order, 24 dB/octave, zero-phase Channel interpolation Bad channels were identiﬁed manually and interpolated with the mean value of the three surrounding channels.",1,0,0
10.1016/j.neuroimage.2022.118937,osf.io/v59qu,"Data and code are avail-able online (https://osf.io/v59qu)., The simulation code is available online (https://osf.io/v59qu)., Data and code availability statement Data and code are available online (https://osf.io/v59qu).",1,0,0
10.1016/j.neuroimage.2021.118791,mne.tools/0.16/manual/preprocessing/ssp.html,"It included: (a) spatial gradient noise cancelation of third-order, (b) band-pass ﬁlter 0.3 – 256 Hz and band-stop ﬁlter (50 Hz), (c) downsampling to 600 Hz, (c) Signal Space Projection (SSP; https://neuroimage.usc.edu/brainstorm/Tutorials/ArtifactsSsp , https://mne.tools/0.16/manual/preprocessing/ssp.html) to remove cardiac and eye movement artifacts (Taulu and Simola, 2006 ; Tesche et al., 1995 ; Uusitalo and Ilmoniemi, 1997), (d) extraction of 5-second-long epochs, (e) DC correction (Pellegrino et al., 2016), (f) visual inspection of epochs and rejection of epochs contaminated by artifacts.",1,0,0
10.1016/j.neuroimage.2021.118791,surfer.nmr.mgh.harvard.edu,"MRI processing was performed with Freesurfer (http://surfer.nmr.mgh.harvard.edu/, ver-sion 6.00) (Dale et al., 1999) applying the recon-all routine.",1,0,0
10.1016/j.neuroimage.2021.118791,neuroimage.usc.edu/brainstorm/tutorials/artifactsssp,"It included: (a) spatial gradient noise cancelation of third-order, (b) band-pass ﬁlter 0.3 – 256 Hz and band-stop ﬁlter (50 Hz), (c) downsampling to 600 Hz, (c) Signal Space Projection (SSP; https://neuroimage.usc.edu/brainstorm/Tutorials/ArtifactsSsp , https://mne.tools/0.16/manual/preprocessing/ssp.html) to remove cardiac and eye movement artifacts (Taulu and Simola, 2006 ; Tesche et al., 1995 ; Uusitalo and Ilmoniemi, 1997), (d) extraction of 5-second-long epochs, (e) DC correction (Pellegrino et al., 2016), (f) visual inspection of epochs and rejection of epochs contaminated by artifacts.",1,0,0
10.1016/j.neuroimage.2022.119229,github.com/uthscsa-nal/shrinkage,A clean copy of these methods is freely available under a MIT licence in the following GitHub repository: https://github.com/UTHSCSA-NAL/shrinkage.,1,0,0
10.1016/j.neuroimage.2022.119229,humanconnectome.org/data/data-use-terms,"HCP’s original publication (Essen et al., 2013) indicates: To aid in the protection of participants’ privacy, the HCP has adopted a two-tiered data access strategy (http://www.humanconnectome.org/data/data-use-terms/).",0,0,1
10.1016/j.neuroimage.2022.119229,cran.r-project.org/web/packages/nlshrink/index.html,"The nlshrink library (version 1.0.1, https://cran.r-project.org/web/packages/nlshrink/index.html) was the only external covariance shrinkage method package used in this work.",1,0,0
10.1016/j.neuroimage.2022.119229,prepro-cessed-connectomes-project.org/abide/index.html,humanconnectome.org) and the ABIDE consortium (http://prepro-cessed-connectomes-project.org/abide/index.html).,0,0,1
10.1016/j.neuroimage.2022.119229,humanconnectome.org,"Ethics Statement The data used in this study are all public/shared data provided by the HCP consortium (https://www.humanconnectome.org) and the ABIDE consortium (http://preprocessed-connectomes-project.org/abide/index.html). 5 N., HCP’s original publication (Essen et al., 2013) indicates: To aid in the protection of participants’ privacy, the HCP has adopted a two-tiered data access strategy (http://www.humanconnectome.org/data/data-use-terms/)., humanconnectome.org) and the ABIDE consortium (http://prepro-cessed-connectomes-project.org/abide/index.html).",0,0,1
10.1016/j.neuroimage.2022.119229,humanconnectome.org,Ethics Statement The data used in this study are all public/shared data provided by the HCP consortium (https://www.humanconnectome.org) and the ABIDE consortium (http://preprocessed-connectomes-project.org/abide/index.html). 5 N.,0,1,0
10.1016/j.neuroimage.2022.119229,preprocessed-connectomes-project.org/abide/index.html,Ethics Statement The data used in this study are all public/shared data provided by the HCP consortium (https://www.humanconnectome.org) and the ABIDE consortium (http://preprocessed-connectomes-project.org/abide/index.html). 5 N.,0,1,0
10.1016/j.neuroimage.2022.119005,keras.io,The CNN was built with Keras (https://keras.io).,1,0,0
10.1016/j.neuroimage.2022.119005,nlp.stanford.edu/projects/glove,"Word embeddings Word-level representations were modelled with GloVe vectors (Pennington et al., 2014) trained on Italian words using the code available on https://nlp.stanford.edu/projects/glove/.",1,0,0
10.1016/j.neuroimage.2022.119005,atlas.brainnetome.org,"Regions were selected using the Brainnetome Atlas (Fan et al., 2016) available at https://atlas.brainnetome.org.",1,0,0
10.1016/j.neuroimage.2022.119005,ﬁl.ion.ucl.ac.uk/spm,"fMRI preprocessing The preprocessing steps were performed using the MATLAB (The Mathworks, Natick, USA) toolbox SPM12 (www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119005,download.wikimedia.org,"Training was performed on the Italian Wikipedia, extracted from a Wikipedia dump (http://download.wikimedia.org/) using WikipediaEx-tractor (http://medialab.di.unipi.it/wiki/Wikipedia_Extractor).",1,0,0
10.1016/j.neuroimage.2022.119005,osf.io/95ftn/?view_only,"2) are available on the Open Science Framework repository: https://osf.io/95ftn/?view_only = 9a1a085583544c3eac44d1c75870599c., 2) are available on the Open Science Framework repository: https://osf.io/95ftn/?view_only = 9a1a085583544c3eac44d1c75870599c.",0,0,1
10.1016/j.neuroimage.2022.119005,hlt.isti.cnr.it/wordembeddings,"Top 1 Top 2 Top 3 Top4 Top 5 Chance level 1.6 3.1 4.7 6.3 7.8 Embeddings nearest centroid classiﬁer 45.4 59.3 66.8 71.7 75.3 CNN Final Layer nearest centroid classiﬁer 69.0 80.4 85.5 88.6 90.6 MLP Final Layer nearest centroid classiﬁer 68.8 80.9 86.5 89.8 91.9 CNN output category 69.8 81.1 86.4 89.4 91.4 MLP output category 74.7 86.1 90.8 93.3 94.9 (Berardi et al., 2015 ; http://hlt.isti.cnr.it/wordembeddings/) and found good performance: hit rate of 0.41, compared to 0.26 reported by those authors.",0,0,1
10.1016/j.neuroimage.2022.119005,tfhub.dev/google/universal-sentence-encoder-multilingual-large/3,"To assess how our model derived from a CNN designed to cap-ture speciﬁc semantic categories compared with pre-trained state-of-the-art sentence encoders, we compared results using the Multilin-gual Universal Sentence Encoder Large V3 (MUSEL3; Google LLC, CA, USA Chidambaram et al., 2019 ;; https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3).",1,0,0
10.1016/j.neuroimage.2022.119005,medialab.di.unipi.it/wiki/wikipedia_extractor,"Training was performed on the Italian Wikipedia, extracted from a Wikipedia dump (http://download.wikimedia.org/) using WikipediaEx-tractor (http://medialab.di.unipi.it/wiki/Wikipedia_Extractor).",1,0,0
10.1016/j.neuroimage.2022.119005,ixa2.si.ehu.es/stswiki/index.php/stsbenchmark,"We used the semantic textual similar-ity benchmark (http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark Cer et al., 2017 ;), where pairs of sentences were rated according to the similarity of their meaning by humans.",1,0,0
10.1016/j.neuroimage.2022.119005,crr.ugent.be/subtlex-it,"Sentences with psycholinguistic con-founds, such as mean lexical frequency outside the 5 and 95 percentile-range, were excluded using the frequency dictionary from Crepaldi and colleagues (Crepaldi et al., 2015 ; http://crr.ugent.be/subtlex-it/).",0,1,0
10.1016/j.neuroimage.2022.119561,github.com/tosatot/quantifying-rhythmicity-in-perceptual-reports,"The code can be obtained here: https://github.com/tosatot/quantifying-rhythmicity-in-perceptual-reports., It is available at: https://github.com/tosatot/quantifying-rhythmicity-in-perceptual-reports.",1,0,0
10.1016/j.neuroimage.2021.118868,github.com/vistalab/vistasoft,Pre-processed functional data were then analyzed in the vistasoft software (https://github.com/vistalab/vistasoft).,1,0,0
10.1016/j.neuroimage.2022.119321,python.org,"Model implementation and simulation The models were implemented using the Python (Python Soft-ware Foundation, https://www.python.org) and C++ (Standard C++ Foundation, https://isocpp.org) programming languages, where we also made use of the SciPy (Virtanen et al., 2020) and NumPy (van der Walt et al., 2011) modules for Python.",1,0,0
10.1016/j.neuroimage.2022.119321,jugit.fz-juelich.de/inm7/public/speciﬁcity-modeling,"The code used for the simulation of the brain network dynamics, the anal-ysis and the visualization can be found here: https://jugit.fz-juelich.de/inm7/public/speciﬁcity-modeling.",1,0,0
10.1016/j.neuroimage.2022.119321,isocpp.org,"Model implementation and simulation The models were implemented using the Python (Python Soft-ware Foundation, https://www.python.org) and C++ (Standard C++ Foundation, https://isocpp.org) programming languages, where we also made use of the SciPy (Virtanen et al., 2020) and NumPy (van der Walt et al., 2011) modules for Python.",1,0,0
10.1016/j.neuroimage.2021.118774,preprocessed-connectomes-project.org/adhd200,Data and code availability All MRI data used in this study are publicly available at the International Neuroimaging Datasharing Initiative website (http://preprocessed-connectomes-project.org/adhd200/).,0,1,0
10.1016/j.neuroimage.2022.119625,stnava.github.io/ants,"Co-registration was further reﬁned us-ing the brainshift correction option (Schönecker et al., 2009) and images were normalized to MNI space using the Advanced Normalization Tool (http://stnava.github.io/ANTs/; Avants et al., 2008).",1,0,0
10.1016/j.neuroimage.2022.119625,sleeptrip.org,"Data preprocessing The Fieldtrip toolbox (Oostenveld et al., 2011 ; http://ﬁeldtriptoolbox.org), Sleeptrip toolbox (https://www.sleeptrip.org), MATLAB R2018a and 2021b (Mathworks Inc., Sherbom, MA) were used for data analysis.",1,0,0
10.1016/j.neuroimage.2022.119625,lead-dbs.org,"DBS lead localization DBS leads were localized in a common space (Montreal Neuro-logical Institute (MNI) 1522009b template (Fonov et al., 2011)) us-ing Lead-DBS software (https://www.lead-dbs.org) with default settings (Horn et al., 2019), in line with previous studies (Horn et al., 2017).",1,0,0
10.1016/j.neuroimage.2022.119547,opengl.org/resources/libraries/glut,"NeuroRacer paradigm The NeuroRacer cognitive control video game was developed using the OpenGL Utility Toolkit (GLUT; http://www.opengl.org/resources/libraries/glut/) to serve as a multitasking challenge that as-sesses visual discrimination (sign matching) while simultaneously per-forming visuomotor tracking (driving a car; see (Anguera et al., 2013) for details).",1,0,0
10.1016/j.neuroimage.2021.118825,crcns.org,"The input neural data to synthesize BOLD signal was taken from a simultaneous extracellular recording of neuronal populations in visual areas V1 and V2 (Semedo et al., 2019; Zandvakili and Kohn, 2015) a previously published and publically available dataset (http://crcns.org).",0,1,0
10.1016/j.neuroimage.2021.118825,portal.nersc.gov/project/crcns/download/v1v2-1,"After MRI conﬁrmed the site of the recording cham-ber, craniotomies over the PFC were performed in second surgery for 1 This dataset is available at https://portal.nersc.gov/project/crcns/download/v1v2-1 Accessed: 2020-06-20. 12 M.",0,1,0
10.1016/j.neuroimage.2021.118825,github.com/mansooreh-pakravan/multivariate-eﬀect-mve-and-second-order-multi-variate-eﬀect-smve-for-fmri-data,"Additionally, the core MATLAB functions of computing MVE and sMVE are publicly accessible on GitHub (https://github.com/Mansooreh-Pakravan/MultiVariate-Eﬀect-MVE-and-second-order-Multi-Variate-Eﬀect-sMVE-for-fMRI-data).",1,0,0
10.1016/j.neuroimage.2021.118825,sr-research.com/eyelink-1000-plus/mojtaba,"Credit authorship contribution statement Mansooreh Pakravan: Methodology, Formal analysis, Software, Writing – original draft, Visualization, Writing –r e v i e w & editing. 2 https://www.sr-research.com/eyelink-1000-plus/Mojtaba Abbaszadeh: Resources, Formal analysis, Writing –review & editing.",0,0,1
10.1016/j.neuroimage.2022.119230,antsbrainextraction.sh,"The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workﬂow (from ANTs), using OASIS30ANTs as target template.",1,0,0
10.1016/j.neuroimage.2022.119309,surfer.nmr.mgh.harvard.edu/fswiki/lgi,"To prepare MRI data for SM quantiﬁcation, the pial output from the recon-all pipeline (h.pial) was processed through an additional script, Local Gyriﬁcation Index (Schaer et al., 2008 ; https://surfer.nmr.mgh.harvard.edu/fswiki/LGI).",1,0,0
10.1016/j.neuroimage.2022.119309,cmadan.github.io/calcsulc,"Measurements were ob-tained via the validated MATLAB toolbox calcSulc (Madan, 2019), which bases calculations on the cortical reconstruction (?h.pial), parcellation (h.aparc.a2009.annot), and sulcal map (?h.sulc) outputs of the FreeSurfer recon-all pipeline along with the “?h.pial-outer-smoothed ”o u t p u t from the local gyriﬁcation analysis (Madan, 2019 ; https://cmadan.github.io/calcSulc/).",1,0,0
10.1016/j.neuroimage.2022.119309,fsl.fmrib.ox.ac.uk/fsl/fslwiki/palm,"This approach was implemented in the Python library Neuro-tools (https://github.com/sahahn/neurotools), using an exchangeabil-ity block approach analogous to that of PALM software (Winkler et al., 2014 ; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/PALM).",1,0,0
10.1016/j.neuroimage.2022.119309,humanconnectome.org/study/hcp-young-adult,"Sweet: Conceptualization, Review & Editing James MacKillop: Conceptualization, Methodology, Writing -Re-view & Editing, Supervision Data and code availability statement Delay reward discounting data and 3T MR imaging data were ob-tained from the WU-Minn HCP Consortium open access 1200 Sub-jects Data Release (see https://www.humanconnectome.org/study/hcp-young-adult) in accordance with the HCP Data Use Terms.",0,1,0
10.1016/j.neuroimage.2022.119309,surfer.nmr.mgh.harvard.edu/fswiki/recon-all,"The FreeSurfer step was the use of an adapted version of the FreeSurfer v5.3.0 main processing stream “recon-all ” which has been widely used in MRI studies for the quantiﬁcation of neuroanatom-ical structures (Fischl, 2012 ; https://surfer.nmr.mgh.harvard.edu/fswiki/recon-all).",1,0,0
10.1016/j.neuroimage.2022.119309,cmadan.github.io/calcfd,"FD measurements were obtained via the vali-dated MATLAB toolbox calcFD , which bases calculations on cortical reconstruction and parcellation outputs from the FreeSurfer recon-all pipeline, including “ribbon.mgz ”a n d “aparc.a2009s + aseg.mgz ”, and local gyriﬁcation index outputs (Madan and Kensinger, 2016 ; http://cmadan.github.io/calcFD/).",1,0,0
10.1016/j.neuroimage.2022.119309,github.com/sahahn/neurotools,"This approach was implemented in the Python library Neuro-tools (https://github.com/sahahn/neurotools), using an exchangeabil-ity block approach analogous to that of PALM software (Winkler et al., 2014 ; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/PALM).",1,0,0
10.1016/j.neuroimage.2022.119152,usafacts.org,"In the United States, the police account for approximately 6% of all full-time employees for state and local governments (USAfacts.org, 2021) and many of their routine responsibilities do not involve public interactions.",0,0,1
10.1016/j.neuroimage.2022.119152,connectedpapers.com,Such a taxonomy is diﬀer-ent than just identifying papers using tools such as Connected Papers (https://www.connectedpapers.com/).,1,0,0
10.1016/j.neuroimage.2022.119350,pavlovia.org,"The experiments were hosted on the Pavlovia.org platform (Peirce et al., 2019), and ran on the participant’s own computer (cf.",0,0,1
10.1016/j.neuroimage.2022.119350,osf.io/3ed8f,Data and analysis code for the current study are available on https://osf.io/3ed8f/.,1,0,0
10.1016/j.neuroimage.2022.119350,osf.io/a7knv/(grootswagers,"Methods We used a previously published stimulus set and corresponding EEG data, obtained from https://osf.io/a7knv/(Grootswagers et al., 2019a).",0,1,0
10.1016/j.neuroimage.2022.119640,ion.ucl.ac.uk/spm,ion.ucl.ac.uk/spm).,0,0,1
10.1016/j.neuroimage.2022.119640,web.stanford.edu/group/dlab/cgi-bin/graph/chart.php,"The light intensity used in this study was 30-50 mW/mm 2 , which is estimated to decrease to 13-25 mW/mm 2 af-ter penetrating 100 μm of tissue, 5-10 mW/mm 2 after penetrating 250 μm of tissue, and < 1 mW/mm 2 after penetrating 750 μm of tissue (https://web.stanford.edu/group/dlab/cgi-bin/graph/chart.php).",0,0,1
10.1016/j.neuroimage.2021.118834,neurofractal.github.io/analyse_opmeg,"Accompanying MATLAB code can be found at https://github.com/FIL-OPMEG/tutorials_interference , which relies upon the analyse_OPMEG tool-box(https://neurofractal.github.io/analyse_OPMEG/), custom motion capture processing code (https://github.com/FIL-OPMEG/optitrack) and the Fieldtrip toolbox version 20210606 (Oostenveld et al., 2011).",1,0,0
10.1016/j.neuroimage.2021.118834,github.com/fil-opmeg/tutorials_interference,"Accompanying MATLAB code can be found at https://github.com/FIL-OPMEG/tutorials_interference , which relies upon the analyse_OPMEG tool-box(https://neurofractal.github.io/analyse_OPMEG/), custom motion capture processing code (https://github.com/FIL-OPMEG/optitrack) and the Fieldtrip toolbox version 20210606 (Oostenveld et al., 2011)., Analysis code is openly available on GitHub: https://github.com/FIL-OPMEG/tutorials_interference.",1,0,0
10.1016/j.neuroimage.2021.118834,github.com/fil-opmeg/optitrack,"Accompanying MATLAB code can be found at https://github.com/FIL-OPMEG/tutorials_interference , which relies upon the analyse_OPMEG tool-box(https://neurofractal.github.io/analyse_OPMEG/), custom motion capture processing code (https://github.com/FIL-OPMEG/optitrack) and the Fieldtrip toolbox version 20210606 (Oostenveld et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119394,neurovault.org,Data and code availability statements Activation maps in the group-level analyses will be available on the website of Neurovault (https://neurovault.org/).,1,0,0
10.1016/j.neuroimage.2022.119586,github.com/rosedwayane/aieeg,"The code and pre-trained IC-U-Net model are available at https://github.com/roseDwayane/AIEEG., The IC-U-Net code is available at https://github.com/roseDwayane/AIEEG., All codes and parameters of the proposed model are available on a GitHub repos-itory at https://github.com/roseDwayane/AIEEG.",1,0,0
10.1016/j.neuroimage.2022.119421,bigbrain.loris.ca,"We obtained the version at a 100 μm isotropic resolution in MNI-ICBM152 space, from the BigBrain3D Volume Data Release 2015 (https://bigbrain.loris.ca).",0,1,0
10.1016/j.neuroimage.2022.119421,github.com/yawenwang1/nbm_review.git,"Thus, our map may be more appropriate for high resolution sub-millimetre studies, and is made publicly available along with the code (https://github.com/YawenWang1/nbM_review.git)., All analysis code used for segmenting nbM from BigBrain, from the ex vivo dataset, (pre-)processing MP2RAGE, registering to standard stereo-tactic space is available on Github (https://github.com/YawenWang1/nbM_review.git).",1,0,0
10.1016/j.neuroimage.2022.119421,layerfmri.com/2018/01/04/high-ﬁeld-mri-scanners,"With the increasing availability of the ultra-high ﬁeld scanners (https://layerfmri.com/2018/01/04/high-ﬁeld-mri-scanners/; U ğurbil 2014) and our own experience with 9.4T quantita-tive MRI, we are increasingly optimistic that regions and nuclei that were thus far challenging to image (‘terra incognita’) can be visualised and characterised in detail.",0,0,1
10.1016/j.neuroimage.2022.119489,github.com/washington-university/hcppipelines/wiki/installation-and-usage-instructions,Pre-processing and extraction of functional time series in fMRI resting state and task data The detailed pre-processing protocol of the HCP resting state and task data sets is explained in detail on the HCP website (https://github.com/Washington-University/HCPpipelines/wiki/Installation-and-Usage-Instructions).,0,0,1
10.1016/j.neuroimage.2022.119489,github.com/washington-university/hcppipelines/blob/3,More information about the ICA-FIX method can be accessed via: https://github.com/Washington-University/HCPpipelines/blob/3 K.,1,0,0
10.1016/j.neuroimage.2022.119489,github.com/katerinac/brain_dynamics,"The codes are pub-licly available at https://github.com/katerinaC/brain_dynamics., The codes are publicly available at https://github.com/katerinaC/brain_dynamics.",1,0,0
10.1016/j.neuroimage.2022.119489,humanconnectome.org,"The full data set containing all the details about the participants, the study protocol, and the preprocessing of the data for all tasks and rest sessions can be obtained at the HCP website (http://www.humanconnectome.org/)., Data and code availability statement The data for all tasks and rest sessions are publicly available at the HCP website (http://www.humanconnectome.org/)., Data and code availability statement The data for all tasks and rest sessions are publicly available at the HCP website (http://www.humanconnectome.org/).",0,1,0
10.1016/j.neuroimage.2022.119489,humanconnectome.org/software/connectome-workbench,Rendered with Connectome Workbench (available at https://www.humanconnectome.org/software/connectome-workbench).,0,0,1
10.1016/j.neuroimage.2022.119182,2.4.2.2,2.4.2.2.,0,0,1
10.1016/j.neuroimage.2022.119182,osf.io/72w38,"The raw eye-tracking and EEG data, stimuli repressors for TRF anal-ysis, data, and code necessary for generating the ﬁgures and computing statistics will be available at https://osf.io/72w38/., Data/code availability statement The raw eye-tracking and EEG data, stimuli repressors for TRF anal-ysis, data and code necessary for generating the ﬁgures and computing statistics will be available at https://osf.io/72w38/.",0,1,0
10.1016/j.neuroimage.2022.119182,2.4.2.1,Temporal response function analysis 2.4.2.1.,0,0,1
10.1016/j.neuroimage.2022.119182,2.4.2.3,2.4.2.3.,0,0,1
10.1016/j.neuroimage.2022.119182,2.4.2.4,2.4.2.4.,0,0,1
10.1016/j.neuroimage.2021.118826,sites.google.com/site/bctnet/comparison/nbs,"Network-based statistic To further localize connectivity in which brain region pair was signif-icant diﬀerence among three experimental conditions, a network-based statistic (NBS, https://sites.google.com/site/bctnet/comparison/nbs) approach was employed (Zalesky et al., 2012 ; Zalesky et al., 2010).",1,0,0
10.1016/j.neuroimage.2021.118826,nitrc.org/projects/basco,"Network construction The functional connectivity between brain regions was es-tablished via beta-series correlation, which allows investigat-ing interregional functional connectivity in event-related fMRI data using the BetA-Series COrrelation software (BASCO v2.0, https://www.nitrc.org/projects/basco/).",1,0,0
10.1016/j.neuroimage.2021.118826,ﬁl.ion.ucl.ac.uk/spm,"Image preprocessing was conducted using the Statistical Para-metric Mapping software (SPM8, Wellcome Department of Cogni-tive Neurology, UCL, London, UK, http://www.ﬁl.ion.ucl.ac.uk/spm) (Friston et al., 1994).",1,0,0
10.1016/j.neuroimage.2021.118826,atlas.brainnetome.org/bnatlas.html,"Ontology and nomenclature of brain regions can be found at http://atlas.brainnetome.org/bnatlas.html., The nomenclature of brain regions see in http://atlas.brainnetome.org/bnatlas.html.",0,0,1
10.1016/j.neuroimage.2021.118826,nitrc.org/projects/bnv,"(https://www.nitrc.org/projects/bnv/) was used to visualize the con-nector and local hubs (Xia et al., 2013).",1,0,0
10.1016/j.neuroimage.2021.118826,github.com/mb3152/brain_graphs,"All graph theory analyses were executed with Bertolero’s Python code (https://github.com/mb3152/brain_graphs) that used the iGraph library (Bertolero et al., 2018).",1,0,0
10.1016/j.neuroimage.2022.119077,neuroimage.usc.edu/brainstorm/introduction,The analysis was performed in MATLAB using Brainstorm toolbox https://neuroimage.usc.edu/brainstorm/Introduction and Brain Con-nectivity toolbox https://sites.google.com/site/bctnet/home.,1,0,0
10.1016/j.neuroimage.2022.119077,sites.google.com/site/bctnet/home,The analysis was performed in MATLAB using Brainstorm toolbox https://neuroimage.usc.edu/brainstorm/Introduction and Brain Con-nectivity toolbox https://sites.google.com/site/bctnet/home.,1,0,0
10.1016/j.neuroimage.2022.119077,neurobs.com,The stimuli were presented us-ing NBS Presentation software (https://www.neurobs.com/).,1,0,0
10.1016/j.neuroimage.2022.119077,neuroimage.usc.edu/brainstorm,Brainstorm is documented and freely available for download under GNU general public license (http://neuroimage.usc.edu/brainstorm).,1,0,0
10.1016/j.neuroimage.2022.119101,developingconnectome.org,"We use the voxel-wise resting state data from the De-veloping Human Connectome Project (dHCP; Hughes et al., 2017 ; https://www.developingconnectome.org) on 267 term-birth newborns and determine voxel-to-voxel functional connectivity., Data availability The neuroimaging data used in this study are available as part of the publicly available Developing Human Connec-tome Project (dHCP; https://www.developingconnectome.org).",0,1,0
10.1016/j.neuroimage.2022.119101,github.com/sayginlab/neonate_molloy,"by identifying which cluster best matches each voxel’s connectivity pattern; available here: https://github.com/SayginLab/neonate_molloy)., Resulting neonate parcellations and full genetic expression results can be found at https://github.com/SayginLab/neonate_molloy., MATLAB code to calculate individual parcellations will be publicly available at https://github.com/SayginLab/neonate_molloy.",0,0,1
10.1016/j.neuroimage.2022.119101,human.brain-map.org/static/download,The genetic data are from the publicly available Allen Human Brain Atlas (AHBA; http://human.brain-map.org/static/download) and BrainSpan Atlas of the Developing Human Brain (http://brainspan.org/static/download.html).,0,1,0
10.1016/j.neuroimage.2022.119101,brainspan.org/rnaseq/search/index.html,"First, to identify which genes are signiﬁcantly changing in levels of expression, diﬀerential expression searches were conducted on http://www.brainspan.org/rnaseq/search/index.html.",0,0,1
10.1016/j.neuroimage.2022.119101,surfer.nmr.mgh.harvard.edu/fswiki/corticalparcellation_yeo2011,The adult 7-network Yeo (liberal mask) solution in MNI space was downloaded from https://surfer.nmr.mgh.harvard.edu/fswiki/CorticalParcellation_Yeo2011.,0,0,1
10.1016/j.neuroimage.2022.119101,brainspan.org/static/download.html,The genetic data are from the publicly available Allen Human Brain Atlas (AHBA; http://human.brain-map.org/static/download) and BrainSpan Atlas of the Developing Human Brain (http://brainspan.org/static/download.html).,0,1,0
10.1016/j.neuroimage.2022.119101,osc.edu,Analyses were com-pleted using the Ohio Supercomputer Center (https://www.osc.edu).,1,0,0
10.1016/j.neuroimage.2021.118698,sccn.ucsd.edu/eeglab/download.php,"Data sharing agreement The analyses were conducted by using publicly available MATLAB toolboxes that can be downloaded at http://audition.ens.fr/adc/NoiseTools/, https://sccn.ucsd.edu/eeglab/download.php , https://github.com/mickcrosse/mTRF-Toolbox.",1,0,0
10.1016/j.neuroimage.2021.118698,github.com/mickcrosse/mtrf-toolbox,"Data sharing agreement The analyses were conducted by using publicly available MATLAB toolboxes that can be downloaded at http://audition.ens.fr/adc/NoiseTools/, https://sccn.ucsd.edu/eeglab/download.php , https://github.com/mickcrosse/mTRF-Toolbox.",1,0,0
10.1016/j.neuroimage.2021.118698,audition.ens.fr/adc/noisetools,"First the average trial EEG epochs (maxi-mum of 83) were normalised via function nt_normcol (Noisetools http://audition.ens.fr/adc/NoiseTools/)., Data sharing agreement The analyses were conducted by using publicly available MATLAB toolboxes that can be downloaded at http://audition.ens.fr/adc/NoiseTools/, https://sccn.ucsd.edu/eeglab/download.php , https://github.com/mickcrosse/mTRF-Toolbox.",1,0,0
10.1016/j.neuroimage.2021.118698,osf.io/q9s5k,"The custom code used for the mTRF, PSD and PAC analysis can be downloaded from https://osf.io/q9s5k/., The ﬁnal data included in the manuscript ﬁgures and statistics can also be downloaded from https://osf.io/q9s5k/.",0,1,0
10.1016/j.neuroimage.2022.119242,osf.io/g3ydf,"C., 2021, osf.io/g3ydf) along with all the codes used for analyses.",1,0,0
10.1016/j.neuroimage.2022.119591,fmrib.ox.ac.uk/fsl),"5.0.9; FMRIB’s Soft-ware Library, www.fmrib.ox.ac.uk/fsl), Advanced Normalization Tools (ANTs; v2.1.0) (Avants et al., 2011), and Matlab R2014b.",1,0,0
10.1016/j.neuroimage.2022.119591,github.com/brain-networks/edge-ts,Code and example data for generating and an-alyzing edge time series are available in Github (https://github.com/brain-networks/edge-ts).,0,1,0
10.1016/j.neuroimage.2022.119591,vimeo.com,Naturalistic stimuli All movies were obtained from Vimeo (https://vimeo.com).,0,0,1
10.1016/j.neuroimage.2022.119729,github.com/tommaullin/blmm,"The BLMM toolbox, as well as the code used for the simulations and timing comparisons described in Sections 2.2 and 2.3 , are available at: https://github.com/TomMaullin/BLMM.",1,0,0
10.1016/j.neuroimage.2022.119729,github.com/tommaullin/blm,"The BLMM toolbox, as well as the code used for the simulations and timing comparisons described in Sections 2.2 and 2.3 , are available at: https://github.com/TomMaullin/BLMM., The BLM toolbox, which is also referenced several times throughout this work, may be found at: https://github.com/TomMaullin/BLM.",1,0,0
10.1016/j.neuroimage.2022.119716,github.com/anders-s-olsen/psilocybin_dynamic_fc,"We have published the MATLAB (The MathWorks, inc.) and R code used to generate the results presented in this study at (https://github.com/anders-s-olsen/psilocybin_dynamic_FC).",1,0,0
10.1016/j.neuroimage.2022.119716,nitrc.org/projects/conn,"fMRI time-series were denoised using CONN (https://www.nitrc.org/projects/conn) (Whitﬁeld-Gabrieli and Nieto-Castanon, 2012) by voxel-wise nuisance regression of 1) three translation and three rotation parameters from realignment and their ﬁrst-order derivatives, and 2) anatomical component correction using the ﬁrst ﬁve principal components and their ﬁrst-order derivatives from white-matter and CSF time-series (Behzadi et al., 2007).",1,0,0
10.1016/j.neuroimage.2022.119716,ﬁl.ion.ucl.ac.uk/spm,The data were preprocessed in SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm).,1,0,0
10.1016/j.neuroimage.2022.119716,nitrc.org/projects/bnv,"We used BrainNet Viewer (Xia et al., 2013) (https://www.nitrc.org/projects/bnv/) to generate connectivity visualizations.",1,0,0
10.1016/j.neuroimage.2022.119716,nitrc.org/projects/artifact_detect,"Motion and signal variance artifacts were identiﬁed using Artifact detection Tool (ART, https://www.nitrc.org/projects/artifact_detect).",1,0,0
10.1016/j.neuroimage.2022.119654,humanconnectome.org,"The signal time course was originally from the WU-Minn HCP young healthy adults (ages 22–35) S1200 release data (https://www.humanconnectome.org/study/hcp-young-adult)., The healthy subjects’ data used in this study are openly available at https://www.humanconnectome.org/.",0,1,0
10.1016/j.neuroimage.2022.119654,humanconnectome.org/study/hcp-young-adult,The signal time course was originally from the WU-Minn HCP young healthy adults (ages 22–35) S1200 release data (https://www.humanconnectome.org/study/hcp-young-adult).,0,1,0
10.1016/j.neuroimage.2022.119654,afni.nimh.nih.gov,"After discarding the ﬁrst ﬁve volumes to allow for T1 equilibra-tion eﬀects, the data were corrected for diﬀerences in acquisition time between slices, realigned to the ﬁrst volume to account for movement artifacts, spatially aligned with anatomic T1-weighted images, and then normalized to the Montreal Neurological Institute (MNI) space via the uniﬁed segmentation approach (Ashburner and Friston, 2005) using the T1-weighted images and spatially smoothed with a Gaussian kernel with a full-width at half-maximum of 8 mm to obtain a smoothing radius com-parable to that of the SPECT CBF as conﬁrmed by 3dFWHMx of the AFNI software (https://afni.nimh.nih.gov/).",1,0,0
10.1016/j.neuroimage.2022.119654,github.com/amemiyas/rsfmri,Code and data availability statement Code is available on GitHub (https://github.com/amemiyas/rsfMRI).,1,0,0
10.1016/j.neuroimage.2022.119592,massive.org.au,"Acknowledgements We thank Naotsugu Tsuchiya and David Grayden for help-ful preliminary discussions about the work, along with MASSIVE (https://www.massive.org.au) for computational resources.",0,0,1
10.1016/j.neuroimage.2022.119592,github.com/yundumbledore/neuroprocimager,The inference-based neural processes imaging framework is implemented in a MATLAB package called NeuroProcImager and is available at https://github.com/yundumbledore/NeuroProcImager.,1,0,0
10.1016/j.neuroimage.2022.119592,consciouscloud.erc.monash.edu,Data and code availability The 22 healthy male resting-state MEG data and their corresponding MRI data are available at http://consciouscloud.erc.monash.edu.,0,1,0
10.1016/j.neuroimage.2022.119056,neuroimage.usc.edu/brainstorm/exportbids,"org/https://neuroimage.usc.edu/brainstorm/https://www.ﬁl.ion.ucl.ac.uk/spm/https://mne.tools Environment MATLAB MATLAB MATLAB MATLAB Python Recommended programming level Beginners to advanced Intermediate to advanced Beginners to advanced Beginners to advanced Intermediate to advanced Interface GUI and scripting Scripting GUI and scripting GUI and scripting Scripting Supported modalities EEG, eye tracking, MoBI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, (f)MRI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, MRI (f)MRI, PET, EEG, and MEG MEG, EEG, iEEG, fNIRS BIDS support bids-matlab-tools (Delorme et al., 2021) data2bids (github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m) BIDS tools (neuroimage.usc.edu/brainstorm/ExportBids) en.wikibooks.org/wiki/SPM/BIDS MNE-BIDS (Appelhoﬀet al., 2019) Online community and support ✔ ✔ ✔ ✔ ✔ Strengths -Advanced ICA integration -High ﬂexibility -No progamming skills required -Dynamic Causal Modeling (DCM) -High ﬂexibility -Mobile brain imaging (MoBI) -Frequency and time-frequency analysis -Advanced visualization & user interface -Advanced statistics (native GLM support) -Machine-learning (scikit-learn) -Advanced statistics inclduing GLM support -Non-parametric statistics -Automatic data organization -Bayesian source analysis framework -Source estimation -Numerous extensions -Sophisticated forward models -Interoperates with multiple MATLAB, Python packages -Support for fMRI, PET, VBM as well -Freesurfer integration References Seminal paper Delorme and Makeig (2004) , Delorme et al.",1,0,0
10.1016/j.neuroimage.2022.119056,mne.tools,"org/https://neuroimage.usc.edu/brainstorm/https://www.ﬁl.ion.ucl.ac.uk/spm/https://mne.tools Environment MATLAB MATLAB MATLAB MATLAB Python Recommended programming level Beginners to advanced Intermediate to advanced Beginners to advanced Beginners to advanced Intermediate to advanced Interface GUI and scripting Scripting GUI and scripting GUI and scripting Scripting Supported modalities EEG, eye tracking, MoBI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, (f)MRI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, MRI (f)MRI, PET, EEG, and MEG MEG, EEG, iEEG, fNIRS BIDS support bids-matlab-tools (Delorme et al., 2021) data2bids (github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m) BIDS tools (neuroimage.usc.edu/brainstorm/ExportBids) en.wikibooks.org/wiki/SPM/BIDS MNE-BIDS (Appelhoﬀet al., 2019) Online community and support ✔ ✔ ✔ ✔ ✔ Strengths -Advanced ICA integration -High ﬂexibility -No progamming skills required -Dynamic Causal Modeling (DCM) -High ﬂexibility -Mobile brain imaging (MoBI) -Frequency and time-frequency analysis -Advanced visualization & user interface -Advanced statistics (native GLM support) -Machine-learning (scikit-learn) -Advanced statistics inclduing GLM support -Non-parametric statistics -Automatic data organization -Bayesian source analysis framework -Source estimation -Numerous extensions -Sophisticated forward models -Interoperates with multiple MATLAB, Python packages -Support for fMRI, PET, VBM as well -Freesurfer integration References Seminal paper Delorme and Makeig (2004) , Delorme et al.",1,0,0
10.1016/j.neuroimage.2022.119056,en.wikibooks.org/wiki/spm/bids,"org/https://neuroimage.usc.edu/brainstorm/https://www.ﬁl.ion.ucl.ac.uk/spm/https://mne.tools Environment MATLAB MATLAB MATLAB MATLAB Python Recommended programming level Beginners to advanced Intermediate to advanced Beginners to advanced Beginners to advanced Intermediate to advanced Interface GUI and scripting Scripting GUI and scripting GUI and scripting Scripting Supported modalities EEG, eye tracking, MoBI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, (f)MRI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, MRI (f)MRI, PET, EEG, and MEG MEG, EEG, iEEG, fNIRS BIDS support bids-matlab-tools (Delorme et al., 2021) data2bids (github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m) BIDS tools (neuroimage.usc.edu/brainstorm/ExportBids) en.wikibooks.org/wiki/SPM/BIDS MNE-BIDS (Appelhoﬀet al., 2019) Online community and support ✔ ✔ ✔ ✔ ✔ Strengths -Advanced ICA integration -High ﬂexibility -No progamming skills required -Dynamic Causal Modeling (DCM) -High ﬂexibility -Mobile brain imaging (MoBI) -Frequency and time-frequency analysis -Advanced visualization & user interface -Advanced statistics (native GLM support) -Machine-learning (scikit-learn) -Advanced statistics inclduing GLM support -Non-parametric statistics -Automatic data organization -Bayesian source analysis framework -Source estimation -Numerous extensions -Sophisticated forward models -Interoperates with multiple MATLAB, Python packages -Support for fMRI, PET, VBM as well -Freesurfer integration References Seminal paper Delorme and Makeig (2004) , Delorme et al.",1,0,0
10.1016/j.neuroimage.2022.119056,neuroimage.usc.edu/brainstorm,"org/https://neuroimage.usc.edu/brainstorm/https://www.ﬁl.ion.ucl.ac.uk/spm/https://mne.tools Environment MATLAB MATLAB MATLAB MATLAB Python Recommended programming level Beginners to advanced Intermediate to advanced Beginners to advanced Beginners to advanced Intermediate to advanced Interface GUI and scripting Scripting GUI and scripting GUI and scripting Scripting Supported modalities EEG, eye tracking, MoBI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, (f)MRI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, MRI (f)MRI, PET, EEG, and MEG MEG, EEG, iEEG, fNIRS BIDS support bids-matlab-tools (Delorme et al., 2021) data2bids (github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m) BIDS tools (neuroimage.usc.edu/brainstorm/ExportBids) en.wikibooks.org/wiki/SPM/BIDS MNE-BIDS (Appelhoﬀet al., 2019) Online community and support ✔ ✔ ✔ ✔ ✔ Strengths -Advanced ICA integration -High ﬂexibility -No progamming skills required -Dynamic Causal Modeling (DCM) -High ﬂexibility -Mobile brain imaging (MoBI) -Frequency and time-frequency analysis -Advanced visualization & user interface -Advanced statistics (native GLM support) -Machine-learning (scikit-learn) -Advanced statistics inclduing GLM support -Non-parametric statistics -Automatic data organization -Bayesian source analysis framework -Source estimation -Numerous extensions -Sophisticated forward models -Interoperates with multiple MATLAB, Python packages -Support for fMRI, PET, VBM as well -Freesurfer integration References Seminal paper Delorme and Makeig (2004) , Delorme et al.",1,0,0
10.1016/j.neuroimage.2022.119056,ﬁl.ion.ucl.ac.uk/spm,"org/https://neuroimage.usc.edu/brainstorm/https://www.ﬁl.ion.ucl.ac.uk/spm/https://mne.tools Environment MATLAB MATLAB MATLAB MATLAB Python Recommended programming level Beginners to advanced Intermediate to advanced Beginners to advanced Beginners to advanced Intermediate to advanced Interface GUI and scripting Scripting GUI and scripting GUI and scripting Scripting Supported modalities EEG, eye tracking, MoBI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, (f)MRI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, MRI (f)MRI, PET, EEG, and MEG MEG, EEG, iEEG, fNIRS BIDS support bids-matlab-tools (Delorme et al., 2021) data2bids (github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m) BIDS tools (neuroimage.usc.edu/brainstorm/ExportBids) en.wikibooks.org/wiki/SPM/BIDS MNE-BIDS (Appelhoﬀet al., 2019) Online community and support ✔ ✔ ✔ ✔ ✔ Strengths -Advanced ICA integration -High ﬂexibility -No progamming skills required -Dynamic Causal Modeling (DCM) -High ﬂexibility -Mobile brain imaging (MoBI) -Frequency and time-frequency analysis -Advanced visualization & user interface -Advanced statistics (native GLM support) -Machine-learning (scikit-learn) -Advanced statistics inclduing GLM support -Non-parametric statistics -Automatic data organization -Bayesian source analysis framework -Source estimation -Numerous extensions -Sophisticated forward models -Interoperates with multiple MATLAB, Python packages -Support for fMRI, PET, VBM as well -Freesurfer integration References Seminal paper Delorme and Makeig (2004) , Delorme et al.",1,0,0
10.1016/j.neuroimage.2022.119056,eeglab.org,EEGLAB FieldTrip Brainstorm SPM MNE-python Toolbox link https://eeglab.org https://www.ﬁeldtriptoolbox.,0,0,1
10.1016/j.neuroimage.2022.119056,github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m,"org/https://neuroimage.usc.edu/brainstorm/https://www.ﬁl.ion.ucl.ac.uk/spm/https://mne.tools Environment MATLAB MATLAB MATLAB MATLAB Python Recommended programming level Beginners to advanced Intermediate to advanced Beginners to advanced Beginners to advanced Intermediate to advanced Interface GUI and scripting Scripting GUI and scripting GUI and scripting Scripting Supported modalities EEG, eye tracking, MoBI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, (f)MRI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, MRI (f)MRI, PET, EEG, and MEG MEG, EEG, iEEG, fNIRS BIDS support bids-matlab-tools (Delorme et al., 2021) data2bids (github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m) BIDS tools (neuroimage.usc.edu/brainstorm/ExportBids) en.wikibooks.org/wiki/SPM/BIDS MNE-BIDS (Appelhoﬀet al., 2019) Online community and support ✔ ✔ ✔ ✔ ✔ Strengths -Advanced ICA integration -High ﬂexibility -No progamming skills required -Dynamic Causal Modeling (DCM) -High ﬂexibility -Mobile brain imaging (MoBI) -Frequency and time-frequency analysis -Advanced visualization & user interface -Advanced statistics (native GLM support) -Machine-learning (scikit-learn) -Advanced statistics inclduing GLM support -Non-parametric statistics -Automatic data organization -Bayesian source analysis framework -Source estimation -Numerous extensions -Sophisticated forward models -Interoperates with multiple MATLAB, Python packages -Support for fMRI, PET, VBM as well -Freesurfer integration References Seminal paper Delorme and Makeig (2004) , Delorme et al.",1,0,0
10.1016/j.neuroimage.2022.119056,github.com,"org/https://neuroimage.usc.edu/brainstorm/https://www.ﬁl.ion.ucl.ac.uk/spm/https://mne.tools Environment MATLAB MATLAB MATLAB MATLAB Python Recommended programming level Beginners to advanced Intermediate to advanced Beginners to advanced Beginners to advanced Intermediate to advanced Interface GUI and scripting Scripting GUI and scripting GUI and scripting Scripting Supported modalities EEG, eye tracking, MoBI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, (f)MRI MEG, EEG, iEEG, fNIRS, Multiunit, motion capture, eye tracking, MRI (f)MRI, PET, EEG, and MEG MEG, EEG, iEEG, fNIRS BIDS support bids-matlab-tools (Delorme et al., 2021) data2bids (github.com/ﬁeldtrip/ﬁeldtrip/blob/release/data2bids.m) BIDS tools (neuroimage.usc.edu/brainstorm/ExportBids) en.wikibooks.org/wiki/SPM/BIDS MNE-BIDS (Appelhoﬀet al., 2019) Online community and support ✔ ✔ ✔ ✔ ✔ Strengths -Advanced ICA integration -High ﬂexibility -No progamming skills required -Dynamic Causal Modeling (DCM) -High ﬂexibility -Mobile brain imaging (MoBI) -Frequency and time-frequency analysis -Advanced visualization & user interface -Advanced statistics (native GLM support) -Machine-learning (scikit-learn) -Advanced statistics inclduing GLM support -Non-parametric statistics -Automatic data organization -Bayesian source analysis framework -Source estimation -Numerous extensions -Sophisticated forward models -Interoperates with multiple MATLAB, Python packages -Support for fMRI, PET, VBM as well -Freesurfer integration References Seminal paper Delorme and Makeig (2004) , Delorme et al.",1,0,0
10.1016/j.neuroimage.2022.119601,ﬁl.ion.ucl.ac.uk,"The analysis of MRI data was performed using Sta-tistical Parametric Mapping (SPM12, https://www.ﬁl.ion.ucl.ac.uk/) in Matlab (R2014a Mathworks, Sherborn, Massachusetts).",1,0,0
10.1016/j.neuroimage.2022.119609,github.com/pennlinc/cubids.git,"Once we have conda installed we create and activate a new environment using the following commands: conda create -n test-env python = 3.8 conda activate test-env To obtain CuBIDS locally, we can use pip to download our software from the Python Package Manager (Pypi) using the following command: pip install CuBIDS Alternatively, we can clone from the CuBIDS GitHub repository using the following command: git clone https://github.com/PennLINC/CuBIDS.git Now that we have a copy of the source code, we can install it by running cd CuBIDS pip install -e. 7 S.",1,0,0
10.1016/j.neuroimage.2022.119609,github.com/pennlinc/cubids/raw/main/cubids/testdata/bids_dataset.zip,"Next, we download BIDS_Dataset.zip (a ZipFile containing the example dataset) and unzip as follows: curl -sSLO https://github.com/PennLINC/CuBIDS/raw/main/cubids/testdata/BIDS_Dataset.zip unzip BIDS_Dataset.zip rm BIDS_Dataset.zip As a ﬁrst step, we use CuBIDS to identify the metadata ﬁelds present in the dataset.",0,1,0
10.1016/j.neuroimage.2022.119609,cubids.readthedocs.io/en/latest,"The source code for CuBIDS is publicly available at https://github.com/PennLINC/CuBIDS , the documentation for our software is available at https://cubids.readthedocs.io/en/latest/, and our package is available for download on the Python Package Manager (pypi) https://pypi.org/project/cubids/., The source code for CuBIDS is publicly available at https://github.com/PennLINC/CuBIDS , the documentation for our software is available at https://cubids.readthedocs.io/en/latest/, and our pack-age is available for download on the Python Package Manager (pypi) https://pypi.org/project/cubids/.",1,0,0
10.1016/j.neuroimage.2022.119609,pypi.org/project/cubids,"The source code for CuBIDS is publicly available at https://github.com/PennLINC/CuBIDS , the documentation for our software is available at https://cubids.readthedocs.io/en/latest/, and our package is available for download on the Python Package Manager (pypi) https://pypi.org/project/cubids/., The source code for CuBIDS is publicly available at https://github.com/PennLINC/CuBIDS , the documentation for our software is available at https://cubids.readthedocs.io/en/latest/, and our pack-age is available for download on the Python Package Manager (pypi) https://pypi.org/project/cubids/.",1,0,0
10.1016/j.neuroimage.2022.119609,github.com/pennlinc/cubids,"The source code for CuBIDS is publicly available at https://github.com/PennLINC/CuBIDS , the documentation for our software is available at https://cubids.readthedocs.io/en/latest/, and our package is available for download on the Python Package Manager (pypi) https://pypi.org/project/cubids/., Once we have conda installed we create and activate a new environment using the following commands: conda create -n test-env python = 3.8 conda activate test-env To obtain CuBIDS locally, we can use pip to download our software from the Python Package Manager (Pypi) using the following command: pip install CuBIDS Alternatively, we can clone from the CuBIDS GitHub repository using the following command: git clone https://github.com/PennLINC/CuBIDS.git Now that we have a copy of the source code, we can install it by running cd CuBIDS pip install -e. 7 S., Next, we download BIDS_Dataset.zip (a ZipFile containing the example dataset) and unzip as follows: curl -sSLO https://github.com/PennLINC/CuBIDS/raw/main/cubids/testdata/BIDS_Dataset.zip unzip BIDS_Dataset.zip rm BIDS_Dataset.zip As a ﬁrst step, we use CuBIDS to identify the metadata ﬁelds present in the dataset., The source code for CuBIDS is publicly available at https://github.com/PennLINC/CuBIDS , the documentation for our software is available at https://cubids.readthedocs.io/en/latest/, and our pack-age is available for download on the Python Package Manager (pypi) https://pypi.org/project/cubids/.",1,0,0
10.1016/j.neuroimage.2022.119138,nitrc.org,CSA-QBI was down-loaded from NITRC (nitrc.org) in 2020.,0,0,1
10.1016/j.neuroimage.2022.119169,itksnap.org,"Placement of ROIs is described in Supplementary Table 1 and these were drawn using the Paintbrush mode in ITK-SNAP (Yushkevich et al., 2006) (http://www.itksnap.org/).",1,0,0
10.1016/j.neuroimage.2022.119169,mig.cs.ucl.ac.uk/index.php?n,"Zhang et al., 2012) using the original NODDI MATLAB toolbox (http://mig.cs.ucl.ac.uk/index.php?n = Tutorial.NODDImatlab).",1,0,0
10.1016/j.neuroimage.2022.119169,git.ecdf.ed.ac.uk/jbrl/neonatal-gfactors,"The code for tract propagation and average calculation, as well as scripts for the data analysis in this paper are available here: https://git.ecdf.ed.ac.uk/jbrl/neonatal-gfactors.",1,0,0
10.1016/j.neuroimage.2022.119169,git.ecdf.ed.ac.uk/jbrl/ena,The seg-mented tracts in the ENA50 template space are available here: https://git.ecdf.ed.ac.uk/jbrl/ena.,0,0,1
10.1016/j.neuroimage.2022.119169,theirworld.org,"Funding This work was supported by Theirworld (www.theirworld.org) and was undertaken in the MRC Centre for Reproductive Health, which is funded by MRC Centre Grant (MRC G1002033).",0,0,1
10.1016/j.neuroimage.2022.119169,brainsimagebank.ac.uk,"Data and code availability Reasonable requests for original image and anonymised data will be considered through the BRAINS governance pro-cess (www.brainsimagebank.ac.uk) (Job et al., 2017).",0,1,0
10.1016/j.neuroimage.2022.119049,musescore.org,Training stimuli were piano melodies created from the sung utter-ances using MuseScore (https://musescore.org) but were ﬁrst converted to MIDI using Melodyne 5.,1,0,0
10.1016/j.neuroimage.2022.119333,ﬁl.ion.ucl.ac.uk/spm,"After preprocessing with the fMRIPrep pipeline, data were smoothed with a Gaussian kernel with 8 mm FWHM us-ing SPM 12 (https://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119333,mathworks.com,"Stimuli Presentation of visual stimuli, application and automatic adjust-ment of the thermal and visceral pain stimuli, as well as the record-ing of behavioral data were performed using Matlab version R2020b (www.mathworks.com) and the Psychotoolbox V3 (Brainard, 1997).",1,0,0
10.1016/j.neuroimage.2022.118926,surfer.nmr.mgh.harvard.edu,"The full segmentation and surface re-construction of structural MRI was performed using the Freesurfer suite (https://surfer.nmr.mgh.harvard.edu/), resulting in a high-deﬁnition cortical layer and the brain, skull, and scalp boundary surfaces.",1,0,0
10.1016/j.neuroimage.2022.119590,28.0.1.1,"Statistical analysis Analyses were performed using IBM SPSS Statistics for Mac, version 28.0.1.1 (IBM Corp., Armonk, NY, USA).",1,0,0
10.1016/j.neuroimage.2021.118810,r.a.de,"References Andreychenko, A., Klomp, D.W.J., Graaf, R.A.de, Luijten, P.R., Boer, V.O., 2013.",0,0,1
10.1016/j.neuroimage.2021.118810,ﬁl.ion.ucl.ac.uk/spm,"SPM12 (Wellcome Center for Human Neuroimaging, UCL, UK, http://www.ﬁl.ion.ucl.ac.uk/spm) was used to segment the T 1 -weighted anatomical images into gray matter (GM), white matter (WM), and cerebrospinal ﬂuid (CSF) images (Ashburner and Friston, 2005).",1,0,0
10.1016/j.neuroimage.2021.118810,vdisoftware.net,"A (1.5 FH) ×(2.5 RL) ×(3 AP) cm 3 spectroscopic voxel was placed in the dorsal anterior cingulate cortex (dACC) and shimmed using the auto-mated B 0 shimming capabilities of the in-house Visual Display Interface (VDI) libraries (The Weizmann Institute, Israel, www.vdisoftware.net) in MATLAB 2020b (The Mathworks, Natick MA).",1,0,0
10.1016/j.neuroimage.2022.119300,nitrc.org/projects/wfu_pickatlas,"All the toolsets used in the processing and analysis pipeline are publicly available (EEGLAB: https://sccn.ucsd.edu/eeglab/index.php , SPM8: https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/, WFU_PickAtlas toolbox https://www.nitrc.org/projects/wfu_pickatlas/, Human Motor Area Template and Basal Ganglia Human Area Template: http://lrnlab.org/).",1,0,0
10.1016/j.neuroimage.2022.119300,ﬁl.ion.ucl.ac.uk/spm/software/spm8,"All the toolsets used in the processing and analysis pipeline are publicly available (EEGLAB: https://sccn.ucsd.edu/eeglab/index.php , SPM8: https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/, WFU_PickAtlas toolbox https://www.nitrc.org/projects/wfu_pickatlas/, Human Motor Area Template and Basal Ganglia Human Area Template: http://lrnlab.org/).",1,0,0
10.1016/j.neuroimage.2022.119300,lrnlab.org,"All the toolsets used in the processing and analysis pipeline are publicly available (EEGLAB: https://sccn.ucsd.edu/eeglab/index.php , SPM8: https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/, WFU_PickAtlas toolbox https://www.nitrc.org/projects/wfu_pickatlas/, Human Motor Area Template and Basal Ganglia Human Area Template: http://lrnlab.org/).",1,0,0
10.1016/j.neuroimage.2022.119300,sccn.ucsd.edu/eeglab/index.php,"All the toolsets used in the processing and analysis pipeline are publicly available (EEGLAB: https://sccn.ucsd.edu/eeglab/index.php , SPM8: https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm8/, WFU_PickAtlas toolbox https://www.nitrc.org/projects/wfu_pickatlas/, Human Motor Area Template and Basal Ganglia Human Area Template: http://lrnlab.org/).",1,0,0
10.1016/j.neuroimage.2022.119745,salimetrics.com,All samples were assayed in duplicate at the University of Nebraska Lincoln Salivary Biosciences Laboratory using a commercially-available assay kit for salivary testosterone (Salimetrics; www.salimetrics.com).,1,0,0
10.1016/j.neuroimage.2022.119745,dnagenotek.com,"Speciﬁcally, children were asked to passively drool into an Oragene DISCOVER (OGR-500; www.dnagenotek.com) collection tube until liquid saliva (not bubble) exceeded the ﬁll line indicated on the tube.",0,0,1
10.1016/j.neuroimage.2022.119343,humanconnectome.org/storage/app/media/documentation/s1200/hcp_s1200_release_reference_manual.pdf,humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf).,0,0,1
10.1016/j.neuroimage.2022.119343,boot.ci,"In a second step, the 95% conﬁdence intervals of these estimates (normal approximation and percentile method) were obtained through the boot.ci function of the boot package (Canty and Ripley, 2020).",1,0,0
10.1016/j.neuroimage.2022.119343,dornsife.usc.edu/labs/rwilcox/software,"Possible sex dif-ferences in PCAM dispersion measures (variances and inter-quantile ranges, IQR) were assessed through the original version and a cus-tomized version of the comvar2 function included in the freely accessible Rallfun-v38 ﬁle (https://dornsife.usc.edu/labs/rwilcox/software/).",1,0,0
10.1016/j.neuroimage.2022.119343,neuro.uni-jena.de/cat,"neuro.uni-jena.de/cat/, version r1184) of the SPM12 (http://www.ﬁl.",1,0,0
10.1016/j.neuroimage.2022.119343,www0.cs.ucl.ac.uk/staﬀ/g.ridgway/vbm/get_totals.m,"After applying this procedure, which does not include any cor-rection for overall head size, voxels were mapped into 116 re-gions according to the Automated Anatomical Labeling atlas (Tzourio-Mazoyer et al., 2002) by calculating the total gray matter volume for each region of interest (VOI) and participant via a MATLAB script (https://www0.cs.ucl.ac.uk/staﬀ/g.ridgway/vbm/get_totals.m).",1,0,0
10.1016/j.neuroimage.2022.119343,3.3.1.2,"Attending to this fact, as well as to the obtained ICC scores (which were substantially higher when ob-tained from average ratings, see Section 3.3.1.2), the ten predictors that 13 C.",0,0,1
10.1016/j.neuroimage.2022.119343,2.4.3.1,"Initially, the included features were the volumetric z-scores of those brain areas identiﬁed as relevant predictors of the PCAM scores yielded by each ML algorithm in each dataset (see 2.4.3.1).",0,1,0
10.1016/j.neuroimage.2022.119343,ugr.es/∼bioest/software/cmd.php?seccion,These two agreement indexes were calculated using software speciﬁcally developed for this purpose and freely available at https://www.ugr.es/∼bioest/software/cmd.php?seccion = agreement.,1,0,0
10.1016/j.neuroimage.2022.119343,ion.ucl.ac.uk/spm/software/spm12,"ion.ucl.ac.uk/spm/software/spm12/, version 6906) software.",1,0,0
10.1016/j.neuroimage.2022.119343,3.3.1.3,"Moreover, the same results are also observed when using: (1) only the ﬁve areas more directly related to the PCAM scores provided by each algorithm as predictors (hence ruling out that the re-duced similarity between the brain proﬁles of members of the same sex category resulted from a “too large ” number of relevant predictors in some of the models); (2) the 10 predictors exhibiting the highest aver-age importance across all algorithms (which should allow more accurate and reliable inferences; see the last paragraph of Section 3.3.1.3 and also Breiman (2001 , DelGiudice (2021), Hancox-Li (2020); and (3) the 116 brain areas of the AAL atlas (thus assessing brain proﬁles’ similarities in a way that is totally independent from the predictors’ importance es-timated in the regression analyses).",0,1,0
10.1016/j.neuroimage.2021.118854,github.com/ferreirafabio80/gfa,The GFA models and experiments were implemented in Python 3.9.1 and are available here: https://github.com/ferreirafabio80/gfa.,1,0,0
10.1016/j.neuroimage.2021.118854,humanconnectome.org/study/hcp-young-adult/data-releases,subjects (only these had rs-fMRI data available) of the 1200-subject data release of the HCP (https://www.humanconnectome.org/study/hcp-young-adult/data-releases).,0,1,0
10.1016/j.neuroimage.2021.118854,humanconnectome.org/study/hcp-young-adult/document/extensively-processed-fmri-data-documentation,Data and code availability The data used in this study was downloaded from the Human Connectome Project website (https://www.humanconnectome.org/study/hcp-young-adult/document/extensively-processed-fmri-data-documentation).,0,1,0
10.1016/j.neuroimage.2022.119420,surfer.nmr.mgh.harvard.edu,T 1 -weighted images were reconstructed and segmented into scalp and brain tissues using the FreeSurfer recon-all pipeline (https://surfer.nmr.mgh.harvard.edu/).,1,0,0
10.1016/j.neuroimage.2022.119420,neuroimage.usc.edu/brainstorm,Source reconstruction was conducted using the minimum-norm method implemented in the Brain-storm toolbox (https://neuroimage.usc.edu/brainstorm/).,1,0,0
10.1016/j.neuroimage.2022.119420,homer-fnirs.org,"The raw fNIRS time series were preprocessed (bandpass ﬁltered be-tween 0.01–0.5 Hz, motion corrected, etc.) with the Homer2 toolbox (https://homer-fnirs.org).",1,0,0
10.1016/j.neuroimage.2022.119587,neurosynth.org,"Center coordinates and spatial extent was based on typical locations for OFA, FFA and STS reported in pre-vious fMRI studies using face localizers (FFA, OFA, STS: Fox et al., 2009 ; FFA: Berman et al. 2010 ; right OFA, right FFA, right STS: Pitcher et al., 2011b ; OFA, right STS: Rossion et al., 2012) and a search on the automated meta-analysis platform Neurosynth.org (STS, neu-rosynth.org/analyses/terms/psts/).",1,0,0
10.1016/j.neuroimage.2022.119587,condition.at,"The combination of two control conditions makes the activated network less dependent on the choice of the control condition.At a conceptual level, there are several options to implement this contrast.",0,0,1
10.1016/j.neuroimage.2022.119587,ﬁl.ion.ucl.ac.uk,"Preprocessing Pre-processing was conducted using SPM12 (Statistical Parametric Mapping, version v6015, Wellcome Trust Centre for Neuroimaging, Lon-don, UK; http://www.ﬁl.ion.ucl.ac.uk) and MATLAB R2009b (Math-Works, Natick, MA, USA) with an in-house pipeline that consisted of the following steps: realignment, coregistration, segmentation, normal-ization, and smoothing.",1,0,0
10.1016/j.neuroimage.2022.119587,osf.io/y6kmg,"The results are indeed nearly identical as can be seen in our supple-mentary data presented on OSF (“Supplement with alternative LI calculation methods ”(https://osf.io/y6kmg/). 2 In order to evaluate the robustness of our results, we also calculated the LI with the number of activated voxels., If no local maximum met els ”o n l y on OSF under “Supplement with alternative LI calculation methods ”(https://osf.io/y6kmg/). 4 I.",0,1,0
10.1016/j.neuroimage.2022.119587,neu-rosynth.org/analyses/terms/psts,"Center coordinates and spatial extent was based on typical locations for OFA, FFA and STS reported in pre-vious fMRI studies using face localizers (FFA, OFA, STS: Fox et al., 2009 ; FFA: Berman et al. 2010 ; right OFA, right FFA, right STS: Pitcher et al., 2011b ; OFA, right STS: Rossion et al., 2012) and a search on the automated meta-analysis platform Neurosynth.org (STS, neu-rosynth.org/analyses/terms/psts/).",1,0,0
10.1016/j.neuroimage.2022.119587,osf.io/s8gwd,"A graphical visualization of the paradigm can be found on this study’s Open Science Framework repository (OSF, https://osf.io/s8gwd/)., The coordinates are summarized on this study’s OSF repository (https://osf.io/s8gwd/)., Model descriptives, diagnos-tics, and estimates of eﬀect sizes (standardized beta coeﬃcients) are provided on OSF (https://osf.io/s8gwd/) along with the R-scripts and data to reproduce the analyses., On OSF (https://osf.io/s8gwd/) we additionally present the group activa-tion pattern for all subjects and the group activation pattern for left-handed subjects., This was also the case when we assessed models separately for right-and left-handers (see OSF, https://osf.io/s8gwd/)., Data availability Supplementary material as well as data and code used in this study are publicly available on the Open Science Framework at https://osf.io/s8gwd/.",1,0,0
10.1016/j.neuroimage.2022.119705,github.com/luisgo/tms_eﬁeld_solvers,"We also compared our method with a FDM under Setting 1 using an implementation available at https://github.com/luisgo/TMS_Eﬁeld_Solvers (Gomez et al., 2020).",1,0,0
10.1016/j.neuroimage.2021.118746,neurobs.com,The ex-periment was implemented in Presentation (Neurobehavioral Systems; https://www.neurobs.com/).,1,0,0
10.1016/j.neuroimage.2021.118746,ru.nl/neuroimaging/ﬁeldtrip,Electrophysiological data analysis was performed using custom-built MATLAB code (version R2019a; The MathWorks; RRID: SCR_001622) and the Fieldtrip toolbox (version 2018.08.01; www.ru.nl/neuroimaging/ﬁeldtrip).,1,0,0
10.1016/j.neuroimage.2022.118966,clinicaltrials.gov,The ME-MENTO cohort protocol was approved by the local ethics committee (“Comitéde Protection des Personnes Sud-Ouest et Outre Mer III ”; ap-proval number 2010-A01394-35) and registered in ClinicalTrials.gov (Identiﬁer: NCT01926249).,0,0,1
10.1016/j.neuroimage.2022.118966,kbioscience.co.uk,"Apolipopro-tein E genotypes were determined by KBiosciences (Hoddesdon, UK; www.kbioscience.co.uk), using ﬂuorescence-based competitive allele-speciﬁc polymerase chain reaction.",0,0,1
10.1016/j.neuroimage.2022.118966,cati-neuroimaging.com,"Brain morphometry Brain MRI acquisition from all centres were curated by the “Cen-ter d’Acquisition et de Traitement des Images ”(CATI), a national plat-form dedicated to neuroimaging (http://cati-neuroimaging.com) using a systematic qualiﬁcation procedure to ensure parameter uniformity and image quality (Operto et al., 2016).",0,0,1
10.1016/j.neuroimage.2022.119386,web.mit.edu/swg/software.htm,Functional connectivity toolbox (CONN) (http://web.mit.edu/swg/software.htm) was used for computing temporal correlation be-tween the deﬁned ROIs.,1,0,0
10.1016/j.neuroimage.2022.119386,biu.bangor.ac.uk/projects.php.en,"Nia Goulden, which can be accessed at http://biu.bangor.ac.uk/projects.php.en.",0,0,1
10.1016/j.neuroimage.2022.119386,jmrui.eu,"Quantiﬁcation was conducted using the Advanced Magnetic Resonance (AMARES) in the Java-based magnetic resonance user’s interface (jMRUI.1, EU project www.jmrui.eu) (Naressi et al., 2001).",1,0,0
10.1016/j.neuroimage.2022.119386,ﬁl.ion.ucl.ac.uk/spm,"fMRI data were analysed using Statis-tical Parametric Map (SPM8, http://www.ﬁl.ion.ucl.ac.uk/spm/).",1,0,0
10.1016/j.neuroimage.2022.119006,github.com/plus-microstate/manuscript_codes/tree/main/meg-cortical-microstates,Codes for running the analyses presented in this script are available at https://github.com/plus-microstate/manuscript_codes/tree/main/MEG-cortical-microstates.,1,0,0
10.1016/j.neuroimage.2022.119006,plus-microstate.github.io,"Code for performing mi-crostate analysis in sensor-or source-space MEG/EEG, simulations as described in this manuscript, statistics and visualisation have been com-piled in the +microstate toolbox (Tait and Zhang, 2021) (https://plus-microstate.github.io), a freely-available open-source toolbox written in MATLAB R2017b.",1,0,0
10.1016/j.neuroimage.2022.119707,osf.io/7kqvh,"Data and experimental code availability All the data generated in the experiment described in this manuscript and the experimental code are available on Open Science Framework at this link: https://osf.io/7kqvh/., Data and code availability statement All the data generated in the experiment described in this manuscript and the experimental code are available on Open Science Framework at this link: https://osf.io/7kqvh/.",0,1,0
10.1016/j.neuroimage.2022.119525,et.al,"Dubois et.al., Mota et.al., Dean et.al., Paydar et.al.",0,0,1
10.1016/j.neuroimage.2022.119525,mrtrix.org,"The HARDI data were preprocessed following denoising (Veraart et al., 2016), removing Gibb’s ring (Kellner et al., 2016), eddy correction (Andersson and Sotiropoulos, 2016), slice-to-volume correction (Andersson et al., 2017), and bias correction (Tustison et al., 2010) in MRtrix3 (https://www.mrtrix.org/).",1,0,0
10.1016/j.neuroimage.2022.119525,fsl.fmrib.ox.ac.uk,Then the T2 image was aligned to the b0 image of dMRI using linear registration followed by nonlinear registration in FSL (https://fsl.fmrib.ox.ac.uk/).,1,0,0
10.1016/j.neuroimage.2022.119525,r-project.org,Statistical analysis Statistical tests were performed using the R-Project 4.0.2 (https://www.r-project.org/).,1,0,0
10.1016/j.neuroimage.2022.118924,stnava.github.io/ants,"As the surface loop coil produced a spherical intensity gradient into the MB-SWIFT volumes, each volume with a mask covering the brain and nearby tissue was corrected by using Advanced Normalization Tools (ANTs, http://stnava.github.io/ANTs/) N4 bias ﬁeld correction (Tustison et al., 2010).",1,0,0
10.1016/j.neuroimage.2022.118924,cmrr.umn.edu/swift/index.php,"Brieﬂy, the MB-SWIFT raw data were reconstructed with SWIFT package 2018 (https://www.cmrr.umn.edu/swift/index.php).",1,0,0
10.1016/j.neuroimage.2022.118924,fsl.fmrib.ox.ac.uk/fsl/fslwiki),"MRI data were processed and analyzed with in-house made scripts, MATLAB (R2011a and R2018b; Mathworks Inc., Natick, MA, USA), Python (version 3.6.9; https://www.python.org/downloads/), FSL (ver-sion 5.0; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki), FreeSurfer (FreeView version 2.0; https://surfer.nmr.mgh.harvard.edu/), and Aedes (Ver-sion 1.0 rev 219; http://aedes.uef.ﬁ).",1,0,0
10.1016/j.neuroimage.2022.118924,fsl.fmrib.ox.ac.uk/fsl/fslwiki/melodic,"Additionally, individual-level independent component analysis (ICA; FSL MELODIC, https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MELODIC) was used to manually detect and regress out independent components that were clearly motion-related artefacts, such as those located only at the edges and having high-amplitude spikes in time series.",1,0,0
10.1016/j.neuroimage.2022.118924,python.org/downloads,"MRI data were processed and analyzed with in-house made scripts, MATLAB (R2011a and R2018b; Mathworks Inc., Natick, MA, USA), Python (version 3.6.9; https://www.python.org/downloads/), FSL (ver-sion 5.0; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki), FreeSurfer (FreeView version 2.0; https://surfer.nmr.mgh.harvard.edu/), and Aedes (Ver-sion 1.0 rev 219; http://aedes.uef.ﬁ).",1,0,0
10.1016/j.neuroimage.2022.118924,surfer.nmr.mgh.harvard.edu,"MRI data were processed and analyzed with in-house made scripts, MATLAB (R2011a and R2018b; Mathworks Inc., Natick, MA, USA), Python (version 3.6.9; https://www.python.org/downloads/), FSL (ver-sion 5.0; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki), FreeSurfer (FreeView version 2.0; https://surfer.nmr.mgh.harvard.edu/), and Aedes (Ver-sion 1.0 rev 219; http://aedes.uef.ﬁ).",1,0,0
10.1016/j.neuroimage.2022.118924,nitrc.org/projects/sigma_template,"The statistical maps were overlaid on SIGMA (Barrière et al., 2019) rat brain template (https://www.nitrc.org/projects/sigma_template).",0,1,0
10.1016/j.neuroimage.2022.119402,github.com/zhengningapm/retro_aqp1,The analysis code is openly available on GitHub (https://github.com/zhengningapm/retro_aqp1).,1,0,0
10.1016/j.neuroimage.2022.119402,stnava.github.io/ants,"To compare ADC values between groups, the DWI images without the diﬀu-sion gradient (b0 images) were utilized to make a brain template using ANTs (Advanced Normalization Tools, http://stnava.github.io/ANTs/). 3 N.",1,0,0
10.1016/j.neuroimage.2022.119402,nitrc.org/projects/mricron,The brains were manually extracted us-ing MRIcron software (https://www.nitrc.org/projects/mricron/).,1,0,0
10.1016/j.neuroimage.2022.119402,nitrc.org/projects/tmbta_2019,"To quantitatively measure the ADC diﬀerence of the brain regions in the two groups, one standard mouse brain template TMBTA (www.nitrc.org/projects/tmbta_2019), was transformed to our homemade template.",0,1,0
10.1016/j.neuroimage.2022.119402,antsregistrationsyn.sh,NeuroImage 258 (2022) 119402 The b0 images were normalized to the homemade template using the antsRegistrationSyN.sh command in ANTs.,1,0,0
10.1016/j.neuroimage.2022.119402,afni.nimh.nih.gov,The aligned ADC maps in the two groups were compared using the 3dttest ++ function in AFNI (https://afni.nimh.nih.gov/).,1,0,0
10.1016/j.neuroimage.2022.119346,eeglab.org,•EEGLAB (Delorme and Makeig (2004) ; https://eeglab.org).,1,0,0
10.1016/j.neuroimage.2022.119346,neuroimage.usc.edu/brainsto,(2011) ; http://neuroimage.usc.edu/brainsto rm).,0,0,1
10.1016/j.neuroimage.2022.119346,mathworks.com/help/stats/evalclusters.html#shared-criterion,"""Other choices of criterion can be selected using the name-value pair input criterion , which can take values of KrzanowskiLai (Murray et al., 2008), CrossValidationIndex (Pascual-Marqui et al., 1995), or the four available criteria in the MATLAB function evalclusters (https://www.mathworks.com/help/stats/evalclusters.html#shared-criterion).""",1,0,0
10.1016/j.neuroimage.2022.119346,box.org/workshop/madrid2019/tutorial_cleaning),"Single-subject resting-state EEG The single-subject resting-state EEG dataset used in Section 3.1 is the open-access data supplied with the Fieldtrip tutorial for cleaning and pre-processing resting-state data (https://ﬁeldtriptool box.org/workshop/madrid2019/tutorial_cleaning), and is described in Chennu et al.",0,1,0
10.1016/j.neuroimage.2022.119346,uzh.ch/keyinst/loret,•LORETA and sLORETA/eLORETA (http://www.uzh.ch/keyinst/loret a).,1,0,0
10.1016/j.neuroimage.2022.119346,plus-microstate.github.io,All codes for the +microstate toolbox are available for download at https://plus-microstate.github.io.,1,0,0
10.1016/j.neuroimage.2022.119346,github.com/ohba-analysis/hmm-mar,"These include Fas-tICA v2.5 (https://research.ics.aalto.ﬁ/ica/fastica/) to use ICA for clus-tering, HMM-MAR (https://github.com/OHBA-analysis/HMM-MAR) to use Hidden Markov Modelling for clustering, and freely available custom-written scripts for data visualization of summary statistics (https://github.com/lukewtait/matlab_data_visualization)., HMM uses the HMM-MAR toolbox (https://github.com/OHBA-analysis/HMM-MAR), by de-fault using the standard options from the example scripts in this toolbox which are based on the pipelines of Baker et al.",1,0,0
10.1016/j.neuroimage.2022.119346,repository.cam.ac.uk/handle/1810/252736,"(2016) at the University of Cambridge data reposi-tory (https://www.repository.cam.ac.uk/handle/1810/252736) under the CC-BY 2.0 licence, and was accessed by us through the Fieldtrip FTP server (accessible via the Fieldtrip tutorial webpage linked above).",0,0,1
10.1016/j.neuroimage.2022.119346,ﬁl.ion.ucl.ac.uk/spm,•SPM (https://www.ﬁl.ion.ucl.ac.uk/spm/).,1,0,0
10.1016/j.neuroimage.2022.119346,github.com/lukewtait/microstate_toolbox,Installing the toolbox +microstate can be freely downloaded from https://github.com/lukewtait/microstate_toolbox.,1,0,0
10.1016/j.neuroimage.2022.119346,osf.io/db9u4,Data used in sections 3.2 and 3.5 is available for down-load at the Open Science Framework (https://osf.io/db9u4/).,0,1,0
10.1016/j.neuroimage.2022.119346,github.com/plus-microstate/toolbox,"Data used in sections 3.1 and 3.4 was downloaded from the Fieldtrip FTP server (ftp://ftp.ﬁeldtriptoolbox.org/pub/ﬁeldtrip/), and prepro-cessed ﬁles are included in the +microstate GitHub repository (https://github.com/plus-microstate/toolbox) along with Matlab scripts used for preprocessing.",0,1,0
10.1016/j.neuroimage.2022.119346,github.com/lukewtait/matlab_data_visualization,"These include Fas-tICA v2.5 (https://research.ics.aalto.ﬁ/ica/fastica/) to use ICA for clus-tering, HMM-MAR (https://github.com/OHBA-analysis/HMM-MAR) to use Hidden Markov Modelling for clustering, and freely available custom-written scripts for data visualization of summary statistics (https://github.com/lukewtait/matlab_data_visualization).",1,0,0
10.1016/j.neuroimage.2022.119181,osf.io/72ebd,"Data availability Data (participant information and preprocessed EEG data) are freely accessible at the following link: https://osf.io/72ebd/., Data availability statement Data (participant information and preprocessed EEG data) are freely accessible at the following data repository https://osf.io/72ebd/.",0,1,0
10.1016/j.neuroimage.2022.119181,letswave.org,"EEG analyses were run on Letswave 6 (https://www.letswave.org/) implemented on Matlab 2017 (MathWorks, USA).",1,0,0
10.1016/j.neuroimage.2021.118770,doc.ic.ac.uk/∼ecr05/msm_hocr_v2,mgh.harvard.edu/fswiki/DownloadAndInstall5.3) for preprocessing and MSM v2 30–32 (https://www.doc.ic.ac.uk/∼ecr05/MSM_HOCR_v2/) and mapalign (https://github.com/satra/mapalign) for functional alignment.,1,0,0
10.1016/j.neuroimage.2021.118770,openmx.ssri.psu.edu,"Own code was developed in R 3.4 for setting up the twin model using R’s package OpenMx version 2.11.5 (Bürger, 2000 ; Neale et al., 2016 ; Hunter, 2018) (https://openmx.ssri.psu.edu/), as well as in Python 3.6 for calculation of connectivity matri-ces.",1,0,0
10.1016/j.neuroimage.2021.118770,github.com/spin-test,Matlab 2018 was used for generation of ﬁgures and the spin test (https://www.github.com/spin-test).,1,0,0
10.1016/j.neuroimage.2021.118770,github.com/cirmuw/functional-twin-analysis,Code and data generated during this study is publicly available on Github (https://github.com/cirmuw/functional-twin-analysis).,0,1,0
10.1016/j.neuroimage.2021.118770,mgh.harvard.edu/fswiki/downloadandinstall5.3,mgh.harvard.edu/fswiki/DownloadAndInstall5.3) for preprocessing and MSM v2 30–32 (https://www.doc.ic.ac.uk/∼ecr05/MSM_HOCR_v2/) and mapalign (https://github.com/satra/mapalign) for functional alignment.,1,0,0
10.1016/j.neuroimage.2021.118770,github.com/washington-university/workbench/releases/tag/v1.3.2,Data and code availability Third party code used in this study included Connectome Work-bench v1.3 26 (https://github.com/Washington-University/workbench/releases/tag/v1.3.2) and Freesurfer v5.3 25 (https://surfer.nmr.,1,0,0
10.1016/j.neuroimage.2021.118770,github.com/satra/mapalign,"A publically available implementation is used to obtain and align diﬀusion maps (mapalign: https://github.com/satra/mapalign)., mgh.harvard.edu/fswiki/DownloadAndInstall5.3) for preprocessing and MSM v2 30–32 (https://www.doc.ic.ac.uk/∼ecr05/MSM_HOCR_v2/) and mapalign (https://github.com/satra/mapalign) for functional alignment.",1,0,0
10.1016/j.neuroimage.2022.119740,osf.io/upnkd,All the code and Fig.s are freely available on OSF (https://osf.io/upnkd/).,1,0,0
10.1016/j.neuroimage.2022.119740,github.com/schorschinho/osprey,Data analysis was performed using the Osprey (https://github.com/schorschinho/osprey) software and MAT-9 T.,1,0,0
10.1016/j.neuroimage.2022.119740,nitrc.org/projects/macromolecularmrs,They will be available on the NITRC portal in the “Macromolecular MRS ”project repository (https://www.nitrc.org/projects/MacromolecularMRS/).,0,0,1
10.1016/j.neuroimage.2022.118893,oxcns.org,https://www.oxcns.org E-mail address: Edmund.Rolls@oxcns.org (E.T.,0,0,1
10.1016/j.neuroimage.2022.118893,ox.ac.uk/crystal/refer.cgi?id,"The details of the im-age acquisition are provided at the UK Biobank website in the form of a protocol (http://biobank.ctsu.ox.ac.uk/crystal/refer.cgi?id = 2367)., ox.ac.uk/crystal/refer.cgi?id = 1977) and elsewhere (Miller et al., 2016).",0,0,1
10.1016/j.neuroimage.2022.118893,biobank.ctsu.ox.ac.uk,"UK Biobank dataset The UK Biobank is a large-scale biomedical database and research re-source dedicated to improving the prevention, diagnosis, and treatment of various diseases (Miller et al., 2016) (https://biobank.ctsu.ox.ac.uk)., Data and code availability statement The data analyzed are available from the UK Biobank (https://biobank.ctsu.ox.ac.uk).",0,1,0
10.1016/j.neuroimage.2022.118893,biobank.ctsu.ox.ac.uk/crystal/refer.cgi?id,The details of the im-age acquisition are provided at the UK Biobank website in the form of a protocol (http://biobank.ctsu.ox.ac.uk/crystal/refer.cgi?id = 2367).,0,0,1
10.1016/j.neuroimage.2022.119502,surfer.nmr.mgh.harvard.edu,The locations of the electrodes were determined using Freesurfer (http://surfer.nmr.mgh.harvard.edu/).,1,0,0
10.1016/j.neuroimage.2022.119700,mrtrix.org,"Raw data were preprocessed using a pipeline in MRtrix3 (https://www.mrtrix.org/), allowing for denoising, bias removal, and between-volume motion correction (Tournier et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119700,github.com/ruikechen/fetal-brain-dmri-atlas,"The CHN dMRI atlas is deposited for public use (https://github.com/RuikeChen/Fetal-Brain-dMRI-Atlas)., Data availability statement The atlas data has been distributed on https://github.com/RuikeChen/Fetal-Brain-dMRI-Atlas., NeuroImage 264 (2022) 119700 Data availability The atlas data has been distributed on https://github.com/RuikeChen/Fetal-Brain-dMRI-Atlas.",0,1,0
10.1016/j.neuroimage.2022.119700,nitrc.org/projects/gretna,"Analysis of Covariance (ANCOVA) was performed between CHD subjects (n = 11, GA 25-30 weeks) and GA-matched normal subjects (n = 40, GA from 25 to 30 weeks) with GA as a covariate in MATLAB using the GRETNA 2.0 toolbox (https://www.nitrc.org/projects/gretna/).",1,0,0
10.1016/j.neuroimage.2022.119700,github.com/svrtk/svrtk,"Images were then corrected for inter-slice motion and reconstructed to 1.2 mm isotropic resolution using SVRTK (https://github.com/SVRTK/SVRTK) (Deprez et al., 2020).",1,0,0
10.1016/j.neuroimage.2022.119689,clinicaltrials.gov,The study was registered in the ClinicalTrials.gov database (Identiﬁer: NCT04330677) and the data analyses were prereg-istered (https://osf.io/hvknp/).,0,0,1
10.1016/j.neuroimage.2022.119689,ﬁl.ion.ucl.ac.uk/spm,fMRI data were preprocessed and analysed using stan-dard procedures in SPM12 software (Wellcome Trust Center for Neu-roimaging; http://www.ﬁl.ion.ucl.ac.uk/spm) implemented in MATLAB (MathWorks).,1,0,0
10.1016/j.neuroimage.2022.119689,nitrc.org/projects/marsbar,"Pa-rameter estimates of signiﬁcant contrasts were extracted using MarsBaR (https://www.nitrc.org/projects/marsbar , RRID: SCR_009605) and fur-ther analysed in SPSS 25 (IBM Corp., Armonk, NY).",1,0,0
10.1016/j.neuroimage.2022.119689,osf.io/hvknp,"The study was registered in the ClinicalTrials.gov database (Identiﬁer: NCT04330677) and the data analyses were prereg-istered (https://osf.io/hvknp/)., The data that support the ﬁndings of the present study are openly available in the repository of the Open Science Foundation at https://osf.io/hvknp/(doi: 10.17605/OSF.IO/HVKNP)., The code that supports the ﬁndings of the present study is openly available in the repository of the Open Science Foundation at https://osf.io/hvknp/(doi: 10.17605/OSF.IO/HVKNP).",0,1,0
10.1016/j.neuroimage.2022.119689,neurovault.org/collections/fbhlskjx,The unthresholded statistical maps of the fMRI results can be accessed at https://neurovault.org/collections/FBHLSKJX/.,0,0,1
10.1016/j.neuroimage.2022.119689,0.14.1.0,"Furthermore, fre-quentist inference was complemented by computing Bayes factors (BFs) with default priors via JASP (version 0.14.1.0).",1,0,0
10.1016/j.neuroimage.2022.119012,who.int/health-3,"Procedure Participants were ﬁrst asked to perform a one-minute auditory checkup (using the app “hear WHO ”, https://www.who.int/health-3 Y.",1,0,0
10.1016/j.neuroimage.2022.119012,github.com/yl321/vocoded-comprehension}speech-comprehension,"Data and code accessibility The script and result for inter-subject decoding are available at github (https://github.com/YL321/vocoded-comprehension}speech-comprehension)., Data and code accessibility The script and result for inter-subject decoding are available at github (https://github.com/YL321/vocoded-comprehension}speech-comprehension).",1,0,0
10.1016/j.neuroimage.2022.119001,24.0.0.0,"Finally, a 3 ×2 factorial rmANOVA (n = 44 complete longi-tudinal data sets) with the factors time (acute, subacute, and chronic) and lesion connectivity (high, low) as well as their interaction was per-formed using IBM SPSS Statistics (version 24.0.0.0).",1,0,0
10.1016/j.neuroimage.2022.119001,neurobs.com,"Stimulus presentation was performed with the software Presentation (Neurobehavioral Systems, Inc., USA, https://www.neurobs.com) in an event-related design.",1,0,0
10.1016/j.neuroimage.2022.119001,nitrc.org/projects/mricron,"2MAY2016, https://www.nitrc.org/projects/mricron) on the most appropriate avail-able scan (typically acute DWI, subacute FLAIR or chronic T1-weighted scan).",0,0,1
10.1016/j.neuroimage.2022.119001,fcon_1000.projects.nitrc.org/indi/enhanced,"This data was obtained from the Enhanced Nathan Kline Institute Rockland Sam-ple (http://fcon_1000.projects.nitrc.org/indi/enhanced/, Nooner et al., 2012).",0,1,0
10.1016/j.neuroimage.2022.118936,fmrib.ox.ac.uk/fsl,Data was preprocessed with AFNI (http://afni.nimh.nih.gov/afni) and FSL (http://www.fmrib.ox.ac.uk/fsl) with the scripts provided by 1000 Functional Connectomes Project (http://www.nitrc.org/projects/fcon_1000).,1,0,0
10.1016/j.neuroimage.2022.118936,afni.nimh.nih.gov/afni,Data was preprocessed with AFNI (http://afni.nimh.nih.gov/afni) and FSL (http://www.fmrib.ox.ac.uk/fsl) with the scripts provided by 1000 Functional Connectomes Project (http://www.nitrc.org/projects/fcon_1000).,1,0,0
10.1016/j.neuroimage.2022.118936,clinicaltrials.gov,This study is a component of a larger study directed at developing biopsychosocial and neurological markers associated with treatment failure in chronic back pain (clinicalTrials.gov: RCT #NCT02991625).,0,0,1
10.1016/j.neuroimage.2022.118936,nitrc.org/projects/fcon_1000,Data was preprocessed with AFNI (http://afni.nimh.nih.gov/afni) and FSL (http://www.fmrib.ox.ac.uk/fsl) with the scripts provided by 1000 Functional Connectomes Project (http://www.nitrc.org/projects/fcon_1000).,1,0,0
10.1016/j.neuroimage.2022.119653,tortoise.nibib.nih.gov,The TORTOISE soft-ware package used for registration and distortion correction of diﬀu-sion MRIs is freely available at https://tortoise.nibib.nih.gov/.,1,0,0
10.1016/j.neuroimage.2022.119653,fsl.fmrib.ox.ac.uk/fsl/fslwiki/fslinstallation,The FSL software package used for brain tissue segmentation is freely available at https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation.,1,0,0
10.1016/j.neuroimage.2022.119653,surfer.nmr.mgh.harvard.edu,The FreeSurfer software package used for reconstruction of white matter and pial corti-cal surfaces is freely available at https://surfer.nmr.mgh.harvard.edu/.,1,0,0
10.1016/j.neuroimage.2022.119013,trendscenter.org/software/gift,"ICA was performed using the GIFT v4.0c software package (https://trendscenter.org/software/gift/) (Iraji et al., 2021).",1,0,0
10.1016/j.neuroimage.2022.119013,neurosynth.org,"We deﬁned an additional node (Node Meta) using a term-based meta-analysis for the term “default mode ”i n Neu-rosynth (https://www.neurosynth.org/) (Yarkoni, Poldrack, Nichols, Van Essen, and Wager, 2011).",0,0,1
10.1016/j.neuroimage.2022.119013,brainmap.org,"Interestingly, spatial patterns obtained by applying independent component analysis (ICA) to spatial maps of thousands of diﬀerent activation conditions derived from the BrainMap meta-analytic tool (https://www.brainmap.org) closely re-semble large-scale networks (Smith et al., 2009), as do data-driven anal-yses of activation maps from task data (Calhoun and Allen, 2013), fur-ther supporting the functional relevance of functional networks.",1,0,0
10.1016/j.neuroimage.2022.119013,ﬁl.ion.ucl.ac.uk/spm,"Preprocessing The rsfMRI data were preprocessed primarily using the statistical parametric mapping (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) tool-box.",1,0,0
10.1016/j.neuroimage.2021.118839,neuroimage.usc.edu/brainstorm,"2011), an accredited software freely available for download online under the GNU general public license (http://neuroimage.usc.edu/brainstorm).",1,0,0
10.1016/j.neuroimage.2022.119344,camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/(though,NeuroImage 258 (2022) 119344 CAN website: https://camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/(though see README.txt ﬁle below for the subset in both datasets).,0,0,1
10.1016/j.neuroimage.2022.119344,bids-standard.github.io/bids-validator,The data passed the BIDS validator version 1.8.9 (https://bids-standard.github.io/bids-validator/).,1,0,0
10.1016/j.neuroimage.2022.119344,portal.dpuk.ukserp.ac.uk,Other data can be uploaded to the server using the ﬁle in/out request mechanism on https://portal.dpuk.ukserp.ac.uk.,0,1,0
10.1016/j.neuroimage.2022.119344,portal.dementiasplatform.uk/apply,The MEG and MRI data are formatted according to international BIDS standards and analysed freely on the DPUK plat-form (https://portal.dementiasplatform.uk/Apply).,0,1,0
10.1016/j.neuroimage.2022.119344,bids.neuroimaging.io,"The data are represented in the Brain Imaging Data Structure (BIDS) format (version 1.4.1; http://bids.neuroimaging.io), which is an in-ternational, community eﬀort (Gorgolewski et al., 2016 ; Niso et al., 2018).",0,1,0
10.1016/j.neuroimage.2022.119344,surfer.nmr.mgh.harvard.edu/fswiki/mri_deface,"Note that the faces on the MRI images were removed using the FreeSurfer (Fischl, 2012) function (https://surfer.nmr.mgh.harvard.edu/fswiki/mri_deface).",1,0,0
10.1016/j.neuroimage.2022.119344,github.com/delshadv/biofind-data-paper,"However, we subsequently converted them to the BIDS format using the script called Xnat2Bids.m, which can be found in the GitHub repository that accompanies this paper (https://github.com/delshadv/BioFIND-data-paper/)., The full code is available here on GitHub repository https://github.com/delshadv/BioFIND-data-paper/, speciﬁcally the MATLAB ﬁles: feature_extraction_test.m, pre-proc_beamform_ROI.m, repeated_CV.m and main.m.",1,0,0
10.1016/j.neuroimage.2022.119344,ukserp.ac.uk,"Other data can be uploaded to the server using the ﬁle in/out request mechanism on https://portal.dpuk.ukserp.ac.uk., ukserp.ac.uk in case one needs to have any other resources, such as other GitHub repositories, libraries, etc.",0,1,0
10.1016/j.neuroimage.2022.119344,portal.dpuk.uksep.ac.uk,The user then needs to follow simple instructions and log in using https://portal.dpuk.uksep.ac.uk/.,0,0,1
10.1016/j.neuroimage.2022.119344,imaging.mrc-cbu.cam.ac.uk/meg/maxﬁlter_v2.2,"MEG maxﬁltering In addition to the raw data, we also provide versions that have been de-noised using Signal Space Separation (SSS) (Taulu and Kajola, 2005) as implemented in MaxFilter version 2.2.12 (https://imaging.mrc-cbu.cam.ac.uk/meg/Maxﬁlter_V2.2)., All other MaxFilter pa-rameters were kept as their default, as described in the manual available on https://imaging.mrc-cbu.cam.ac.uk/meg/Maxﬁlter_V2.2 and output to the log ﬁles below.",1,0,0
10.1016/j.neuroimage.2022.119344,mne.tools/dev/generated/commands.html#mne-anonymize,"The precise date and time of recording were scrambled in the FIFF ﬁle using the “mne_anonymize ”function (https://mne.tools/dev/generated/commands.html#mne-anonymize) of the MNE software (Gramfort et al., 2013), to reduce the risk of participant identiﬁcation.",1,0,0
10.1016/j.neuroimage.2022.119344,cam-can.org,"The 91 controls from Cambridge were selected from the population-derived CamCAN cohort of healthy people from the same geographic region (www.cam-can.org), chosen to have similar age and sex distri-bution.",0,1,0
10.1016/j.neuroimage.2022.119344,dementiasplatform.uk/apply,"The MEG and MRI data are formatted according to international BIDS standards and analysed freely on the DPUK plat-form (https://portal.dementiasplatform.uk/Apply)., dementiasplatform.uk/Apply)., dementiasplatform.uk/Apply (the data are also summarised on the DPUK cohort website, https://doi.org/10.48532/007000).",0,1,0
10.1016/j.neuroimage.2022.119344,ohba-analysis.github.io/osl-docs,"The continuous data were then epoched into 2 s windows, and atypical epochs were automati-cally marked as “bad ”using the artefact detection function in the OSL toolbox (https://ohba-analysis.github.io/osl-docs/).",1,0,0
10.1016/j.neuroimage.2022.119344,github.com/delshadv/biofind-data-paper,"The MATLAB script used for maxﬁltering is also provided in the accompanying GitHub directory (“maxﬁlter_BIDS.m ”in https://github.com/delshadv/BioFIND-data-paper)., However, we subsequently converted them to the BIDS format using the script called Xnat2Bids.m, which can be found in the GitHub repository that accompanies this paper (https://github.com/delshadv/BioFIND-data-paper/)., The full code is available here on GitHub repository https://github.com/delshadv/BioFIND-data-paper/, speciﬁcally the MATLAB ﬁles: feature_extraction_test.m, pre-proc_beamform_ROI.m, repeated_CV.m and main.m., Code availability The custom-written code to implement all validation anal-yses is available on GitHub (https://github.com/delshadv/BioFIND-data-paper).",1,0,0
10.1016/j.neuroimage.2022.119344,xnat.org,DPUK required the ﬁles to be originally uploaded in XNAT format (https://www.xnat.org/).,0,0,1
10.1016/j.neuroimage.2022.119188,maastrichtuniversity.nl,"Schwartze), david.linden@ maastrichtuniversity.nl (D.E.J.",0,0,1
10.1016/j.neuroimage.2021.118845,neurovault.org,All unthresholded statistical maps will be made available in Neurovault (neurovault.org) upon publication.,0,0,1
10.1016/j.neuroimage.2021.118845,fmrib.ox.ac.uk/fsl,"Image processing and data analysis were implemented using FSL version 5.0.9 (FMRIB, Oxford, UK, http://www.fmrib.ox.ac.uk/fsl/).",1,0,0
10.1016/j.neuroimage.2022.119251,github.com/mia-ieeg/mia,"Freely available for download under the GNU General Public License version 3 and accessible from our public Git repository (https://github.com/MIA-iEEG/mia), MIA is also available as a plugin for Brainstorm (release September 2021; Tadel et al.",1,0,0
10.1016/j.neuroimage.2022.119251,openneuro.org,OpenNeuro at openneuro.org) opens up perspectives for processing large amounts of data and call for the possibility to test other types of classiﬁcations.,0,0,1
10.1016/j.neuroimage.2022.119571,brain-development.org/ixi-dataset/2,To summarize the performance of artefact 1 Download from https://brain-development.org/ixi-dataset/2 Download from https://www.oasis-brains.org/10 Y.-C.,0,0,1
10.1016/j.neuroimage.2022.119571,oasis-brains.org/10,To summarize the performance of artefact 1 Download from https://brain-development.org/ixi-dataset/2 Download from https://www.oasis-brains.org/10 Y.-C.,0,0,1
10.1016/j.neuroimage.2022.119178,neuro.uni-jena.de/cat12-html/cat_methods_qa.html,"The threshold used in this study (> 80 percentiles) is higher than the thresholds used in some previous studies (Gaser and Dahnke, 2016 ; Ma et al., 2022), and it corresponds to the quality rating of the ""no motion artifact"" example image shown in the CAT12 reference manual (http://www.neuro.uni-jena.de/cat12-html/cat_methods_QA.html).",0,0,1
10.1016/j.neuroimage.2022.119178,phi-group.top/resources.html,"The brain growth charts are shared with the public (http://phi-group.top/resources.html)., Data and code availability The brain growth charts are shared at: http://phi-group.top/resources.html.",0,1,0
10.1016/j.neuroimage.2022.119178,osf.io/fm7cq/?view_only,"Data availability We shared the brain templates for children from 1 to 6 years old (3D nifty ﬁles), the growth curve models of all brain regions, and the code to perform the analysis in a public open-science repository: (https://osf.io/fm7cq/?view_only = 9716e89f09e04b4bb2b4f0323ab2b 684)., We have shared the following data and code in a public open-science repository: (https://osf.io/fm7cq/?view_only = 9716e89f09e04b4bb2b4f0323ab2b684): Individual brain images of 285 participants (265 TDC and 20 DSLD, age span 1–6 years old) scanned on a 1.5T scanner, with age, sex, and diagnosis (n).",0,1,0
10.1016/j.neuroimage.2021.118795,biobank.ndph.ox.ac.uk/ukb/docs.cgi?id,NeuroImage 249 (2022) 118795 Data availability The FC endophenotypes are available here : https://biobank.ndph.ox.ac.uk/ukb/docs.cgi?id = 1 with project #64984.,0,0,1
10.1016/j.neuroimage.2021.118795,github.com/precimed/mostest,"Code availability This study used openly available software and codes, specif-ically GCTA (https://cnsgenomics.com/software/gcta/#GREML), PLINK (http://zzz.bwh.harvard.edu/plink/), MOSTest (https://github.com/precimed/mostest), and FUMA (https://fuma.ctglab.nl/).",1,0,0
10.1016/j.neuroimage.2021.118795,ukbiobank.ac.uk/ethics,"The summury statistics can be accessed via GWAS Cata-log : https://www.ebi.ac.uk/gwas/studies/GCP000274 Ethics statement UK Biobank dataset: informed consent is obtained from all UK Biobank participants; ethical procedures are controlled by a dedicated Ethics and Guidance Council (http://www.ukbiobank.ac.uk/ethics) that has developed with UK Biobank an Ethics and Governance Framework (given in full at http://www.ukbiobank.ac.uk/wp-content/uploads/2011/05/EGF20082.pdf), with IRB approval also obtained from the North West Multi-center Research Ethics Committee.",0,0,1
10.1016/j.neuroimage.2021.118795,ebi.ac.uk/gwas/studies/gcp000274,"The summury statistics can be accessed via GWAS Cata-log : https://www.ebi.ac.uk/gwas/studies/GCP000274 Ethics statement UK Biobank dataset: informed consent is obtained from all UK Biobank participants; ethical procedures are controlled by a dedicated Ethics and Guidance Council (http://www.ukbiobank.ac.uk/ethics) that has developed with UK Biobank an Ethics and Governance Framework (given in full at http://www.ukbiobank.ac.uk/wp-content/uploads/2011/05/EGF20082.pdf), with IRB approval also obtained from the North West Multi-center Research Ethics Committee.",0,0,1
10.1016/j.neuroimage.2021.118795,cnsgenomics.com/software/gcta/#greml,"Code availability This study used openly available software and codes, specif-ically GCTA (https://cnsgenomics.com/software/gcta/#GREML), PLINK (http://zzz.bwh.harvard.edu/plink/), MOSTest (https://github.com/precimed/mostest), and FUMA (https://fuma.ctglab.nl/).",1,0,0
10.1016/j.neuroimage.2021.118795,ukbiobank.ac.uk/wp-content/uploads/2011/05/egf20082.pdf),"The summury statistics can be accessed via GWAS Cata-log : https://www.ebi.ac.uk/gwas/studies/GCP000274 Ethics statement UK Biobank dataset: informed consent is obtained from all UK Biobank participants; ethical procedures are controlled by a dedicated Ethics and Guidance Council (http://www.ukbiobank.ac.uk/ethics) that has developed with UK Biobank an Ethics and Governance Framework (given in full at http://www.ukbiobank.ac.uk/wp-content/uploads/2011/05/EGF20082.pdf), with IRB approval also obtained from the North West Multi-center Research Ethics Committee.",0,0,1
10.1016/j.neuroimage.2021.118795,fuma.ctglab.nl,"Code availability This study used openly available software and codes, specif-ically GCTA (https://cnsgenomics.com/software/gcta/#GREML), PLINK (http://zzz.bwh.harvard.edu/plink/), MOSTest (https://github.com/precimed/mostest), and FUMA (https://fuma.ctglab.nl/).",1,0,0
10.1016/j.neuroimage.2021.118795,zzz.bwh.harvard.edu/plink,"Code availability This study used openly available software and codes, specif-ically GCTA (https://cnsgenomics.com/software/gcta/#GREML), PLINK (http://zzz.bwh.harvard.edu/plink/), MOSTest (https://github.com/precimed/mostest), and FUMA (https://fuma.ctglab.nl/).",1,0,0
10.1016/j.neuroimage.2021.118795,bcblab.com/bcb/atlas_of_human_brain_connections.html,The anatomical connectivity atlas used is available at (http://www.bcblab.com/BCB/Atlas_of_Human_Brain_Connections.html).,1,0,0
10.1016/j.neuroimage.2022.119264,osf.io/cx8a9,Data availability The source data are also publicly available at https://osf.io/cx8a9/.,0,1,0
10.1016/j.neuroimage.2022.119264,github.com/ewabeldzik/thetabold,Code availability All code generated for this study’s analyses are publicly available at https://github.com/ewabeldzik/thetaBOLD.,1,0,0
10.1016/j.neuroimage.2021.118833,data.mendeley.com/datasets/39gpcy7cgs/draft?a,"Data availability The data sets generated and analysed during the current study are freely available at https://data.mendeley.com/datasets/39gpcy7cgs/draft?a = 5fe6d0be-e46c-4d07-a968-c3ea2310725c., Code availability Custom MATLAB code for dMRI pre-and post-processing of data is freely available at https://data.mendeley.com/datasets/39gpcy7cgs/draft?a = 5fe6d0be-e46c-4d07-a968-c3ea2310725c.",1,0,0
10.1016/j.neuroimage.2022.119186,automeris.io/webplotdigitizer,"To compare the shape of the ‘neural DSF’ against the DSF measured in a range of psychophysical studies (Fig. 5 , Panel D: (Bradshaw et al., 2006 ; Bradshaw and Rogers, 1999 ; Didyk et al., 2011 ; Hess et al., 1999 ; Hogervorst et al., 2000 ; Kane et al., 2014 ; Lankheet and Lennie, 1996 ; Lee and Rogers, 1997 ; Peterzell et al., 2017 ; Pulliam, 1982 ; Rogers and Graham, 1982 ; Schumer and Ganz, 1979 ; Serrano-Pedraza and Read, 2010 ; Tyler, 1973 ; Tyler and Kontsevich, 2001)), we extracted reported data using WebPlotDigitizer (software freely available at https://automeris.io/WebPlotDigitizer/) and normalised by dividing each threshold against the lowest thresh-old in each dataset, forcing each DSF to bottom out at 1.",1,0,0
10.1016/j.neuroimage.2022.119440,qnlab.weill.cornell.edu/research/pre-processing-fmri-data,The code for Filter-Shift and LG-RBSN have been shared on our laboratory website (https://qnlab.weill.cornell.edu/research/pre-processing-fmri-data) as well as our laboratory github repository page (https://github.com/QuantitativeNeuroimagingLaboratory).,1,0,0
10.1016/j.neuroimage.2022.119440,fsl.fmrib.ox.ac.uk/fsl/fslwiki,FSL (V5.0.7) (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/) and in-house-developed pack-ages were used in the preprocessing of fMRI data.,1,0,0
10.1016/j.neuroimage.2022.119440,github.com/quantitativeneuroimaginglaboratory,The code for Filter-Shift and LG-RBSN have been shared on our laboratory website (https://qnlab.weill.cornell.edu/research/pre-processing-fmri-data) as well as our laboratory github repository page (https://github.com/QuantitativeNeuroimagingLaboratory).,1,0,0
10.1016/j.neuroimage.2022.119252,cyceron.fr/web/aal__anatomical_automatic_labeling.html,Peak voxel and clus-ter characteristics were generated using the aal toolbox within SPM8 (http://www.cyceron.fr/web/aal__anatomical_automatic_labeling.html).,1,0,0
10.1016/j.neuroimage.2022.119252,ﬁl.ion.ucl.ac.uk/spm,"Brieﬂy, T1 MRI images were realigned, segmented, normalized, modulated, and smoothed us-ing a customized template based on the study sample, as implemented in SPM8 (http://www.ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2022.119252,imageowl.com,"Image distortion associated with changes in software over time was controlled for by scanning a calibration phantom (ADNI MAGPHAM, The Phantom Laboratory) at the end of each MRI session and subsequently applying distortion correction to each image volume (Image Owl, Inc., Greenwich, NY; http://www.imageowl.com).",0,0,1
10.1016/j.neuroimage.2022.119734,data.donders.ru.nl/doc/help/helppages/user-manual/login-proﬁle.html?8,"For more informa-tion about the ORCID option and alternative ways to login, see: https://data.donders.ru.nl/doc/help/helppages/user-manual/login-proﬁle.html?8.",0,0,1
10.1016/j.neuroimage.2022.119734,neurobs.com,"All tasks were pro-grammed using the Presentation software (Version 20.2, Neurobehav-ioral Systems, Inc., Berkeley, CA, www.neurobs.com).",1,0,0
10.1016/j.neuroimage.2022.119734,data.donders.ru.nl,"Data and code availability statement The Dataset is stored as a Research Documentation Collection in the Donders Repository (https://data.donders.ru.nl/)., First, you need to create a user proﬁle in the Don-ders Repository by logging in with your SURFconext or ORCID account (https://data.donders.ru.nl/login)., For more informa-tion about the ORCID option and alternative ways to login, see: https://data.donders.ru.nl/doc/help/helppages/user-manual/login-proﬁle.html?8., Upon completion of these steps, users will be granted access to the collection and can view and download ﬁles through the Donders Repository website (for more information, see: https://data.donders.ru.nl/doc/help/user-manual/transfer-data.html). 13 L.",0,0,1
10.1016/j.neuroimage.2022.119734,data.donders.ru.nl/doc/help/user-manual/transfer-data.html,"Upon completion of these steps, users will be granted access to the collection and can view and download ﬁles through the Donders Repository website (for more information, see: https://data.donders.ru.nl/doc/help/user-manual/transfer-data.html). 13 L.",0,0,1
10.1016/j.neuroimage.2022.119734,p.o.box,"Doeller d , e , f , Mirjam Ernestus a , Judith Holler c , b , Branka Milivojevic c , Asli Özyürek a , b , c , Wim Pouw c , b , Iris van Rooij c , g , Herbert Schriefers c , Ivan Toni c , James Trujillo c , b , Sara Bögels c , h , ∗ a Centre for Language Studies, Radboud University, Nijmegen, the Netherlands b Max Planck Institute for Psycholinguistics, Nijmegen, the Netherlands c Donders Institute for Brain, Cognition, and Behaviour, Centre for Cognitive Neuroimaging, Radboud University, P.O.Box 9010, Nijmegen, Gelderland 6500, the Netherlands d Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany e Kavli Institute for Systems Neuroscience, Centre for Neural Computation, The Egil and Pauline Braathen and Fred Kavli Centre for Cortical Microcircuits, Jebsen Centre for Alzheimer’s Disease, Norwegian University of Science and Technology, Trondheim, Norway f Wilhelm Wundt Institute of Psychology, Leipzig University, Leipzig, Germany g Department of Linguistics, Cognitive Science, and Semiotics, and the Interacting Minds Centre at Aarhus University, Denmark h Department of Cognition and Communication, Tilburg University, the Netherlands a r t i c l e i n f o Keywords: Multimodal data Face-to-face interaction Referential communication Motion tracking fMRI Conceptual alignment a b s t r a c t We present a dataset of behavioural and fMRI observations acquired in the context of humans involved in mul-timodal referential communication., Yet, much work in linguistics and cognitive neuro-science has focused on individuals’ coding-decoding of signals according ∗ Corresponding author at: Donders Institute for Brain, Cognition, and Behaviour, Centre for Cognitive Neuroimaging, Radboud University, P.O.Box 9010, Nijmegen, Gelderland 6500, the Netherlands.",0,0,1
10.1016/j.neuroimage.2022.119734,data.donders.ru.nl/login,"First, you need to create a user proﬁle in the Don-ders Repository by logging in with your SURFconext or ORCID account (https://data.donders.ru.nl/login).",0,1,0
10.1016/j.neuroimage.2021.118801,db.humanconnectome.org/megatrawl,"HCP MegaTrawl (https://db.humanconnectome.org/megatrawl/) predicted HCP behav-ioral measures using functional connectivity (i.e., partial correlation) 7 S.",1,0,0
10.1016/j.neuroimage.2021.118801,humanconnectome.org/study/hcp-young-adult,Dataset We used rs-fMRI data of the HCP 1200-subjects release (https://www.humanconnectome.org/study/hcp-young-adult/).,0,1,0
10.1016/j.neuroimage.2022.119362,github.com/wang4412/csf_ssfp,"Codes and data availability MATLAB code for dictionary generation, Monte-Carlo simula-tions, and ﬂow quantiﬁcation with example data is available at https://github.com/wang4412/CSF_SSFP.",1,0,0
10.1016/j.neuroimage.2022.118968,github.com/raj-lab-ucsf,"Data availability All data used in this study will be made available upon reason-able request and relevant code will be uploaded to https://github.com/Raj-Lab-UCSF repository., Data availability All data used in this study will be made available upon reason-able request and relevant code will be uploaded to https://github.com/Raj-Lab-UCSF repository.",0,1,0
10.1016/j.neuroimage.2021.118780,nisox.org/software/snpm13,"Longer reaction times, which often serve as a behavioral signature of surprise (Huettel et al., 2002; Meyniel et al., 2016; Vassena et al., 2020), presumably reﬂect the cognitive processes involved in choosing a new action after landing in an unexpected state. 5 http://nisox.org/Software/SnPM13/. 6 https://nilearn.github.io/index.html.",1,0,0
10.1016/j.neuroimage.2021.118780,github.com/joramsoch/macs,"However, since at all other states, 4 https://github.com/JoramSoch/MACS 6 V.",0,0,1
10.1016/j.neuroimage.2021.118780,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"They then optimize the policy preferences 𝑝 (𝑠, 𝑎) of all the pre-ceding within-episode decisions directly with gradient ascent using the 3 https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/.",1,0,0
10.1016/j.neuroimage.2021.118780,nilearn.github.io/index.html,"Longer reaction times, which often serve as a behavioral signature of surprise (Huettel et al., 2002; Meyniel et al., 2016; Vassena et al., 2020), presumably reﬂect the cognitive processes involved in choosing a new action after landing in an unexpected state. 5 http://nisox.org/Software/SnPM13/. 6 https://nilearn.github.io/index.html.",1,0,0
10.1016/j.neuroimage.2022.119215,github.com/abcd-study/abcd_extract_eprime,"Data and code availability Behavioral analysis code provided by the ABCD Study Data Analysis, Informatics & Resource Center (DAIRC) are available at https://github.com/ABCD-STUDY/abcd_extract_eprime.",1,0,0
10.1016/j.neuroimage.2022.119215,github.com/dcan-labs/abcd-hcp-pipeline,"Raw images were processed using the ABCD-BIDS prepro-cessing pipeline (for details, see https://github.com/DCAN-Labs/abcd-hcp-pipeline ; https://osf.io/89pyd/; Sturgeon et al., 2021), which is based on the Human Connectome Project’s minimal preprocessing pipeline (Glasser et al., 2013).",1,0,0
10.1016/j.neuroimage.2022.119215,abcdstudy.org/principal-investigators.html,A listing of participating sites and a complete listing of the study investigators can be found at abcdstudy.org/principal-investigators.html.,0,0,1
10.1016/j.neuroimage.2022.119215,nda.nih.gov/abcd,ABCD Study data are made openly available to the research community at The National Institute of Mental Health Data Archive (https://nda.nih.gov/abcd) as they are collected.,0,1,0
10.1016/j.neuroimage.2022.119215,github.com/cbedetti/dcm2bids,"Data were ﬁrst converted to BIDS for-mat using dcm2bids (https://github.com/cbedetti/Dcm2Bids), which re-organizes nifti images produced with dcm2niix (Xiangrui Li et al., 2016).",1,0,0
10.1016/j.neuroimage.2022.119215,nda.nih.gov/study.html?id,DOIs can be found at nda.nih.gov/study.html?id = 721.,0,0,1
10.1016/j.neuroimage.2022.119215,abcd-study.org,"ABCD Acknowledgement Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development (ABCD) Study (abcd-study.org), held in the NIMH Data Archive (NDA).",0,1,0
10.1016/j.neuroimage.2022.119215,abcdstudy.org/scientists/protocols,"NeuroImage 255 (2022) 119215 during MRI data collection, participants completed structural, diﬀusion-weighted, resting-state, and three task-based imaging scans using a pro-tocol described in previous work (Casey et al 2018) and detailed at https://abcdstudy.org/scientists/protocols.",0,0,1
10.1016/j.neuroimage.2022.119215,osf.io/89pyd,"Raw images were processed using the ABCD-BIDS prepro-cessing pipeline (for details, see https://github.com/DCAN-Labs/abcd-hcp-pipeline ; https://osf.io/89pyd/; Sturgeon et al., 2021), which is based on the Human Connectome Project’s minimal preprocessing pipeline (Glasser et al., 2013).",1,0,0
10.1016/j.neuroimage.2022.119215,abcdstudy.org/nih-collaborators,A full list of supporters is available at abcdstudy.org/nih-collaborators.,0,0,1
10.1016/j.neuroimage.2022.119215,openneuro.org/datasets/ds004097,Data are available at https://openneuro.org/datasets/ds004097.,0,1,0
10.1016/j.neuroimage.2022.119215,github.com/monicadrosenberg/a-abcd,Be-havioral analysis code speciﬁc to this study are available at https://github.com/monicadrosenberg/a-ABCD.,1,0,0
10.1016/j.neuroimage.2022.119215,osf.io/psv5m,"Image preprocessing Neuroimaging preprocessing steps mirrored those used in the pub-licly available ABCD-BIDS Community Collection (ABCC; Feczko et al., 2021 ; https://osf.io/psv5m/).",0,1,0
10.1016/j.neuroimage.2022.119473,3.1.1.2,3.1.1.2.,0,0,1
10.1016/j.neuroimage.2022.119473,3.1.2.1,Dystonia 3.1.2.1.,0,0,1
10.1016/j.neuroimage.2022.119473,3.1.1.1,3.1.1.1.,0,0,1
10.1016/j.neuroimage.2022.119473,3.1.3.1,Essential tremor (ET) 3.1.3.1.,0,0,1
10.1016/j.neuroimage.2022.119473,3.1.1.3,3.1.1.3.,0,0,1
10.1016/j.neuroimage.2022.119473,brainmap.org/ale,"The ALE meta-analysis was conducted using GingerALE (v3.2; http://www.brainmap.org/ale/) (Eickhoﬀet al., 2009).",1,0,0
10.1016/j.neuroimage.2022.119473,3.1.1.4,3.1.1.4.,0,0,1
10.1016/j.neuroimage.2022.118879,freesurfer.net/3,"Note that the DSS transforms unaveraged trials, since it serves as a spatial ﬁlter (similar to PCA or ICA). 3 https://mne.tools/stable/index.html 4 http://audition.ens.fr/adc/NoiseTools/5 http://freesurfer.net/3 S.-G.",1,0,0
10.1016/j.neuroimage.2022.118879,osf.io/kzjcd/4,"For this purpose, trials were re-epoched for 6 https://osf.io/kzjcd/4 S.-G.",0,0,1
10.1016/j.neuroimage.2022.118879,osf.io/kzjcd,"For this purpose, trials were re-epoched for 6 https://osf.io/kzjcd/4 S.-G., All MATLAB and Python code used to analyze data and create visualiza-tion for the current study is available on the Open Science Framework (https://osf.io/kzjcd/).",0,0,1
10.1016/j.neuroimage.2022.118879,audition.ens.fr/adc/noisetools/5,"Note that the DSS transforms unaveraged trials, since it serves as a spatial ﬁlter (similar to PCA or ICA). 3 https://mne.tools/stable/index.html 4 http://audition.ens.fr/adc/NoiseTools/5 http://freesurfer.net/3 S.-G.",1,0,0
10.1016/j.neuroimage.2022.118879,ccrma.stanford.edu/∼jos/sasp/example_synthesis_1_f_noise.html,Example waveforms are shown in Fig. 1 A. 1 http://www.mathworks.com/products/matlab/2 https://ccrma.stanford.edu/∼jos/sasp/Example_Synthesis_1_F_Noise.html 2 S.-G.,0,0,1
10.1016/j.neuroimage.2022.118879,mne.tools/stable/index.html,"Note that the DSS transforms unaveraged trials, since it serves as a spatial ﬁlter (similar to PCA or ICA). 3 https://mne.tools/stable/index.html 4 http://audition.ens.fr/adc/NoiseTools/5 http://freesurfer.net/3 S.-G.",1,0,0
10.1016/j.neuroimage.2022.118879,mathworks.com/products/matlab/2,Example waveforms are shown in Fig. 1 A. 1 http://www.mathworks.com/products/matlab/2 https://ccrma.stanford.edu/∼jos/sasp/Example_Synthesis_1_F_Noise.html 2 S.-G.,0,0,1
10.1016/j.neuroimage.2022.119612,igraph.org/r,Graph features: Graph measurements were computed using the igraph R package (igraph.org/r).,1,0,0
10.1016/j.neuroimage.2022.119612,github.com/kwagstyl/surface_tools,Intracortical equivolumetric surfaces are generated using https://github.com/kwagstyl/surface_tools.,1,0,0
10.1016/j.neuroimage.2022.119612,bids-apps.neuroimaging.io/apps,"mi-capipe was tested on several datasets and is available at https://github.com/MICA-MNI/micapipe , documented at https://micapipe.readthedocs.io/, and containerized as a BIDS App http://bids-apps.neuroimaging.io/apps/., Micapipe is delivered as a docker container via BIDS-App  http://bids-apps.neuroimaging.io/apps/(Gorgolewski et al., 2017), and available on ReproNim  https://github.com/ReproNim/containers (Halchenko et al., 2021)., Micapipe is delivered as a docker container via BIDS-App  http://bids-apps.neuroimaging.io/apps/(Gorgolewski et al., 2017), and available on ReproNim  https://github.com/ReproNim/containers (Halchenko et al., 2021).",0,0,1
10.1016/j.neuroimage.2022.119612,micapipe.readthedocs.io,"In addition to its code-base being openly available on GitHub (http://github.com/MICA-MNI/micapipe), micapipe is also available as a Docker container, included as a BIDS App, and is accompanied by detailed tutorials and an expandable documentation (http://micapipe.readthedocs.io).",1,0,0
10.1016/j.neuroimage.2022.119612,micapipe.readthedocs.io,"mi-capipe was tested on several datasets and is available at https://github.com/MICA-MNI/micapipe , documented at https://micapipe.readthedocs.io/, and containerized as a BIDS App http://bids-apps.neuroimaging.io/apps/., Data and code availability statement An expandable documentation at https://micapipe.readthedocs.io describes installation, usage, pipeline steps, updates, extra features, and provides a series of ready-to-use tutorials., A documentation with detailed descriptions on the installation, implementation, as well as usage exam-ples and output ﬁles are available at https://micapipe.readthedocs.io/., Data and code availability An expandable documentation at https://micapipe.readthedocs.io describes installation, usage, pipeline steps, updates, extra features, and provides a series of ready-to-use tutorials.",0,0,1
10.1016/j.neuroimage.2022.119612,readthedocs.io,"mi-capipe was tested on several datasets and is available at https://github.com/MICA-MNI/micapipe , documented at https://micapipe.readthedocs.io/, and containerized as a BIDS App http://bids-apps.neuroimaging.io/apps/., In addition to its code-base being openly available on GitHub (http://github.com/MICA-MNI/micapipe), micapipe is also available as a Docker container, included as a BIDS App, and is accompanied by detailed tutorials and an expandable documentation (http://micapipe.readthedocs.io)., Data and code availability statement An expandable documentation at https://micapipe.readthedocs.io describes installation, usage, pipeline steps, updates, extra features, and provides a series of ready-to-use tutorials., A documentation with detailed descriptions on the installation, implementation, as well as usage exam-ples and output ﬁles are available at https://micapipe.readthedocs.io/., readthedocs.io , Vos de Wael et al., 2020), with the following options: normalized angle kernel, diﬀusion embedding with alpha = 0.5 and au-tomatic estimation of the diﬀusion time (See micapipe-supplementary for details)., Data and code availability An expandable documentation at https://micapipe.readthedocs.io describes installation, usage, pipeline steps, updates, extra features, and provides a series of ready-to-use tutorials.",0,0,1
10.1016/j.neuroimage.2022.119612,github.com/mica-mni/micapipe,"In addition to its code-base being openly available on GitHub (http://github.com/MICA-MNI/micapipe), micapipe is also available as a Docker container, included as a BIDS App, and is accompanied by detailed tutorials and an expandable documentation (http://micapipe.readthedocs.io).",1,0,0
10.1016/j.neuroimage.2022.119612,bids-apps.neuroimaging.io/apps/(gorgolewski,"Micapipe is delivered as a docker container via BIDS-App  http://bids-apps.neuroimaging.io/apps/(Gorgolewski et al., 2017), and available on ReproNim  https://github.com/ReproNim/containers (Halchenko et al., 2021)., Micapipe is delivered as a docker container via BIDS-App  http://bids-apps.neuroimaging.io/apps/(Gorgolewski et al., 2017), and available on ReproNim  https://github.com/ReproNim/containers (Halchenko et al., 2021).",0,0,1
10.1016/j.neuroimage.2022.119612,github.com/repronim/containers,"Micapipe is delivered as a docker container via BIDS-App  http://bids-apps.neuroimaging.io/apps/(Gorgolewski et al., 2017), and available on ReproNim  https://github.com/ReproNim/containers (Halchenko et al., 2021)., Micapipe is delivered as a docker container via BIDS-App  http://bids-apps.neuroimaging.io/apps/(Gorgolewski et al., 2017), and available on ReproNim  https://github.com/ReproNim/containers (Halchenko et al., 2021).",0,0,1
10.1016/j.neuroimage.2022.119612,humanconnectome.org,"For the HCP dataset, re-search procedures and ethical guidelines were followed in accordance with the Institutional Review Boards, with details on the HCP website (http://www.humanconnectome.org/).",0,0,1
10.1016/j.neuroimage.2022.119612,micapipe.readthedocs.io,"mi-capipe was tested on several datasets and is available at https://github.com/MICA-MNI/micapipe , documented at https://micapipe.readthedocs.io/, and containerized as a BIDS App http://bids-apps.neuroimaging.io/apps/., A documentation with detailed descriptions on the installation, implementation, as well as usage exam-ples and output ﬁles are available at https://micapipe.readthedocs.io/.",1,0,0
10.1016/j.neuroimage.2022.119612,github.com/mica-mni/micapipe,"mi-capipe was tested on several datasets and is available at https://github.com/MICA-MNI/micapipe , documented at https://micapipe.readthedocs.io/, and containerized as a BIDS App http://bids-apps.neuroimaging.io/apps/., All code can be found at https://github.com/MICA-MNI/micapipe , and is published under the Gen-eral Public License 3.0., Code for ﬁgures and tables can be found in the micapipe-supplementary GitHub repository (https://github.com/MICA-MNI/micapipe-supplementary)., All code can be found at https://github.com/MICA-MNI/micapipe , and is published under the General Public License 3.0., Code for ﬁgures and ta-bles can be found in the micapipe-supplementary GitHub repository (https://github.com/MICA-MNI/micapipe-supplementary).",1,0,0
10.1016/j.neuroimage.2022.119612,github.com/mica-mni/micapipe-supplementary,"Code for ﬁgures and tables can be found in the micapipe-supplementary GitHub repository (https://github.com/MICA-MNI/micapipe-supplementary)., Code for ﬁgures and ta-bles can be found in the micapipe-supplementary GitHub repository (https://github.com/MICA-MNI/micapipe-supplementary).",1,0,0
10.1016/j.neuroimage.2022.119288,github.com/apcspencer/dfc_dimreduction,The code used in this study is publicly available at https://github.com/apcspencer/dFC_DimReduction.,1,0,0
10.1016/j.neuroimage.2022.119288,trendscenter.org/software/simtb,"Synthetic data We produced synthetic data using SimTB (Erhardt et al., 2012) (https://trendscenter.org/software/simtb/) to simulate BOLD activity in a set of 𝑁nodes under a model of spatiotemporal separability, us-ing code modiﬁed from that originally used in Allen et al., The SimTB model (Erhardt et al., 2012) used to generate synthetic data was obtained from https://trendscenter.org/software/simtb/.",1,0,0
10.1016/j.neuroimage.2022.119288,humanconnectome.org,"Human data For application to real-world data, we obtained resting-state fMRI data from the HCP1200 release from the Human Connectome Project (Essen et al., 2013) (https://www.humanconnectome.org)., Data availability statement Real-world fMRI data were obtained from the Human Connectome Project (Essen et al., 2013) (https://www.humanconnectome.org).",0,1,0
10.1016/j.neuroimage.2022.119371,audacityteam.org,Stimuli and Tasks Auditory stimuli consisted of a 70-ms (10 ‒ms rise/fall time) sine wave tone with a frequency of either 1.0 kHz (standard tone(ST); 70%) or 1.5 kHz (deviant tone(DT); 30%) at 65 dB SPL and were created with the Audacity software® (version 2.3.3) (http://audacityteam.org/).,1,0,0
10.1016/j.neuroimage.2022.119371,freesurfer.net,"The cortical surface for the source model was constructed from the individual structural MRI with the Freesurfer software (RRID: SCR_001847, Martinos Center for Biomedical Imaging, http://freesurfer.net ; Dale et al., 1999 ; Fischl et al., 1999a ; Fischl et al., 1999b).",1,0,0
10.1016/j.neuroimage.2022.119371,dennisparren.com,Visual stimuli were created by Studio Dennis Parren (www.dennisparren.com) and were there for the sole purpose of engaging the participants.,0,0,1
10.1016/j.neuroimage.2022.119371,osf.io/rhb5z,De-rived data supporting the ﬁndings of this study are available here: https://osf.io/rhb5z/.,0,0,1
10.1016/j.neuroimage.2022.119210,adni.loni.usc.edu,"Data used in prepara-tion of this article were obtained from the Alzheimer’s Disease Neu-roimaging Initiative (ADNI) and the Australian Imaging Biomarkers and Lifestyle Study of Ageing (AIBL) databases (adni.loni.usc.edu), and the Pediatric Imaging, Neurocognition and Genetics (PING) study database (chd.ucsd.edu/research/ping-study.html, now shared through the NIMH Data Archive (NDA)).",0,1,0
10.1016/j.neuroimage.2022.119210,github.com/estenhl/pyment-public,"To promote transparency and reproducibility we have implemented an easy-to-use Keras interface for all the trained models, both the brain age and clinical predictors, and a pipeline for preprocessing images, avail-able on our GitHub http://www.github.com/estenhl/pyment-public., NeuroImage 256 (2022) 119210 Code availability All of the trained brain age models and a pipeline for preprocess-ing images is released in our GitHub repo at http://www.github.com/estenhl/pyment-public Funding sources This work was funded by the U iO:LifeScience Convergence Envi-ronment (project: 4MENT), The Research Council of Norway (302854, 223273, 249795, 298646, 300767), the South-Eastern Norway Regional Health Authority (2014097, 2016083, 2018037, 2018076, 2019101), Stiftelsen Kristian Gerhard Jebsen, ERA-Net Cofund through the ERA PerMed project ’IMPLEMENT’, and the European Research Council under the European Union’s Horizon 2020 research and Innovation program (ERC StG, Grant 802998) and the Wellcome Trust grant (215698/Z/19/Z).",0,0,1
10.1016/j.neuroimage.2022.119210,chd.ucsd.edu/research/ping-study.html,"Data used in prepara-tion of this article were obtained from the Alzheimer’s Disease Neu-roimaging Initiative (ADNI) and the Australian Imaging Biomarkers and Lifestyle Study of Ageing (AIBL) databases (adni.loni.usc.edu), and the Pediatric Imaging, Neurocognition and Genetics (PING) study database (chd.ucsd.edu/research/ping-study.html, now shared through the NIMH Data Archive (NDA)).",0,1,0
10.1016/j.neuroimage.2022.119619,github.com/florinneuro/stn_beta_burst_erf,"The customized Matlab code is available at https://github.com/FlorinNeuro/stn_beta_burst_erf., Customized Matlab code is available at https://github.com/FlorinNeuro/stn_beta_burst_erf.",1,0,0
10.1016/j.neuroimage.2022.119619,freesurfer.net,"To do so, the individual cortical surfaces (white matter –g r a y matter) were extracted from the individual T1-weighted MRI scans (3 T scanner and 1 mm 3 voxel size) using Freesurfer (http://freesurfer.net , v.5.3.0).",1,0,0
10.1016/j.neuroimage.2022.119619,neuroimage.usc.edu/brainstorm/introduction,"Signal processing All preprocessing and further analyses were done using Mat-lab (version R 2016b; MathWorks, Natick, USA) and the tool-box Brainstorm (http://neuroimage.usc.edu/brainstorm/Introduction ; Tadel et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119496,github.com/babadilab/nlgc,Data and code availability statement A python implementation of the NLGC algorithm is deposited on the open-source repository Github (https://github.com/BabadiLab/NLGC) to facilitate reproducibility and adoption by the neuroimag-ing community.,1,0,0
10.1016/j.neuroimage.2022.119496,drum.lib.umd.edu,The data used in this paper is archived and publicly available over the Digital Repository at the University of Maryland (https://drum.lib.umd.edu/).,0,1,0
10.1016/j.neuroimage.2022.119150,5.3.2.2,5.3.2.2.,0,0,1
10.1016/j.neuroimage.2022.119150,5.4.1.2,5.4.1.2.,0,0,1
10.1016/j.neuroimage.2022.119150,5.3.2.1,5.3.2.1.,0,0,1
10.1016/j.neuroimage.2022.119150,5.4.1.3,5.4.1.3.,0,0,1
10.1016/j.neuroimage.2022.119150,5.4.1.1,5.4.1.1.,0,0,1
10.1016/j.neuroimage.2021.118784,seonjoo.github.io/neural,"for statistical evaluations and visualizations can be found here: https://seonjoo.github.io/neural ﬂexibility_submission/., for statistical evaluations and visual-izations can be found here: https://seonjoo.github.io/neuralﬂexibility_submission/.",0,0,1
10.1016/j.neuroimage.2021.118784,seonjoo.github.io/neuralﬂexibility_submission,for statistical evaluations and visual-izations can be found here: https://seonjoo.github.io/neuralﬂexibility_submission/.,0,0,1
10.1016/j.neuroimage.2022.119404,fmri.wfubmc.edu,The ROI masks were deﬁned as the anatomi-cal masks created by the Wake Forest University (WFU) Pick Atlas SPM toolbox (http://fmri.wfubmc.edu) with the automated anatomical at-las (AAL).,1,0,0
10.1016/j.neuroimage.2022.119404,mturk.com,"Data collection was performed with the online survey platform SoSci Survey (https://www.soscisurvey.de), and participants got access to the survey through a participation invite published on Amazon Mechanical Turk (https://www.mturk.com/).",0,0,1
10.1016/j.neuroimage.2022.119404,goo.gl/kjvydz,The cluster extent threshold was determined by the SPM extension “cp_cluster_Pthresh.m ”(https://goo.gl/kjVydz).,1,0,0
10.1016/j.neuroimage.2022.119404,cran.r-project.org/web/packages/mass/index.html,The variable selection was per-formed with the stepAIC function of the MASS package (https://cran.r-project.org/web/packages/MASS/index.html) in R.,1,0,0
10.1016/j.neuroimage.2022.119404,bids.neuroimaging.io,"Raw data were arranged into BIDS format (http://bids.neuroimaging.io/; Gorgolewski et al., 2016).",0,1,0
10.1016/j.neuroimage.2022.119404,github.com/grousselet/bootcorci,The comparison of the cor-relation coeﬃcients was performed using a bootstrap approach with the R package bootcorci (https://github.com/GRousselet/bootcorci). 3 Y.,1,0,0
10.1016/j.neuroimage.2022.119404,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"fMRI data processing and mass-univariate functional segregation analyses Imaging data preprocessing was performed with a combina-tion of Nipype (Gorgolewski et al., 2011) and MATLAB (ver-sion R2018b 9.5.0; MathWorks) with Statistical Parametric Mapping (SPM12; https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/).",1,0,0
10.1016/j.neuroimage.2022.119404,cran.r-project.org/web/packages/car/index.html,"The multicollinearity for independent variables was diag-nosed using the variance inﬂation factor (VIF) that measures the correla-tion among independent variables, in the R package car (https://cran.r-project.org/web/packages/car/index.html).",1,0,0
10.1016/j.neuroimage.2022.119404,github.com/yili-zhao/genuine-pretended-disgust-task.git,"Codes used in this study (mainly DCM-relevant analyses), processed datasets, and documents indicating necessary identiﬁers of participants and pa-rameters for each ﬁle are accessible at https://github.com/Yili-Zhao/Genuine-pretended-disgust-task.git.",1,0,0
10.1016/j.neuroimage.2022.119404,soscisurvey.de,"Data collection was performed with the online survey platform SoSci Survey (https://www.soscisurvey.de), and participants got access to the survey through a participation invite published on Amazon Mechanical Turk (https://www.mturk.com/).",0,0,1
10.1016/j.neuroimage.2022.119144,globalbrainconsortium.org,Acknowledgments This project has been promoted by the Global Brain Consor-tium (https://globalbrainconsortium.org/; http://ccc-axis.org/) and the Ludmer Foundation.,0,0,1
10.1016/j.neuroimage.2022.119144,braincanada.ca/funded_grants/an-eeg-platform-for-national-and-international-eeg-based-neuroscience-eegnet,It also carried out the auspices of HBHL (https://www.mcgill.ca/hbhl/) and EEGnet (https://braincanada.ca/funded_grants/an-eeg-platform-for-national-and-international-eeg-based-neuroscience-eegnet/) projects.,0,0,1
10.1016/j.neuroimage.2022.119144,mcgill.ca/hbhl,It also carried out the auspices of HBHL (https://www.mcgill.ca/hbhl/) and EEGnet (https://braincanada.ca/funded_grants/an-eeg-platform-for-national-and-international-eeg-based-neuroscience-eegnet/) projects.,0,0,1
10.1016/j.neuroimage.2022.119144,ifcn.info,"Both studies used stan-dard protocols developed jointly by the Brain Research Laboratories, NYU, and the Cuban Neuroscience Center (Hernandez-Gonzalez et al., 2011), based on the IFCN guidelines for resting-state EEG analysis (Inter-national Federation of Clinical Neurophysiology IFCN) (www.ifcn.info).",0,0,1
10.1016/j.neuroimage.2022.119144,biorxiv.org/cgi/content/short/2022.01.12,Overview on code and data in https://biorxiv.org/cgi/content/short/2022.01.12.,0,0,1
10.1016/j.neuroimage.2022.119144,math.mcgill.ca/keith/surfstat,"Statistical analysis To compare the z spectra of the two groups (PEM and CON) at the two study times (Childhood and Adulthood), we used a mass univariate linear mixed model (LMM) model as implemented in the SurfStat pack-age (Worsley et al., 2009) (https://math.mcgill.ca/keith/surfstat/).",1,0,0
10.1016/j.neuroimage.2022.119144,ccc-axis.org,Acknowledgments This project has been promoted by the Global Brain Consor-tium (https://globalbrainconsortium.org/; http://ccc-axis.org/) and the Ludmer Foundation.,0,0,1
10.1016/j.neuroimage.2022.119764,brainmap.org/ale/manual.pdf,"The ALE meta ‐analysis followed four main steps: computation of ALE scores, establishing a null distribution for statistical testing, thresholding, and cluster statistics, as described in detail in the GingerALE Manual (http://brainmap.org/ale/manual.pdf).",0,0,1
10.1016/j.neuroimage.2022.119764,brainmap.org/ale,"Activation likelihood estimation (ALE) We used the GingerALE software package (version 3.0.2, http://brainmap.org/ale/) to perform the ALE meta-analyses on coordinates in MNI space (Eickhoﬀ et al., 2012 ; Eickhoﬀ et al., 2009 ; Turkeltaub et al., 2012)., The ALE meta ‐analysis followed four main steps: computation of ALE scores, establishing a null distribution for statistical testing, thresholding, and cluster statistics, as described in detail in the GingerALE Manual (http://brainmap.org/ale/manual.pdf).",1,0,0
10.1016/j.neuroimage.2022.119764,psycnet.apa.org,We also searched on Google Scholar and PsycNet (https://psycnet.apa.org/) us-ing the same key words but found no new studies.,0,0,1
10.1016/j.neuroimage.2022.119341,github.com/felipemoser/kelluwen,The BEAN implementation and related code can be found under www.github.com/felipemoser/kelluwen.,1,0,0
10.1016/j.neuroimage.2022.119341,crl.med.harvard.edu/research/fetal_brain_atlas,(2017) and is available at http://crl.med.harvard.edu/research/fetal_brain_atlas/.,0,0,1
10.1016/j.neuroimage.2022.119100,db.humanconnectome.org,More information on these clips can be found in Finn and Bandettini (2021) and at https://db.humanconnectome.org.,0,0,1
10.1016/j.neuroimage.2022.119100,github.com/davidgruskin/hcp_rsfc_isc,Code for all analyses can be found in the following Github repository: https://github.com/davidgruskin/hcp_rsfc_isc.,1,0,0
10.1016/j.neuroimage.2022.119100,github.com/yalemrrc/cpm,"Code for these rCPM analyses was adapted from https://github.com/YaleMRRC/CPM (Greene et al., 2020).",1,0,0
10.1016/j.neuroimage.2022.119100,pennlinc.github.io/s-a_archetypalaxis,(2021) and down-loaded from https://pennlinc.github.io/S-A_ArchetypalAxis/).,0,0,1
10.1016/j.neuroimage.2022.119100,db.humanconnectome.org,"More information on these clips can be found in Finn and Bandettini (2021) and at https://db.humanconnectome.org., Data Availability The raw HCP data used for this project can be downloaded from ConnectomeDB (db.humanconnectome.org).",0,1,0
10.1016/j.neuroimage.2022.119100,github.com/thomasyeolab/cbig/tree/master/stable_projects/brain_parcellation/kong2,(2021) (down-loaded from https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/brain_parcellation/Kong2).,0,0,1
10.1016/j.neuroimage.2022.119080,dbm.neuro.uni-jena.de/tfce,"Threshold-free cluster en-hancement (TFCE) was applied using the TFCE toolbox for SPM12 (R174, http://dbm.neuro.uni-jena.de/tfce/).",1,0,0
10.1016/j.neuroimage.2021.118828,github.com/developmentalimagingmcri/vibes-brain-age,"Python code supporting this study is available at: https://github.com/DevelopmentalImagingMCRI/VIBeS-brain-age Author statement Claire Kelly: Conceptualisation, Formal analysis, Investigation, Writing-original draft, Writing-review and editing, Visualisation.",1,0,0
10.1016/j.neuroimage.2021.118828,nda.nih.gov/edit_collection.html?id,"Data were acquired from The Pediatric Imaging, Neurocog-nition, and Genetics (PING) data repository (Jernigan et al., 2016), made available via the NIMH Data Archive (https://nda.nih.gov/edit_collection.html?id = 2607)., Data and code availability statement Data from the PING study are made available via the NIMH Data Archive (https://nda.nih.gov/edit_collection.html?id = 2607).",0,1,0
10.1016/j.neuroimage.2022.119348,fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/recruitement.html,"For more information about the recruitment strategy, please see http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/Recruitement.html.",0,0,1
10.1016/j.neuroimage.2022.119348,health.aiaudit.org/web/challenges/challenge-page/338/overview,"The information for the multi-target regression benchmark challenge is provided here: https://health.aiaudit.org/web/challenges/challenge-page/338/overview., The multi-target regression benchmark challenge is accessible here: https://health.aiaudit.org/web/challenges/challenge-page/338/overview.",0,0,1
10.1016/j.neuroimage.2022.119348,sccn.ucsd.edu/wiki/plugin_list_process,"EEG data were automatically preprocessed using the following steps: First, bad channels were detected by the algorithms implemented in the EEGLAB (v14.1.2) plugin clean_rawdata (http://sccn.ucsd.edu/wiki/Plugin_list_process).",1,0,0
10.1016/j.neuroimage.2022.119348,osf.io/2vw6j/(under,NeuroImage 258 (2022) 119348 Data and code availability statement for the manuscript The data are available online https://osf.io/2vw6j/(under CC-By At-tribution 4.0 International).,0,1,0
10.1016/j.neuroimage.2022.119348,osf.io/2vw6j,"The channel location ﬁle and visualization of the EEG montage are accessible on OSF (https://osf.io/2vw6j/)., All the data can be accessed via OSF: https://osf.io/2vw6j/., NeuroImage 258 (2022) 119348 Data and code availability statement for the manuscript The data are available online https://osf.io/2vw6j/(under CC-By At-tribution 4.0 International).",0,1,0
10.1016/j.neuroimage.2022.119348,paris-saclay-cds.github.io/autism_challenge,"Similar eﬀorts in the context of autism spectrum disorder are the ABIDE dataset (Di Martino et al., 2014) and the IMPAC challenge (https://paris-saclay-cds.github.io/autism_challenge/).",0,1,0
10.1016/j.neuroimage.2022.119348,brainclinics.com/resources,"the TD-BRAIN: https://brainclinics.com/resources/)., Deep learning approaches use large amounts of (EEG) data and most public EEG datasets only have small number of subjects with some exceptions to sleep (https://sleepdata.org/) and epilepsy (Obeid and Pi-cone 2016), the TD-BRAIN: https://brainclinics.com/resources/), and the current HBN data set.",0,1,0
10.1016/j.neuroimage.2022.119348,aiau-dit.org,"Finally, the two benchmark challenges are hosted on aiau-dit.org.",0,0,1
10.1016/j.neuroimage.2022.119348,nimh.nih.gov/research/research-funded-by-nimh/rdoc,"In response to this situation, the national insti-tute of mental health of the USA launched the Research Domain Cri-teria (RDoC) initiative (https://www.nimh.nih.gov/research/research-funded-by-nimh/rdoc) to develop a neuroscience-based nosologi-cal framework (“neurophenotyping ”) for future research on psy-chopathology by categorizing individuals using a dimensional approach (Insel et al., 2010).",0,0,1
10.1016/j.neuroimage.2022.119348,aiaudit.org,"We are hosting critical parts of the challenges infrastructure at a public platform aiaudit.org to con-tinue the challenge independently after the completion of the current evaluation round., predicting DSM diagnosis) can be found here: https://health.aiaudit.org/web/challenges/challenge-page/337/overview., The information for the multi-target regression benchmark challenge is provided here: https://health.aiaudit.org/web/challenges/challenge-page/338/overview., The ranking will be displayed in a public leaderboard at the challenges’ websites on health.aiaudit.org (see previous speciﬁc link for each benchmark chal-lenge)., The aiaudit.org platform does not allow uploading a docker solution yet., A software interface will be provided so that participants can make sure that their code will run on the aiaudit.org server., Participants are invited to frequently update their model on the aiaudit.org platform., The multi-task multi-label classiﬁcation challenge is hosted here: https://health.aiaudit.org/web/challenges/challenge-page/337/overview., The multi-target regression benchmark challenge is accessible here: https://health.aiaudit.org/web/challenges/challenge-page/338/overview., Acknowledgements We thank the aiaudit.org team for providing infrastructure for our benchmark challenge.",0,0,1
10.1016/j.neuroimage.2022.119348,brainclinics.com/resources,"the TD-BRAIN: https://brainclinics.com/resources/)., Deep learning approaches use large amounts of (EEG) data and most public EEG datasets only have small number of subjects with some exceptions to sleep (https://sleepdata.org/) and epilepsy (Obeid and Pi-cone 2016), the TD-BRAIN: https://brainclinics.com/resources/), and the current HBN data set.",0,1,0
10.1016/j.neuroimage.2022.119348,childmind.org,All data were recorded at a facility of the Child Mind Institute (https://childmind.org/).,0,1,0
10.1016/j.neuroimage.2022.119348,sleepdata.org,"Deep learning approaches use large amounts of (EEG) data and most public EEG datasets only have small number of subjects with some exceptions to sleep (https://sleepdata.org/) and epilepsy (Obeid and Pi-cone 2016), the TD-BRAIN: https://brainclinics.com/resources/), and the current HBN data set.",0,1,0
10.1016/j.neuroimage.2022.119348,health.aiaudit.org,"predicting DSM diagnosis) can be found here: https://health.aiaudit.org/web/challenges/challenge-page/337/overview., The information for the multi-target regression benchmark challenge is provided here: https://health.aiaudit.org/web/challenges/challenge-page/338/overview., The ranking will be displayed in a public leaderboard at the challenges’ websites on health.aiaudit.org (see previous speciﬁc link for each benchmark chal-lenge)., The multi-task multi-label classiﬁcation challenge is hosted here: https://health.aiaudit.org/web/challenges/challenge-page/337/overview., The multi-target regression benchmark challenge is accessible here: https://health.aiaudit.org/web/challenges/challenge-page/338/overview.",0,0,1
10.1016/j.neuroimage.2022.119348,bic.mni.mcgill.ca/servicesatlases/icbm152nlin2009,"A for-ward model was derived from the MNI ICBM 2009 template brain (http://www.bic.mni.mcgill.ca/ServicesAtlases/ICBM152NLin2009) using the OpenMEEG implementation (Gramfort, Papadopoulo, Olivi, & Clerc, 2010) of the Boundary Element Method (BEM).",1,0,0
10.1016/j.neuroimage.2022.119348,github.com/martynaplomecka/ai4health,"In our repository 1 , we provide an intuitive interface to reproduce our results and to use the methods presented here as a starting point for 1 https://github.com/MartynaPlomecka/AI4Health 9 N., The code for reproducing the baseline mod-els results are accessible here: https://github.com/MartynaPlomecka/AI4Health.",1,0,0
10.1016/j.neuroimage.2022.119348,health.aiaudit.org/web/challenges/challenge-page/337/overview,"predicting DSM diagnosis) can be found here: https://health.aiaudit.org/web/challenges/challenge-page/337/overview., The multi-task multi-label classiﬁcation challenge is hosted here: https://health.aiaudit.org/web/challenges/challenge-page/337/overview.",0,0,1
10.1016/j.neuroimage.2022.119673,netwiki.amath.unc.edu/genlouvain,"We ran the optimization employing the openly-available genlouvain package (http://netwiki.amath.unc.edu/GenLouvain/GenLouvain), im-plemented in Matlab (Jutla et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119673,fcon_1000.projects.nitrc.org/indi/enhanced/neurodata.html,The anonymized dataset is freely available at http://fcon_1000.projects.nitrc.org/indi/enhanced/neurodata.html.,0,1,0
10.1016/j.neuroimage.2022.119673,netwiki.amath.unc.edu/genlouvain/genlouvain,"We ran the optimization employing the openly-available genlouvain package (http://netwiki.amath.unc.edu/GenLouvain/GenLouvain), im-plemented in Matlab (Jutla et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119673,fcon_1000.projects.nitrc.org/indi/enhanced,"NKI dataset First, we considered data from the Nathan Kline Institute Rock-land Sample project (Nooner et al., 2012) (NKI-RS, http://fcon_1000.projects.nitrc.org/indi/enhanced/)., The anonymized dataset is freely available at http://fcon_1000.projects.nitrc.org/indi/enhanced/neurodata.html.",0,1,0
10.1016/j.neuroimage.2022.119673,github.com/mariagraziap/multimodal_multisubject_modularity,Code availability MATLAB code to build the multi-modal and multi-subject network and run the extended modularity optimization is available at https://github.com/mariagraziaP/multimodal_multisubject_modularity.,1,0,0
10.1016/j.neuroimage.2021.118789,github.com/britta-wstnr/beamformer_examples,"The code to reproduce the ﬁgures can be retrieved from https://github.com/britta-wstnr/beamformer_examples., Code availability The code used to generate the ﬁgure of this study is available under https://github.com/britta-wstnr/beamformer_examples.",1,0,0
10.1016/j.neuroimage.2021.118789,github.com/vyoussofzadeh/dics-beamformer-for-brainstorm,"Youssofzadeh, https://github.com/vyoussofzadeh/DICS-beamformer-for-Brainstorm).",0,0,1
10.1016/j.neuroimage.2022.119527,adni.loni.usc.edu,"Bourgeat). 1 Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf https://doi.org/10.1016/j.neuroimage.2022.119527., Data used in the preparation of this article were partly obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)., The AIBL data can be downloaded through LONI after registration at http://adni.loni.usc.edu/category/aibl-study-data/The ADNI data can be downloaded through LONI after registration at http://adni.loni.usc.edu/data-samples/access-data/The OASIS3 data can be downloaded through XNAT after registration at https://central.xnat.org/data/projects/OASIS3 The python code used to build the NMF models as well as the NMF models can be downloaded from 10.25919/5f8400a0b6a1e Credit authorship contribution statement Pierrick Bourgeat: Methodology, Software, Validation, Formal analysis, Data curation, Writing –original draft.",0,0,1
10.1016/j.neuroimage.2022.119527,adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf,A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf https://doi.org/10.1016/j.neuroimage.2022.119527.,0,0,1
10.1016/j.neuroimage.2022.119527,gaain.org/centiloid-project,This composite reference region has been widely used for SUVR 1 http://www.gaain.org/centiloid-project.,0,0,1
10.1016/j.neuroimage.2022.119527,central.xnat.org/data/projects/oasis3,"The AIBL data can be downloaded through LONI after registration at http://adni.loni.usc.edu/category/aibl-study-data/The ADNI data can be downloaded through LONI after registration at http://adni.loni.usc.edu/data-samples/access-data/The OASIS3 data can be downloaded through XNAT after registration at https://central.xnat.org/data/projects/OASIS3 The python code used to build the NMF models as well as the NMF models can be downloaded from 10.25919/5f8400a0b6a1e Credit authorship contribution statement Pierrick Bourgeat: Methodology, Software, Validation, Formal analysis, Data curation, Writing –original draft.",0,1,0
10.1016/j.neuroimage.2022.119527,adni-info.org,"For up-to-date information, see www.adni-info.org.",0,0,1
10.1016/j.neuroimage.2022.119527,fnih.org,Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org).,0,0,1
10.1016/j.neuroimage.2022.119527,adni.loni.usc.edu/data-samples/access-data/the,"The AIBL data can be downloaded through LONI after registration at http://adni.loni.usc.edu/category/aibl-study-data/The ADNI data can be downloaded through LONI after registration at http://adni.loni.usc.edu/data-samples/access-data/The OASIS3 data can be downloaded through XNAT after registration at https://central.xnat.org/data/projects/OASIS3 The python code used to build the NMF models as well as the NMF models can be downloaded from 10.25919/5f8400a0b6a1e Credit authorship contribution statement Pierrick Bourgeat: Methodology, Software, Validation, Formal analysis, Data curation, Writing –original draft.",0,1,0
10.1016/j.neuroimage.2022.119527,adni.loni.usc.edu/category/aibl-study-data/the,"The AIBL data can be downloaded through LONI after registration at http://adni.loni.usc.edu/category/aibl-study-data/The ADNI data can be downloaded through LONI after registration at http://adni.loni.usc.edu/data-samples/access-data/The OASIS3 data can be downloaded through XNAT after registration at https://central.xnat.org/data/projects/OASIS3 The python code used to build the NMF models as well as the NMF models can be downloaded from 10.25919/5f8400a0b6a1e Credit authorship contribution statement Pierrick Bourgeat: Methodology, Software, Validation, Formal analysis, Data curation, Writing –original draft.",0,1,0
10.1016/j.neuroimage.2022.119126,bankmycell.com/blog/how-many-phones-are-in-the-world,"Since inception of the iPhones in 2007, these ﬁnger gestures are an evolutionary new behavior that humans have rapidly mastered over the past 14 years, and now 3.8 bil-lion people worldwide are estimated to own a smartphone, as of July 2021 (https://www.bankmycell.com/blog/how-many-phones-are-in-the-world)., https://www.bankmycell.com/blog/how-many-phones-are-in-the-world#sources (accessed 22 Jury 2021).",0,0,1
10.1016/j.neuroimage.2022.119126,brain.labsolver.org/diﬀusion-mri-templates/hcp-842-hcp-1021,"As previously performed (Mitsuhashi et al., 2021 ; Sonoda et al., 2021), we generated DWI tractography using the open-source data av-eraged across 1065 individuals participating in the Human Connec-tome Project (http://brain.labsolver.org/diﬀusion-mri-templates/hcp-842-hcp-1021 ; Yeh et al., 2018).",0,1,0
10.1016/j.neuroimage.2022.119126,bankmycell.com/blog/how-many-phones-are-in-the-world#sources,https://www.bankmycell.com/blog/how-many-phones-are-in-the-world#sources (accessed 22 Jury 2021).,0,0,1
10.1016/j.neuroimage.2022.119126,lumosity.com/hcp,We want to thank Lumos Labs for providing us with the software as a part of the Lumosity Human Cognition Project (https://www.lumosity.com/hcp).,1,0,0
10.1016/j.neuroimage.2022.119126,surfer.nmr.mgh.harvard.edu,"The FreeSurfer script (http://surfer.nmr.mgh.harvard.edu) spatially normal-ized given electrode sites to standard Montreal Neurological Institute (MNI) space (Fig. 2 ; Ghosh et al., 2010 ; Mitsuhashi et al., 2021).",1,0,0
10.1016/j.neuroimage.2022.119126,lumosity.com,"1), a cognitive ﬂexibility game on the Lumosity platform (https://www.lumosity.com/; Lumos Labs, Inc, San Francisco, CA), (iv) gameplay during the interictal state, (v) swipe responses strictly using the same hand, and (vi) correct responses in more than 60% of trials (chance level = 25%)., We want to thank Lumos Labs for providing us with the software as a part of the Lumosity Human Cognition Project (https://www.lumosity.com/hcp).",1,0,0
10.1016/j.neuroimage.2022.119126,dsi-studio.labsolver.org,The DSI Studio (http://dsi-studio.labsolver.org/) generated streamlines between these ROIs within Montreal Neurological Institute (MNI) standard space.,1,0,0
10.1016/j.neuroimage.2022.119057,2.2.6.3,2.2.6.3.,0,0,1
10.1016/j.neuroimage.2022.119057,2.2.6.4,2.2.6.4.,0,0,1
10.1016/j.neuroimage.2022.119057,2.2.7.6,2.2.7.6.,0,0,1
10.1016/j.neuroimage.2022.119057,2.2.3.1,iEEG data analysis 2.2.3.1.,0,0,1
10.1016/j.neuroimage.2022.119057,2.2.3.2,2.2.3.2.,0,0,1
10.1016/j.neuroimage.2022.119057,2.2.7.5,2.2.7.5.,0,0,1
10.1016/j.neuroimage.2022.119755,github.com/cbort-ncbib/oct-cbort,NeuroImage 264 (2022) 119755 Data and code availability statement An open-source Python package for reconstruction of conventional OCT tomograms and depth-resolved tissue birefringence is available on http://github.com/CBORT-NCBIB/oct-cbort.,1,0,0
10.1016/j.neuroimage.2022.119636,github.com/thomasyeolab/ooi2022_mmp_hcp,"Data for the HCP are available in this Github repository (https://github.com/ThomasYeoLab/Ooi2022_MMP_HCP)., Data for the HCP are available in this Github repository (https://github.com/ThomasYeoLab/Ooi2022_MMP_HCP).",0,1,0
10.1016/j.neuroimage.2022.119636,nscc.sg,"Our computational work was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).",1,0,0
10.1016/j.neuroimage.2022.119636,abcdstudy.org/consortium_members,A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.,0,0,1
10.1016/j.neuroimage.2022.119636,github.com/thomasyeolab/cbig,"Code for this study is publicly available in the Github reposi-tory maintained by the Computational Brain Imaging Group (https://github.com/ThomasYeoLab/CBIG)., Processing pipelines for diﬀusion data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG2022_DiﬀProc), and functional data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG_fMRI_Preproc2016) are provided in their respective links., Code for this study is publicly available in the Github reposi-tory maintained by the Computational Brain Imaging Group (https://github.com/ThomasYeoLab/CBIG)., Processing pipelines for diﬀusion data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG2022_DiﬀProc), and functional data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG_fMRI_Preproc2016) are provided in their respective links.",1,0,0
10.1016/j.neuroimage.2022.119636,github.com/thomasyeolab/cbig/tree/master/stable_projects/preprocessing/cbig2022_diﬀproc,"Processing pipelines for diﬀusion data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG2022_DiﬀProc), and functional data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG_fMRI_Preproc2016) are provided in their respective links., Processing pipelines for diﬀusion data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG2022_DiﬀProc), and functional data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG_fMRI_Preproc2016) are provided in their respective links.",0,0,1
10.1016/j.neuroimage.2022.119636,abcdstudy.org,"Any additional data can be accessed directly from the HCP (https://www.humanconnectome.org/) and ABCD (https://abcdstudy.org/) websites, as they are both publicly available., Any additional data can be accessed directly from the HCP (https://www.humanconnectome.org/) and ABCD (https://abcdstudy.org/) websites, as they are both publicly available., Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development SM (ABCD) Study (https://abcdstudy.org), held in the NIMH Data Archive (NDA)., A full list of supporters is available at https://abcdstudy.org/federal-partners.html., A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.",0,1,0
10.1016/j.neuroimage.2022.119636,abcdstudy.org/federal-partners.html,A full list of supporters is available at https://abcdstudy.org/federal-partners.html.,0,0,1
10.1016/j.neuroimage.2022.119636,humanconnectome.org,"Any additional data can be accessed directly from the HCP (https://www.humanconnectome.org/) and ABCD (https://abcdstudy.org/) websites, as they are both publicly available., Any additional data can be accessed directly from the HCP (https://www.humanconnectome.org/) and ABCD (https://abcdstudy.org/) websites, as they are both publicly available.",0,1,0
10.1016/j.neuroimage.2022.119636,github.com/thomasyeolab/cbig/tree/master/stable_projects/preprocessing/cbig_fmri_preproc2016,"Processing pipelines for diﬀusion data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG2022_DiﬀProc), and functional data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG_fMRI_Preproc2016) are provided in their respective links., Processing pipelines for diﬀusion data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG2022_DiﬀProc), and functional data (https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/preprocessing/CBIG_fMRI_Preproc2016) are provided in their respective links.",0,0,1
10.1016/j.neuroimage.2022.119636,abcdstudy.org,"Any additional data can be accessed directly from the HCP (https://www.humanconnectome.org/) and ABCD (https://abcdstudy.org/) websites, as they are both publicly available., Any additional data can be accessed directly from the HCP (https://www.humanconnectome.org/) and ABCD (https://abcdstudy.org/) websites, as they are both publicly available., A full list of supporters is available at https://abcdstudy.org/federal-partners.html., A listing of participating sites and a complete listing of the study investigators can be found at https://abcdstudy.org/consortium_members/.",0,1,0
10.1016/j.neuroimage.2022.119476,openneuro.org/datasets/ds000224/versions/1.0.3,Dataset and Code Availability MSC data has been made publicly available (https://openneuro.org/datasets/ds000224/versions/1.0.3).,0,1,0
10.1016/j.neuroimage.2022.119476,github.com/grattonlab/ladwig_2022_events_static_fc,The parcellated timeseries used for these analyses is available here (https://github.com/GrattonLab/MSC_ROI_data).The code for the analyses in this paper is available at (https://github.com/GrattonLab/Ladwig_2022_Events_Static_FC).,1,0,0
10.1016/j.neuroimage.2022.119476,github.com/grattonlab/msc_roi_data,The parcellated timeseries used for these analyses is available here (https://github.com/GrattonLab/MSC_ROI_data).The code for the analyses in this paper is available at (https://github.com/GrattonLab/Ladwig_2022_Events_Static_FC).,1,0,0
10.1016/j.neuroimage.2022.119524,osf.io/z24eg/credit,"For reasons of transparency, additional ﬁles are publicly available under the following link: https://osf.io/z24eg/Credit authorship contribution statement Amelie M.",0,0,1
10.1016/j.neuroimage.2022.119524,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"fMRI data preprocessing All brain image preprocessing and basic statistical analyses were performed with SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and custom Matlab scripts (Version R2020b; The MathWorks Inc., Natick, MA, USA).",1,0,0
10.1016/j.neuroimage.2022.119524,fmri.wfubmc.edu/cms/software,"All ROIs were derived from the automated anatomical labelling (AAL) atlas and created using the SPM Wake Forest University (WFU) Pickatlas toolbox (http://www.fmri.wfubmc.edu/cms/software , version 2.3) (Maldjian et al., 2003).",1,0,0
10.1016/j.neuroimage.2021.118867,clinicaltrials.gov,"Six of them participated after providing informed consent to a protocol approved by the National Institutes of Health Combined Neuroscience Institutional Review Board (93-M-0170, ClinicalTrials.gov identiﬁer: NCT00001360) in accordance with the Belmont Report and U.S.",0,0,1
10.1016/j.neuroimage.2021.118867,github.com/layerfmri/laynii,"Layering methods and proﬁle extraction Layer-speciﬁc analyses were conducted using the open software suite LAYNII (https://github.com/layerfMRI/LAYNII) (Huber et al., 2021).",1,0,0
10.1016/j.neuroimage.2021.118842,vinci.sf.mpg.de,"Data and code availability The PET data sets are stored in a repository: https://doi.org/10.26165/JUELICH-DATA/MGRYJV The PET analysis software VINCI (Max Planck Institute for Metabolism Research) is freely available (for academia) at: http://vinci.sf.mpg.de There are software versions for MS Windows, Linux and MacOS.",1,0,0
10.1016/j.neuroimage.2022.119450,osf.io/d3nkc,Other study data and code are available at: https://osf.io/d3nkc/.,0,1,0
10.1016/j.neuroimage.2022.119450,ed.ac.uk/edinburgh-imaging,"Imaging was carried out at the Edinburgh Imaging Facility (www.ed.ac.uk/edinburgh-imaging), University of Edinburgh, which is part of the SINAPSE collaboration (www.sinapse.ac.uk).",0,0,1
10.1016/j.neuroimage.2022.119450,sinapse.ac.uk,"Imaging was carried out at the Edinburgh Imaging Facility (www.ed.ac.uk/edinburgh-imaging), University of Edinburgh, which is part of the SINAPSE collaboration (www.sinapse.ac.uk).",0,0,1
10.1016/j.neuroimage.2022.119450,neurovault.org/collections/11009,Data and code availability Group-level results maps and ROI masks are archived at: https://neurovault.org/collections/11009/.,0,1,0
10.1016/j.neuroimage.2022.119439,github.com/antsx/ants,"Speciﬁcally, the study template was created following the script “antsMultivariateTemplateConstruction2.sh ”of ANTS (https://github.com/ANTsX/ANTs), running an iterative registration proce-dure based on the FA and L0 images, using the b-spline SyN transfor-mation, and cross correlation as a cost function.",1,0,0
10.1016/j.neuroimage.2022.119439,github.com/robbert-harms/mdt,"To derive NODDI met-rics, we employed the “microstructure diﬀusion toolbox ”(Harms, 2017) (https://github.com/robbert-harms/MDT) running on Python 3.8, and CUDA 10.",1,0,0
10.1016/j.neuroimage.2022.119061,litteratureaudio.com,"Individual noise components were obtained from a French au-diobook database (http://www.litteratureaudio.com), normalized, and mixed linearly.",0,1,0
10.1016/j.neuroimage.2022.119061,osf.io/4q3tz,"Data availability The data and the code that support the ﬁndings of this study are available on the Open Science Framework at (https://osf.io/4q3tz/)., Data availability The MEG data as well as analysis code are available at https://osf.io/4q3tz/.",0,1,0
10.1016/j.neuroimage.2022.119759,osf.io/x934v,"Müller: Conceptu-alization, Methodology, Resources, Writing-Reviewing and Editing, Su-pervision, Funding acquisition Data and code availability statement Anonymized and preprocessed EEG data, and code to replicate the manuscript ﬁgures will be made available at osf: https://osf.io/x934v/.",0,0,1
10.1016/j.neuroimage.2022.119759,github.com/klabhub/bayesfactor,"Tests for equality were achieved by Bayes Factor (BF) testing employing a standard JZS prior of √2/2 (Rouder et al., 2009) as implemented by Bart Krekelbergs (Bayes Factor Matlab package: https://github.com/klabhub/bayesFactor).",1,0,0
10.1016/j.neuroimage.2022.119175,psychopy.org,"The entire sequence was coded and delivered with the Psy-choPy toolbox (http://www.psychopy.org/) (Peirce, 2009 , 2007).",1,0,0
10.1016/j.neuroimage.2022.118919,ida.loni.usc.edu/login.jsp,The template Human Connectome Project (HCP) connectome used in the preparation of this work were obtained from the MGH-USC HCP database (https://ida.loni.usc.edu/login.jsp).,0,1,0
10.1016/j.neuroimage.2022.118919,github.com/raj-lab-ucsf/spectrome-revisited,"Data availability The code and processed datasets (processed connectivity and dis-tance matrices, and the MEG spectra) used in this work are available at https://github.com/Raj-Lab-UCSF/spectrome-revisited and are based on the original SGM repository (Xie et al., 2020).",0,1,0
10.1016/j.neuroimage.2022.119032,nitrc.org/projects/gppi,"Also, an average reference condition contrast was created by taking the three reference conditions together relative to the control condition ((self, similar, dissimilar) > control) To examine functional connectivity of the vMPFC and dMPFC during self-and other-referential processing, the generalized form of context-dependent psycho-physiological interaction analyses was performed (gPPI, (McLaren et al., 2012) using the gPPI toolbox (version 13.1, https://www.nitrc.org/projects/gppi)).",1,0,0
10.1016/j.neuroimage.2022.119032,2.8.3.1,MRI group-level analyses: adolescents wave 3 versus young adults 2.8.3.1.,0,0,1
10.1016/j.neuroimage.2022.119032,2.8.2.1,2.8.2.1.,0,0,1
10.1016/j.neuroimage.2022.119032,marsbar.sourceforge.net,"ROI analyses were performed by extracting the average contrast es-timate (self, similar, dissimilar all relative to control) within a ROI for each subject and each wave (MarsBar toolbox, version 0.44, http://marsbar.sourceforge.net/).",1,0,0
10.1016/j.neuroimage.2022.119032,ﬁl.ion.ucl.ac.uk/spm,"In brief, all images were preprocessed using SPM12 (http://www.ﬁl.ion.ucl.ac.uk/spm).",1,0,0
10.1016/j.neuroimage.2022.119032,2.8.3.3,2.8.3.3.,0,0,1
10.1016/j.neuroimage.2022.119032,2.8.2.3,2.8.2.3.,0,0,1
10.1016/j.neuroimage.2022.119032,2.8.3.2,2.8.3.2.,0,0,1
10.1016/j.neuroimage.2022.119032,github.com/marietvbuuren/self_other_longitudinal,"(https://github.com/marietvbuuren/self_other_longitudinal)., (https://github.com/marietvbuuren/self_other_longitudinal).",0,0,1
10.1016/j.neuroimage.2022.119032,2.8.2.2,2.8.2.2.,0,0,1
10.1016/j.neuroimage.2022.119373,mcgill.ca/bic/resources/omega,Data and code availability statement Data were obtained from the Open MEG Archive (OMEGA) (https://www.mcgill.ca/bic/resources/omega).,0,1,0
10.1016/j.neuroimage.2022.119373,github.com/necog-uam/omega-naturalfrequencies,"All scripts neces-sary to reproduce the analysis and the ﬁgures in this paper are available at https://github.com/necog-UAM/OMEGA-NaturalFrequencies., All scripts necessary to reproduce the analysis and the ﬁgures in this paper are available at https://github.com/necog-UAM/OMEGA-NaturalFrequencies.",1,0,0
10.1016/j.neuroimage.2022.119306,clinicaltrials.gov,"Among the rest of participants, the most frequently used preregistra-tion platform was the Open Science Framework (OSF, 32.5%), followed by ClinicalTrials.gov (25.1%), and AsPredicted (9.5%).",0,0,1
10.1016/j.neuroimage.2022.119306,convertcsv.com/email-extractor.htm,"The search re-sults were exported to text ﬁles, which were further processed with an online app to remove duplicates and to extract email addresses (https://www.convertcsv.com/email-extractor.htm).",0,0,1
10.1016/j.neuroimage.2022.119306,github.com/christianparet/survey-on-open-science-practices-in-functional-neuroimaging.-dataset-and-materials,"Participants A PubMed search with the search term (“fMRI ”O R “functional magnetic resonance imaging ”O R “functional Magnetic Resonance Imaging ”)) AND ((“2010/01/01 ”Date -Publication : “3000 ”Date -Publication)) 2 and Filter: Humans was done to collect email ad-dresses from corresponding authors of scientiﬁc articles published 1 https://github.com/christianparet/Survey-on-Open-Science-Practices-in-Functional-Neuroimaging.-Dataset-and-Materials. 2 Practically, we did four searches, each covering a period of two or more years (i.e., 2010-2012, 2013-2014, 2015-2016, 2017-today), to limit the total number of exported entries per search.",0,0,1
10.1016/j.neuroimage.2022.119306,github.com/christianparet/survey-on-open-science-practices-in-functional-neuroimaging,"Participants A PubMed search with the search term (“fMRI ”O R “functional magnetic resonance imaging ”O R “functional Magnetic Resonance Imaging ”)) AND ((“2010/01/01 ”Date -Publication : “3000 ”Date -Publication)) 2 and Filter: Humans was done to collect email ad-dresses from corresponding authors of scientiﬁc articles published 1 https://github.com/christianparet/Survey-on-Open-Science-Practices-in-Functional-Neuroimaging.-Dataset-and-Materials. 2 Practically, we did four searches, each covering a period of two or more years (i.e., 2010-2012, 2013-2014, 2015-2016, 2017-today), to limit the total number of exported entries per search.",0,0,1
10.1016/j.neuroimage.2022.119095,millisecond.com,"After each training ses-sion, logﬁles were automatically uploaded to the Millisecond Software website (https://www.millisecond.com/).",0,0,1
10.1016/j.neuroimage.2022.119095,github.com/neurolabusc/dcm2niix,"Imaging preprocessing All DICOM neuroimaging data was converted to 3D-NIFTI vol-umes using the dcm2niix tool (https://github.com/neurolabusc/dcm2niix), expect for fMRI data, for which we used dcm2nii.",1,0,0
10.1016/j.neuroimage.2022.119095,ﬁl.ion.ucl.ac.uk/spm/software/spm12,"Task fMRI and FDG-PET data were pre-processed using SPM12 (https://www.ﬁl.ion.ucl.ac.uk/spm/software/spm12/) and MATLAB v2017b (The MathWorks Inc., Natick, Massachusetts, USA).",1,0,0
10.1016/j.neuroimage.2022.119581,youtube.com/watch?v,A video example of a test trial is available online at https://www.youtube.com/watch?v = LMs-Gpo2Ss7M Exploration.,0,0,1
10.1016/j.neuroimage.2022.119639,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"Electrodes locations were non-linearly co-registered to the patient MP-RAGE taken before implanta-tion, and then to MNI standard space using FLIRT (FMRIB’s Linear Image Registration Tool) and FNIRT (FMRIB’s Non-linear Image Registration Tool) (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki) (Matsumoto et al., 2004 ; Matsumoto et al., 2011).",1,0,0
10.1016/j.neuroimage.2022.119639,neurosynth.org,"Firstly, we obtained the resting-state functional connectivity map from the rs-fMRI connectivity database (available on NeuroSynth web-site; http://neurosynth.org/) (Yarkoni et al., 2011).",0,1,0
10.1016/j.neuroimage.2022.119552,lead-dbs.org,"These novice raters, one medical student and 2 neurol-ogy residents with ∼1 year experience in DBS programming, were in-troduced to Lead-DBS by a structured two days’ workshop comparable to the Lead-DBS workshops that have regularly been held worldwide (All screencasts from a 2018 workshop in Shanghai can be found on-line: https://www.lead-dbs.org/helpsupport/knowledge-base/videos/)., All code used to analyze data as presented in the manuscript is openly available within the Lead-DBS software (https://github.com/netstim/leaddbs ; https://www.lead-dbs.org/).",0,0,1
10.1016/j.neuroimage.2022.119552,lead-dbs.org/helpsupport/knowledge-base/videos,"These novice raters, one medical student and 2 neurol-ogy residents with ∼1 year experience in DBS programming, were in-troduced to Lead-DBS by a structured two days’ workshop comparable to the Lead-DBS workshops that have regularly been held worldwide (All screencasts from a 2018 workshop in Shanghai can be found on-line: https://www.lead-dbs.org/helpsupport/knowledge-base/videos/).",0,0,1
10.1016/j.neuroimage.2022.119552,jpnd.eu,The project is supported through the following funding organisations under the aegis of JPND -www.jpnd.eu (A.H.: the Deutsches Zentrum für Luft-und Raumfahrt -Germany; B.C.M.v.W.: the Netherlands organisation for Health Research and Development (ZonMw) -The Netherlands).,0,0,1
10.1016/j.neuroimage.2022.119552,lead-dbs.org,"These novice raters, one medical student and 2 neurol-ogy residents with ∼1 year experience in DBS programming, were in-troduced to Lead-DBS by a structured two days’ workshop comparable to the Lead-DBS workshops that have regularly been held worldwide (All screencasts from a 2018 workshop in Shanghai can be found on-line: https://www.lead-dbs.org/helpsupport/knowledge-base/videos/)., Electrode localization was performed with Lead-DBS software (www.lead-dbs.org ; Horn & Kühn, 2015b) using the default work-ﬂow of version 2.1.7 as described in Horn et al., All code used to analyze data as presented in the manuscript is openly available within the Lead-DBS software (https://github.com/netstim/leaddbs ; https://www.lead-dbs.org/).",1,0,0
10.1016/j.neuroimage.2022.119552,github.com/netstim/leaddbs,All code used to analyze data as presented in the manuscript is openly available within the Lead-DBS software (https://github.com/netstim/leaddbs ; https://www.lead-dbs.org/).,1,0,0
10.1016/j.neuroimage.2022.118922,github.com/daducci/commit,"The code is open source and freely available at https://github.com/daducci/COMMIT., The code is open source and freely-available at https://github.com/daducci/COMMIT.",1,0,0
10.1016/j.neuroimage.2022.118922,surfer.nmr.mgh.harvard.edu,"MP2RAGE images were processed using FreeSurfer (http://surfer.nmr.mgh.harvard.edu) to obtain subject speciﬁc gray matter parcellations in 84 regions of interest (ROIs) ac-cording to the standard Desikan-Killiany atlas (Desikan et al., 2006).",1,0,0
10.1016/j.neuroimage.2022.118922,fsl.fmrib.ox.ac.uk,"MRI processing dMRI images were pre-processed to remove noise (Veraart and Novikov, 2016 ; Veraart et al., 2016), eddy currents (Andersson and Sotiropoulos 2016 ; Andersson et al., 2016), motion and EPI distortion artefacts (Andersson et al., 2003 ; Smith et al., 2004) using MRtrix3 (Tournier et al., 2019) and FSL (https://fsl.fmrib.ox.ac.uk).",1,0,0
10.1016/j.neuroimage.2022.119713,github.com/ohba-analysis/hmm-mar,HMM training and state-wise frequency analysis was performed using the HMM-MAR toolbox (https://github.com/OHBA-analysis/HMM-MAR).,1,0,0
10.1016/j.neuroimage.2022.119713,osf.io/btwgy/(dataset,"Data availability ROI time series, along with the underlying MATLAB code of this work are available via the open science framework: https://osf.io/8cyt6/(dataset 1) and https://osf.io/btwgy/(dataset 2).",0,1,0
10.1016/j.neuroimage.2022.119713,github.com/ohba-analysis/meg-roi-nets,"A multivariate correction for spatial-leakage (Colclough et al., 2015) was applied using the ROInets toolbox (https://github.com/OHBA-analysis/MEG-ROI-nets).",1,0,0
10.1016/j.neuroimage.2022.119713,osf.io/8cyt6/(dataset,"Data availability ROI time series, along with the underlying MATLAB code of this work are available via the open science framework: https://osf.io/8cyt6/(dataset 1) and https://osf.io/btwgy/(dataset 2).",0,1,0
10.1016/j.neuroimage.2022.119688,visualneuroscience.nl,"Anonymized and prepro-cessed time series for each visual area can be downloaded at this link: http://www.visualneuroscience.nl/cf., The code for the bCF framework can be obtained via http://www.visualneuroscience.nl/cf., The code presented in this manuscript will be make openly available after acceptance at http://www.visualneuroscience.nl Sincerely yours, on the behalf of all authors, Azzurra Invernizzi, PhD Data Availability Data will be made available on request.",1,0,0
10.1016/j.neuroimage.2022.119688,fsl.fmrib.ox.ac.uk/fsl/fslwiki,"Then, functional data were aligned using an in-house code based on the following steps: 1) run FLIRT (the au-tomated registration tool of FLS, https://fsl.fmrib.ox.ac.uk/fsl/fslwiki , Woolrich et al., 2009 ; Smith et al., 2004 ; “FSL ”, 2012), then 2) run the robust registration algorithm described by Nestares et al.",1,0,0
10.1016/j.neuroimage.2022.119688,wiki.hpc.rug.nl/peregrine/start,"Currently, we addressed this by using parallel GPU computing and implementing the method using cluster computing (https://wiki.hpc.rug.nl/peregrine/start) -per node, we used 48 Intel Xeon 2.5 GHz cores with 512 GB of internal memory (Avesani et al., 2019).",1,0,0
10.1016/j.neuroimage.2022.119688,white.stanford.edu,"Standard fMRI data analysis Preprocessing and standard (pRF and CF) analyses of fMRI data were done using ITKGray (http://www.itk.org) and the mrVista tool-box for MatLab (VISTASOFT, https://github.com/vistalab/vistasoft ; http://www.white.stanford.edu)., In this way, the functional time series will be spatially interpolated to the nearest functional voxel (VISTASOFT, https://github.com/vistalab/vistasoft ; http://www.white.stanford.edu).",1,0,0
10.1016/j.neuroimage.2022.119688,github.com/vistalab/vistasoft,"Standard fMRI data analysis Preprocessing and standard (pRF and CF) analyses of fMRI data were done using ITKGray (http://www.itk.org) and the mrVista tool-box for MatLab (VISTASOFT, https://github.com/vistalab/vistasoft ; http://www.white.stanford.edu)., In this way, the functional time series will be spatially interpolated to the nearest functional voxel (VISTASOFT, https://github.com/vistalab/vistasoft ; http://www.white.stanford.edu).",1,0,0
10.1016/j.neuroimage.2022.119688,2.5.2.2,"2.5.2.2., In the bCF B , (Section 2.5.2.2), 𝜷is estimated jointly with CF size and constrained to be positive (Zeidman et al., 2018) using the following equation: 𝜷= 𝒆 𝒙 𝒑 (𝒍 𝜷)(14) The latent variable 𝒍 𝜷is deﬁned with a prior distribution 𝑵 (−2 , 5) and the proposed 𝜷value is controlled by 𝒍 𝜷𝒑 𝒓 𝒐 𝒑 𝒐 𝒔 𝒆 𝒅 (Eq.",1,0,0
10.1016/j.neuroimage.2022.119688,2.5.3.3,"𝒍 𝝈𝒑 𝒓 𝒐 𝒑 𝒐 𝒔 𝒆 𝒅 = 𝑵 (𝒍 𝝈𝒂 𝒄 𝒄 𝒆 𝒑 𝒕 𝒆 𝒅 , 𝐰)(13) 2.5.3.3.",0,0,1
10.1016/j.neuroimage.2022.119688,psychtoolbox.org,"Stimuli were gener-ated and displayed using the Psychtoolbox (http://psychtoolbox.org/) and VISTADISP toolbox (VISTA Lab, Stanford University), which are both MatLab based (Brainard, 1997 ; Pelli, 1997).",1,0,0
10.1016/j.neuroimage.2022.119688,p.o.box,"While modeling receptive ﬁelds in the visual cortex in terms of their response ∗ Corresponding author at: Laboratory for Experimental Ophthalmology, University Medical Center Groningen, P.O.Box 30.001, 9700 RB Groningen, the Nether-lands.",0,0,1
10.1016/j.neuroimage.2022.119688,2.5.2.1,"2.5.2.1., As an aside, note that in bCF A (Section 2.5.2.1), 𝜷is esti-mated at a higher hierarchical level inside the loop using an OLS ﬁt and can, theoretically, range between −∞and +∞.",1,0,0
10.1016/j.neuroimage.2022.119688,2.5.3.2,"𝒅 𝒑 𝒓 𝒐 𝒑 𝒐 𝒔 𝒆 𝒅 = 𝑫 (𝒗 , 𝒗 0 𝒑 𝒓 𝒐 𝒑 𝒐 𝒔 𝒆 𝒅 (10) 𝒅 𝒂 𝒄 𝒄 𝒆 𝒑 𝑡 𝒆 𝒅 = 𝑫 (𝒗 , 𝒗 0 𝒂 𝒄 𝒄 𝒆 𝒑 𝒕 𝒆 𝒅 (11) 2.5.3.2.",0,0,1
10.1016/j.neuroimage.2022.119688,visualneuroscience.nl/cf,"Anonymized and prepro-cessed time series for each visual area can be downloaded at this link: http://www.visualneuroscience.nl/cf., The code for the bCF framework can be obtained via http://www.visualneuroscience.nl/cf.",1,0,0
10.1016/j.neuroimage.2022.119688,2.5.3.1,2.5.3.1.,0,0,1
10.1016/j.neuroimage.2022.119688,itk.org,"Standard fMRI data analysis Preprocessing and standard (pRF and CF) analyses of fMRI data were done using ITKGray (http://www.itk.org) and the mrVista tool-box for MatLab (VISTASOFT, https://github.com/vistalab/vistasoft ; http://www.white.stanford.edu).",1,0,0
10.1016/j.neuroimage.2022.118939,osf.io/k3xce,Data and code availability De-identiﬁed data and custom-built MATLAB codes are available at https://osf.io/k3xce.,0,1,0
10.1016/j.neuroimage.2022.119149,surfer.nmr.mgh.harvard.edu,T1-weighted MR images were processed using FreeSurfer (https://surfer.nmr.mgh.harvard.edu/).,1,0,0
