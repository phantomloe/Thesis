{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7067776c",
   "metadata": {},
   "source": [
    "# Table of contents \n",
    "- [Setup](#setup) \n",
    "    - [Target](#target)\n",
    "    - [Libraries](#libraries)\n",
    "- [Gather datasets](#gatherdatasets)\n",
    "    - [Get content](#getcontent)\n",
    "        - [Section patterns v1](#sectionpatternsv1)\n",
    "        - [Section patterns v2](#sectionpatternsv2)\n",
    "        - \n",
    "    - [Get datasets](#getdatasets)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af309d",
   "metadata": {},
   "source": [
    "<a name='setup'></a>\n",
    "# 0. Setup \n",
    "\n",
    "This notebook contains the code to extract the datasets used in the articles published in NeuroImage in 2022. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='target'></a> \n",
    "## 0.1. Target\n",
    "The goal is the use pypdf to locate and extract the datasets used for analysis in the research articles. Based on an initial review of nine random \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='libraries'></a>\n",
    "## 0.2. Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json \n",
    "import os \n",
    "import re \n",
    "\n",
    "import pypdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6525d",
   "metadata": {},
   "source": [
    "<a name='gatherdatasets'></a>\n",
    "# 1. Gather datasets \n",
    "\n",
    "PLAN OF ATTACK TO EXPLORE: \n",
    "* IF - Locate 'Data availability' (or similar) section and look for links - if multiple, save all of them and look at surrounding words for context \n",
    "* ELSE If there is no 'Data availability' (or similar) section \n",
    "\t* Look at wording in section 2.1 \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='getcontent'></a>\n",
    "## 1.1. Get content \n",
    "\n",
    "I use the work of Akkoç (2023) and Sourget (2023) to search the PDFs for their datasets. I am using the code from two separate git repositories as inspiration for the two functions presented in this section. \n",
    "- *get_section* is losely interpreted from Akkoç (2023) using the following breadcrumb in the github repository: PublicDatasets/ArticleAnalyser.ipynb, section '2.1 Get section function'\n",
    "- *get_content* is losely interpreted from Soruget (2023) using the following breadcrumb in the github repository: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "\n",
    "<br>\n",
    "\n",
    "References: \n",
    "- Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022)\n",
    "- Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc7a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(pdf_path, alt_pdf_path, section_patterns):\n",
    "    \"\"\"Get a PDF. \n",
    "    This function is loosely interpreted from Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget\n",
    "    specifically: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "    \n",
    "    Parameters: \n",
    "    :param pdf_path (str): Path to the PDF file.\n",
    "    :param json_file_path (str): Path to the JSON file containing the DOIs of the relevant research articles. \n",
    "    \n",
    "    Returns: \n",
    "    :return: Extracted content or 'Editorial board' if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_file = open(pdf_path, 'rb')\n",
    "        pdf_reader = pypdf.PdfReader(pdf_file)\n",
    "        # Read the entire PDF content\n",
    "        pdf_text = \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "        \n",
    "        # Extract sections using the provided section patterns\n",
    "        content = get_section(pdf_text, section_patterns)\n",
    "        if content: \n",
    "            return content \n",
    "        pdf_file.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Try to open the PDF from the alternative directory\n",
    "            alternative_pdf_path = os.path.join(alt_pdf_path, os.path.basename(pdf_path))\n",
    "            print(alternative_pdf_path)\n",
    "            pdf_file = open(alternative_pdf_path, 'rb')\n",
    "            return 'Editorial board'\n",
    "        except FileNotFoundError:\n",
    "            # If PDF is not found in the original or alternative directory, return 'Editorial board'\n",
    "            return 'Editorial board'\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "\n",
    "\n",
    "def get_section(article, section_patterns):\n",
    "    \"\"\"Get sections from a research paper based on patterns.\n",
    "    This function is losely interpreted from Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022) with some alterations.\n",
    "    specifically PublicDatasets/ArticleAnalyser.ipynb, section '2.1 Get section function'\n",
    "    \n",
    "    Parameters: \n",
    "    :param article (str): Text contents of the research paper.\n",
    "    :param section_patterns (list of lists): A list of lists where each inner list represents the start and end patterns.\n",
    "    \n",
    "    Returns: \n",
    "    :return: The extracted section text.\n",
    "    \"\"\"\n",
    "    article_lower = article.lower()  # Convert contents to lowercase\n",
    "\n",
    "    # Attempt to find the section based on the current patterns (case-insensitive)\n",
    "    for start_patterns, end_patterns in section_patterns:\n",
    "        for start_pattern in start_patterns:\n",
    "            start_pattern = re.compile(re.escape(start_pattern), re.IGNORECASE)\n",
    "            match_start = start_pattern.search(article_lower)\n",
    "            if match_start:\n",
    "                idx0 = match_start.start()\n",
    "                for end_pattern in end_patterns:\n",
    "                    end_pattern = re.compile(re.escape(end_pattern), re.IGNORECASE)\n",
    "                    match_end = end_pattern.search(article_lower[idx0:])\n",
    "                    if match_end:\n",
    "                        end_idx = idx0 + match_end.end()\n",
    "                        section = article[idx0:end_idx]  # Extract the matched section\n",
    "                        return section\n",
    "\n",
    "    # If no match is found, return an empty string\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3593a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing PDFs\n",
    "pdf_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_articles_doi/'\n",
    "alternative_pdf_directory = '../Data/ElsevierAPI/downloaded_pdfs/fulltext_editorialboard_doi/'\n",
    "\n",
    "# Path to the JSON file containing DOI values\n",
    "json_file_path = '../Data/ElsevierAPI/downloadedPDFs_info.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4c760",
   "metadata": {},
   "source": [
    "<a name='sectionpatternsv1'></a>\n",
    "### 1.1.1. Section patterns v1 \n",
    "Before I continue working on extracting the dataset names and potential links from the sections, I am curious to see how the section pattern performs. \n",
    "\n",
    "I investigate the first ten DOIs in downloadedPDFs_info.json() to see exactly what text sections were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055f59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns = [\n",
    "    ([\"Data and Code Availability\", \"Data Availability\"], [\"3\", \"CRediT authorship contribution statement\", \"Acknowledgements\", \"References\"]),\n",
    "    ([\"2.1\"], [\"2.2\"]),\n",
    "    ([\"Resource\", \"3.1 'Resource'\"], [\"3.2\"]),\n",
    "    ([\"Fig.\\d+\", \"Fig.\\d+\\.?\", \"Figure \\d+\"], [\"https?://[^\\s]+\"]),\n",
    "    ([\"Tab.\\d+\", \"Table \\d+\\.?\"], [\"https?://[^\\s]+\", \"[\\w\\s-]+\\d{4}\"]),\n",
    "    ([\"Introduction\", \"1\"], [\"2\"]),\n",
    "    ([\"Abstract\"], [\"1\", \"Introduction\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc263ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store individual results\n",
    "results_list = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    # Get the first 10 DOIs\n",
    "    first_10_dois = doi_data['DOIs'][:10]\n",
    "\n",
    "    for doi in first_10_dois:\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        section_content = get_content(pdf_path, alternative_pdf_directory, section_patterns)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results_list.append({\"DOI\": doi, \"Section\": section_content})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052a4712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119451</td>\n",
       "      <td>Data and code availability  statements  \\nSpec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119632</td>\n",
       "      <td>Data and code availability  \\nThe data incorpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119584</td>\n",
       "      <td>Data Availability  \\nData will be made availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119550</td>\n",
       "      <td>Data and code availability  \\nAll data used in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119710</td>\n",
       "      <td>Data and code availability  statement  \\nAll i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119338</td>\n",
       "      <td>Data and code availability  \\nNo data were acq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1016/j.neuroimage.2022.118986</td>\n",
       "      <td>Data and Code Availability  Statement  \\nAll c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119192</td>\n",
       "      <td>Data and code availability  statement  \\nData ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119177</td>\n",
       "      <td>Data and code availability  statement  \\nThe E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119110</td>\n",
       "      <td>Data and code availability  statements  : The ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                DOI  \\\n",
       "0  10.1016/j.neuroimage.2022.119451   \n",
       "1  10.1016/j.neuroimage.2022.119632   \n",
       "2  10.1016/j.neuroimage.2022.119584   \n",
       "3  10.1016/j.neuroimage.2022.119550   \n",
       "4  10.1016/j.neuroimage.2022.119710   \n",
       "5  10.1016/j.neuroimage.2022.119338   \n",
       "6  10.1016/j.neuroimage.2022.118986   \n",
       "7  10.1016/j.neuroimage.2022.119192   \n",
       "8  10.1016/j.neuroimage.2022.119177   \n",
       "9  10.1016/j.neuroimage.2022.119110   \n",
       "\n",
       "                                             Section  \n",
       "0  Data and code availability  statements  \\nSpec...  \n",
       "1  Data and code availability  \\nThe data incorpo...  \n",
       "2  Data Availability  \\nData will be made availab...  \n",
       "3  Data and code availability  \\nAll data used in...  \n",
       "4  Data and code availability  statement  \\nAll i...  \n",
       "5  Data and code availability  \\nNo data were acq...  \n",
       "6  Data and Code Availability  Statement  \\nAll c...  \n",
       "7  Data and code availability  statement  \\nData ...  \n",
       "8  Data and code availability  statement  \\nThe E...  \n",
       "9  Data and code availability  statements  : The ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2943c03",
   "metadata": {},
   "source": [
    "*In the following description, I refer to the index of the articles in results_df.*\n",
    "\n",
    "Observations from the text sections extracted with section_patterns: \n",
    "* In 1, 2, 4, 7, and 9, **the text is cut short because there's a mention of a number 3** within the section (in a link, in a release number, etc.). \n",
    "* In 2, they call it: 'Data/code availability statement'\n",
    "* In 2 and 9, the **end of the section can be 'Acknowledgements'**.\n",
    "* In 3 and 6, the **end of the section can be 'Declaration of Competing Interest'**.\n",
    "* In 4, 5, and 8, the **section ends with 'Credit authorship contribution statement'**.\n",
    "* In 5, we see that the use of a **URL does not necessarily mean that it's pointing to data (in this case, it's code and software)**. \n",
    "* In 6, we see that **the formulation of the text is important** (as the github link both contains data and code, but that is tricky to see). \n",
    "* In 7 and 8, they **mention which dataset they used, but do not link it**. \n",
    "* In 9, it says: \"The review summarizes data but does not contain new data.\" (this is important if I want to look into and further filter the documents for significance testing). \n",
    "\n",
    "<br>\n",
    "From this investigation I can see that I need to edit the section patterns. Ideas: \n",
    "\n",
    "- Maybe the end of a section can be \\n\\n? \n",
    "- Section end '3' should be called '3. ' - maybe this will fix some \n",
    "- Add variations: \n",
    "    - Section starts: \n",
    "        - Data/code availability statement\n",
    "    - Section ends: \n",
    "        - [data and code] Declaration of Competing Interest\n",
    "        - [data and code] Acknowledgements\n",
    "        - [data and code] Credit authorship contribution statement\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "FOR FUTURE STEPS: \n",
    "- URLs do not necessarily link to the data. \n",
    "- A git repository can contain both data and code - but not always. \n",
    "- The dataset might only be mentioned by name and not linked (so far, I've only seen the names in camelcase). \n",
    "- QUESTION: How do we treat reviews that summarizes data but does not contain new data? Is the reuse of a dataset not also the same as not containing new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92ce80",
   "metadata": {},
   "source": [
    "<a name='sectionpatternsv2'></a>\n",
    "### 1.1.2. Section patterns v2 \n",
    "Based on my exploration on the performance of the first section patterns, I can see that they need to be rewritten. For version 2, I made a few edits: \n",
    "* Add variations\n",
    "    * Section starts: \n",
    "        * Data/code availability statement \n",
    "    * Section ends: \n",
    "        * '\\n\\n' (this could be a general way to end the section) \n",
    "        * [data and code] Declaration of Competing Interest\n",
    "        * [data and code] Acknowledgements\n",
    "        * [data and code] Credit authorship contribution statement\n",
    "* Change pattern containing numbers (e.g., '3' is now '3. ')\n",
    "<br>\n",
    "I investigate the next ten DOIs in downloadedPDFs_info.json() to see exactly what text sections were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a805110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns_v2 = [\n",
    "    ([\"Data and Code Availability\", \"Data Availability\", \"Data/code availability\"], [\"3. \", \"CRediT authorship contribution statement\", \"Acknowledgements\", \"References\", \"Declaration of Competing Interests\", \"Credit authorship contribution statement\", \"\\n\\n\"]),\n",
    "    ([\"2.1.\"], [\"2.2.\"]),\n",
    "    ([\"Resource\", \"3.1.\"], [\"3.2.\"]),\n",
    "    ([\"Fig.\\d+\", \"Fig.\\d+\\.?\", \"Figure \\d+\"], [\"https?://[^\\s]+\"]),\n",
    "    ([\"Tab.\\d+\", \"Table \\d+\\.?\"], [\"https?://[^\\s]+\", \"[\\w\\s-]+\\d{4}\"]),\n",
    "    ([\"Introduction\", \"1. \"], [\"2. \"]),\n",
    "    ([\"Abstract\"], [\"1. \", \"Introduction\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "878c40a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/ElsevierAPI/downloaded_pdfs/fulltext_editorialboard_doi/10.1016.S1053-8119(22)00043-X.pdf\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store individual results\n",
    "results_list_v2 = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    # Get the first 10 DOIs\n",
    "    first_10_dois = doi_data['DOIs'][11:21]\n",
    "\n",
    "    for doi in first_10_dois:\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        section_content = get_content(pdf_path, alternative_pdf_directory, section_patterns_v2)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results_list_v2.append({\"DOI\": doi, \"Section\": section_content})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df2 = pd.DataFrame(results_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8340b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.neuroimage.2022.118931</td>\n",
       "      <td>Data availability  \\nThe Matlab code for the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119447</td>\n",
       "      <td>2.1. Participants  \\nThirty-ﬁve  people (20 fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119403</td>\n",
       "      <td>Data and code availability  statement  \\nData ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.neuroimage.2021.118831</td>\n",
       "      <td>Data and code availability  statements  \\nThe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119308</td>\n",
       "      <td>Data and code availability  statement  \\nFinni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1016/S1053-8119(22)00043-X</td>\n",
       "      <td>Editorial board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1016/j.neuroimage.2021.118792</td>\n",
       "      <td>data availability  \\nThe Shen 268 atlas is ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1016/j.neuroimage.2022.118890</td>\n",
       "      <td>data availability  \\nThe code used to run the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119339</td>\n",
       "      <td>Data and code availability  statement  \\nThe h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119295</td>\n",
       "      <td>Introduction  \\nReal-time  functional  magneti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                DOI  \\\n",
       "0  10.1016/j.neuroimage.2022.118931   \n",
       "1  10.1016/j.neuroimage.2022.119447   \n",
       "2  10.1016/j.neuroimage.2022.119403   \n",
       "3  10.1016/j.neuroimage.2021.118831   \n",
       "4  10.1016/j.neuroimage.2022.119308   \n",
       "5     10.1016/S1053-8119(22)00043-X   \n",
       "6  10.1016/j.neuroimage.2021.118792   \n",
       "7  10.1016/j.neuroimage.2022.118890   \n",
       "8  10.1016/j.neuroimage.2022.119339   \n",
       "9  10.1016/j.neuroimage.2022.119295   \n",
       "\n",
       "                                             Section  \n",
       "0  Data availability  \\nThe Matlab code for the p...  \n",
       "1  2.1. Participants  \\nThirty-ﬁve  people (20 fe...  \n",
       "2  Data and code availability  statement  \\nData ...  \n",
       "3  Data and code availability  statements  \\nThe ...  \n",
       "4  Data and code availability  statement  \\nFinni...  \n",
       "5                                    Editorial board  \n",
       "6  data availability  \\nThe Shen 268 atlas is ava...  \n",
       "7  data availability  \\nThe code used to run the ...  \n",
       "8  Data and code availability  statement  \\nThe h...  \n",
       "9  Introduction  \\nReal-time  functional  magneti...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc69422",
   "metadata": {},
   "source": [
    "*In the following description, I refer to the index of the articles in results_df2.*\n",
    "\n",
    "Observations from the text sections extracted with section_patterns_v2: \n",
    "- In 0, there are links, but these are not to the dataset - they write \"The used data can be shared with other researchers upon reasonable request.\" \n",
    "- In 0 and 7, the next section is called 'Supplementary materials' - which means that my attempt at \\n\\n did not work.  \n",
    "- In 2, the only mention of data was picked up in section 2.1.\n",
    "- In 2, the 'Declaration  of Competing  Interest' was not picked up - it looks like it's because there are double spaces between the words. \n",
    "- In 3, the 'Credit authorship  contribution  statement' is not picked - double spaces?\n",
    "- In 6, the data section is called 'Code and data availability' - but it was picked up by 'data availability'. \n",
    "- In 6, there are multiple links mentioned - one for data (an atlas), one for the code, and one for the data. \n",
    "    - NB! When copying the URL for the data, it is broken up by the formatting: https://www.humanconnectome.org/study/hcp-young-adult/ document/1200-subjects-data-release - this is also the case for the atlas. \n",
    "- In 7 and 8, there are spaces in the URL. \n",
    "- In 8, the following section 'Declaration of Competing Interest' was not picked up. \n",
    "- In 9, the introduction was picked up: but it does not look like any data is analysed in this article. \n",
    "\n",
    "<br>\n",
    "From this investigation I can see that I need to edit the section patterns further. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "TO DO: \n",
    "\n",
    "- Section_patterns that do not work: \n",
    "    - Section_end: \\n\\n\n",
    "    - Section_end: 'Declaration  of Competing  Interest' + Section_end: 'Credit authorship  contribution  statement' + Section_end: 'Supplementary materials'\n",
    "        - double-spaces between words mess these up\n",
    "- Section_patterns I'm worried about: \n",
    "    - Section_start: 2.1. - what if it's '2.1'?\n",
    "    - Section_start: 'Code and data availability'\n",
    "- Undiscovered section_patterns: \n",
    "    - Section_end: 'Ethics statement'\n",
    "- Worries \n",
    "    - How to get the name of the dataset itself and the url\n",
    "        - The URL can be broken up by spaces (due to line changes in the pdf) - can I find a way to find out which is the entire URL? \n",
    "            - Is there any slashes in the text ahead? A parenthesis, dot, comma, or another symbol might end it URL. \n",
    "    - If someone uses e.g., HCP, do they use all of the data? Do I need to catch more text-sections to learn this (in relation to the discussion of significance testing - if they use different parts of the dataset, they are not testing on the same). \n",
    "        - \"Due to HCP and dHCP privacy policies, the preprocessed resting-state images of human adults and neonates (with their IDs) can only be shared upon request with qualified investigators who agree to the Restricted Data Use Terms of these two datasets.\" (from 10.1016/j.neuroimage.2022.119339)\n",
    "    - What if the article does not analyse any data? (e.g., 10.1016/j.neuroimage.2022.119295 presents a software package for the execution of RT-fMRI experiments. \n",
    "- FUNCTION GET_CONTENT: Make a comment about trying the \"Editorial board\" texts in the other file - just so I don't get en \"Error reading PDF:\" \n",
    "    - Make an addition to 'get_section' where the says 'Editorial board' instead of None for the section text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be6ece",
   "metadata": {},
   "source": [
    "### 1.1.3. Section patterns v3 \n",
    "\n",
    "I want to make a regex_pattern work, as it seems like a double space after \n",
    "\n",
    "TO DO: \n",
    "- Section_patterns that do not work: \n",
    "    - Section_end: \\n\\n\n",
    "    - Section_end: 'Declaration  of Competing  Interest' + Section_end: 'Credit authorship  contribution  statement' + Section_end: 'Supplementary materials'\n",
    "        - double-spaces between words messed these up. \n",
    "- Section_patterns I'm worried about: \n",
    "    - Section_start: '2.1.' - what if it's '2.1'?\n",
    "    - Section_start: 'Code and data availability'\n",
    "- Undiscovered section_patterns: \n",
    "    - Section_end: 'Ethics statement'\n",
    "- Make the titles case sensitive, and it seems like most only capitalize the first word (see investigation in ../Code/articles_groundtruth.ipynb under 'Ground truth/Investigation/Section titles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fe3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_regex(pdf_path, alt_pdf_path, section_patterns):\n",
    "    try:\n",
    "        pdf_file = open(pdf_path, 'rb')\n",
    "        pdf_reader = pypdf.PdfReader(pdf_file)\n",
    "        # Read the entire PDF content\n",
    "        pdf_text = \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "        \n",
    "        # Extract sections using the provided section patterns\n",
    "        content, matched_start_pattern, start_pattern, end_pattern = get_section_regex(pdf_text, section_patterns)\n",
    "        \n",
    "        if content:\n",
    "            return content, matched_start_pattern, start_pattern, end_pattern\n",
    "        else:\n",
    "            # Handle the case where no content is found\n",
    "            return content, matched_start_pattern, start_pattern, end_pattern\n",
    "        pdf_file.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Try to open the PDF from the alternative directory\n",
    "            alternative_pdf_path = os.path.join(alt_pdf_path, os.path.basename(pdf_path))\n",
    "            pdf_file = open(alternative_pdf_path, 'rb')\n",
    "            return 'Editorial board', '', '', ''\n",
    "        except FileNotFoundError:\n",
    "            # If PDF is not found in the original or alternative directory, return 'Editorial board'\n",
    "            return 'Editorial board', '', '', ''\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "\n",
    "\n",
    "def get_section_regex(article, section_patterns):\n",
    "    \"\"\"This function extracts text sections from articles based on provided regex patterns.\n",
    "\n",
    "    Parameters:\n",
    "    :param article (str): The text of the article.\n",
    "    :param section_patterns (list of tuple): A list of tuples containing start and end regex patterns.\n",
    "\n",
    "    Returns:\n",
    "    :returns tuple: A tuple containing the extracted section text, the matched start pattern, and the matched end pattern.\n",
    "               If no section is found, it returns ('', '', '').\n",
    "    \"\"\"\n",
    "    matched_pattern = None  # Variable to store the matched start pattern\n",
    "    start_match = None      # Variable to store the specific matched start pattern\n",
    "    end_match = None        # Variable to store the specific matched end pattern\n",
    "    \n",
    "    # Iterate through each pattern pair\n",
    "    for start_pattern, end_pattern in section_patterns:\n",
    "        # Find all matches of the start pattern in the article\n",
    "        start_matches = re.finditer(start_pattern, article)\n",
    "\n",
    "        # Iterate through each start match\n",
    "        for match in start_matches:\n",
    "            start_idx = match.start()  # Get the start position of the start match\n",
    "\n",
    "            # Search for the end pattern starting from the end position of the start match\n",
    "            end_match = re.search(end_pattern, article[start_idx:])\n",
    "            \n",
    "            if end_match:\n",
    "                end_idx = start_idx + end_match.start()  # Calculate the end position of the section\n",
    "                section_text = article[start_idx:end_idx].strip()  # Extract the section text\n",
    "\n",
    "                # Store the matched start and end patterns\n",
    "                matched_pattern = start_pattern\n",
    "                start_match = match\n",
    "                end_match = end_match\n",
    "\n",
    "                # Return the section text and matched patterns\n",
    "                return section_text, matched_pattern, start_match, end_match\n",
    "\n",
    "    # If no match is found, return an empty string and the last matched patterns\n",
    "    return '', '', '', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075aca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns_regex = [\n",
    "    (r'(?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availability |(?<![\\'\"]) \\s*?\\n?Data\\s+availability |(?<![\\'\"]) \\s*?\\n?Data/code\\s+availability', r'\\s*?\\n\\n |\\s*?\\n?3\\. | \\s*?\\n?CRediT\\s+authorship\\s+contribution\\s+statement(?:s)? | \\s*?\\n?Acknowledgement(?:s)? | \\s*?\\n?Acknowledgment(?:s)? | \\s*?\\n?Reference(?:s)? | \\s*?\\n?Declaration\\s+of\\s+Competing\\s+Interest(?:s)? | \\s*?\\n?Credit\\s+authorship\\s+contribution\\s+statement(?:s)? | \\s*?\\n?Funding | \\s*?\\n?Supplementary\\s+materials | \\s*?\\n?Ethic(?:s)? statement(?:s)?'),\n",
    "    (r'\\n?2\\.1\\.', r'\\n?2\\.2. | \\n\\n '),\n",
    "    (r'\\n?Resource | \\n?3\\.1\\.\\s*?\\n?', r'\\n?3\\.2.\\s*?| \\s*?\\n\\n '),\n",
    "    (r'\\n?Introduction\\s*?\\n? | \\s*?\\n?1\\.\\s*?\\n? ', r'\\s*?\\n?2\\.\\s*?\\n? | \\s*?\\n\\n '),\n",
    "    (r'\\n?Fig\\.\\d+ | \\n?Fig\\.\\d+\\.? | \\n?Figure \\d+', r'https?://[^\\s]+ | \\s*?\\n\\n '),\n",
    "    (r'\\n?Tab\\.\\d+ | \\n?Table \\d+\\.?', r'https?://[^\\s]+ | [\\w\\s-]+\\d{4} | \\s*?\\n\\n '),\n",
    "    (r'\\n?Abstract\\s*?\\n? | \\s*?\\n?1\\.\\s*?\\n? ', r'\\n?Introduction\\s*?\\n? | \\s*?\\n\\n ')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e7f81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store individual results\n",
    "results_list_regex = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    # Get the first X DOIs\n",
    "    first_dois = doi_data['DOIs'][11:21] # 0:11 to compare with results_df, 11:21 to compare with results_df2\n",
    "\n",
    "    for doi in first_dois:\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content function for each DOI\n",
    "        section_content_regex, matched_pattern, start_match, end_match = get_content_regex(pdf_path, alternative_pdf_directory, section_patterns_regex)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results_list_regex.append({\"DOI\": doi, \"Section\": section_content_regex, \"Matched_pattern\": matched_pattern, \"Start_pattern\": start_match, \"End_pattern\": end_match})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df_regex = pd.DataFrame(results_list_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac93db",
   "metadata": {},
   "source": [
    "results_df: 0-10, i.e., [0:11]\n",
    "\n",
    "results_df2: 11-20, i.e., [11:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d88753dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data availability  \\nThe Matlab code for the proposed  vein segmentation  algorithm  \\nis available  on github: https://github.com/SinaStraub/GRE  _ vessel _ \\nseg.git and example  data on Zenodo.org:  https://doi.org/10.  \\n5281/zenodo.5791233  \\nThe used data can be shared with other researchers  upon reasonable  \\nrequest.  \\nSupplementary  materials  \\nSupplementary  material  associated  with this article can be found, in \\nthe online version,  at doi:10.1016/j.neuroimage.2022.118931  . \\nAppendices  \\nAll equations  are to be understood  voxel-wise,  however,  spatial co- \\nordinates  are omitted  when possible.  \\nA. True susceptibility-weighted  images \\nIn contrast  to susceptibility-weighted  data, true susceptibility-  \\nweighted  images (tSWI) are generated  using susceptibility  masks 𝑊 in- \\nstead of phase masks ( Liu et al., 2014 ), \\n𝑡𝑆𝑊 𝐼 = 𝑚𝑎𝑔 ⋅𝑊 𝑛 , where 𝑊 = ⎧ \\n⎪ \\n⎨ \\n⎪ ⎩ 1 , 𝑓𝑜𝑟 𝜒≤ 𝜒1 , \\n1 − 𝜒− 𝜒1 \\n𝜒2 − 𝜒1 , 𝑓𝑜𝑟 𝜒1 < 𝜒≤ 𝜒2 , \\n0 , 𝑓𝑜𝑟 𝜒> χ2 , (A.1) \\nwhere [ 𝜒1 , 𝜒2 ] deﬁnes the range of susceptibility  values for which the \\ncontrast  will be improved.  tSWI has been shown to provide  a better \\nvisualization  of the venous vasculature,  especially  for data at isotropic  \\nresolution.  \\nB. Background  suppression  and suppression  of ﬁeld inhomogeneity  artifacts  \\nJin et al. proposed  an inverted  two-dimensional  Hamming  ﬁlter ap- \\nproach to enhance  venous vasculature  through  background  suppression  \\n( Jin et al., 2014 ). In the presented  study, a three-dimensional  ﬁlter with \\nﬁlter size 2H x , 2H y and 2H z is used. The inverted  Hamming  ﬁlter 𝑖𝐻\\n𝑖𝐻 (𝑛 𝑥 , 𝑛 𝑦 , 𝑛 𝑧 )\\n= ⎧ \\n⎪ \\n⎨ \\n⎪ ⎩ 0 . 6 [ \\n1 . 0 − cos ( \\n𝜋√ \\n𝑛 2 𝑥 \\n𝐻 2 𝑥 + 𝑛 2 𝑦 \\n𝐻 2 𝑦 + 𝑛 2 𝑧 \\n𝐻 2 𝑧 ) ] \\n, 𝑓𝑜𝑟 𝑛 2 \\n𝑥 \\n𝐻 2 𝑥 + 𝑛 2 \\n𝑦 \\n𝐻 2 𝑦 + 𝑛 2 \\n𝑧 \\n𝐻 2 𝑧 ≤ 1 , \\n1 . 0 , 𝑒𝑙𝑠𝑒. \\n(B.1) \\ncan be applied  by a simple point-wise  multiplication  in the Fourier  do- \\nmain. Moreover,  due to its sensitivity  to non-local  ﬁeld inhomogeneities,  \\ngradient  echo data, often inherit severe signal drop-outs  in magnitude  \\ndata and excess gradients  in phase images,  especially  bilaterally  in the \\ntemporo-basal  region and the medial fronto-basal  region. In vesselness  \\nfunctions,  this can lead to vessel-like  artifacts.  Therefore,  it has been \\nproposed  to regularize  the vesselness  function  to reduce these artifacts  \\n( Monti et al., 2017 ). According  to Monti et al., a regularization  func- \\ntion 𝑟𝑒𝑔with values between  0 and 1, which is close to zero where the \\nprobability  for vessel like-artifacts  is high, can be computed  from the \\ndiﬀerence  Δbetween  the unwrapped  phase image and the local phase \\nimage (see also Section  3.2 Data Processing)  as \\n𝑟𝑒𝑔 = ⎧ \\n⎪ \\n⎨ \\n⎪ ⎩ 𝑝 𝑚𝑎𝑥 − |̃Δ|\\n𝑝 𝑚𝑎𝑥 − 𝑝 𝑚𝑖𝑛 , 𝑝 𝑚𝑖𝑛 ≤ ||̃Δ||≤ 𝑝 𝑚𝑎𝑥 \\n1 , ||̃Δ||< 𝑝 𝑚𝑖𝑛 \\n0 , 𝑒𝑙𝑠𝑒 (B.2) \\n9  S. Straub, J. Stiegeler, E. El-Sanosy  et al. NeuroImage  250 (2022) 118931 \\nwhere ̃Δis the extension  of Δafter erosion  using the local mean values, \\n𝑝 𝑚𝑎𝑥 and 𝑝 𝑚𝑖𝑛 are the 99.7th and 88th percentiles  of |̃Δ|. \\nC. The shearlet  transform  and shearlet  denoising  \\nThe shearlet  transform  \\ue23f\\ue234( Kutyniok  et al., 2016 ) is a multiscale  \\ntransform  and a natural  extension  of the wavelet  transform.  It is based \\non parabolic  scaling,  shearing  and translation  applied  to a few gener- \\nating functions  and allows therefore  for an eﬃcient  representation  of \\nanisotropic  features  in images ( Guo and Labate,  2007 ) \\nMathematically,  the discrete  shearlet  transform  of a function  𝑓 ∈\\n𝐿 2 ( ℝ 3 ) associated  to the pyramid-adapted  shearlet  system \\n𝑆𝐻 (\\n𝜙, 𝜓, ̃𝜓 , ⌣ 𝜓 ; 𝑐 )\\n= Φ(𝜙; 𝑐 1 )∪Ψ( 𝜓; 𝑐 ) ∪̃Ψ( ̃𝜓 ; 𝑐 ) ∪⌣ 𝜓 (⌣ 𝜓 ; 𝑐 )\\n, \\nwith \\nΦ(𝜙, 𝑐 1 )= { 𝜙𝑚 = 𝜙( ⋅− 𝑚 ) ∶ 𝑚 ∈𝑐 1 ℤ 3 } , \\nΨ( 𝜓, 𝑐 ) = { \\n𝜓 𝑗,𝑘,𝑚 = 2 𝑗 𝜓 (𝑆 𝑘 𝐴 2 𝑗 ⋅− 𝑚 )∶ 𝑗 ≥ 0 , |𝑘 |≤ 2 𝑗 \\n2 , 𝑚 ∈𝑀 𝑐 ℤ 3 } \\n, \\ñΨ( ̃𝜓 , 𝑐 ) = { \\ñ𝜓 𝑗,𝑘,𝑚 = 2 𝑗 ̃𝜓 (̃𝑆 𝑘 ̃𝐴 2 𝑗 ⋅− 𝑚 )∶ 𝑗 ≥ 0 , |𝑘 |≤ 2 𝑗 \\n2 , 𝑚 ∈̃𝑀 𝑐 ℤ 3 } \\n, \\n⌣ \\nΨ( ⌣ \\nΨ, 𝑐 ) \\n= { ⌣ \\nΨ𝑗,𝑘,𝑚 = 2 𝑗 ⌣ \\nΨ( ⌣ \\n𝑆 𝑘 ⌣ \\n𝐴 2 𝑗 ⋅− 𝑚 ) \\n∶ 𝑗 ≥ 0 , |𝑘 |≤ 2 𝑗 \\n2 , 𝑚 ∈⌣ \\n𝑀 𝑐 ℤ 3 } \\n, \\n𝐴, ̃𝐴 , ⌣ \\n𝐴 parabolic  scaling matrices,  𝑆 𝑘 , ̃𝑆 𝑘 , ⌣ \\n𝑆 𝑘 shearing  matrices,  \\n𝜙, 𝜓, ̃𝜓 , ⌣ 𝜓 ∈𝐿 2 ( ℝ 3 ) the scaling function  and the shearlet  generators,  \\n𝑗 ∈ℕ 0 , 𝑐 = ( 𝑐 1 , 𝑐 2 ) ∈ℝ 2 \\n+ , 𝑘 ∈ℤ 2 is deﬁned  as \\n𝑆 𝐻 𝜙,𝜓, ̃𝜓 , ⌣ 𝜓 𝑓 ( \\n𝑚 ′, ( 𝑗, 𝑘, 𝑚 ) , (̃𝑗 , ̃𝑘 , ̃𝑚 ), ( ⌣ \\n𝑗 , ⌣ \\n𝑘 , ⌣ 𝑚 ) ) \\n= ( \\n⟨𝑓, 𝜙𝑚 ′⟩, ⟨𝑓, 𝜓 𝑗,𝑘,𝑚 ⟩, ⟨𝑓, ̃𝜓 ̃𝑗 , ̃𝑘 , ̃𝑚 ⟩, ⟨ \\n𝑓, ⌣ 𝜓 ⌣ \\n𝑗 , ⌣ \\n𝑘 , ⌣ 𝑚 ⟩ ) \\n, \\nwhere ( 𝑚 ′, ( 𝑗, 𝑘, 𝑚 ) , ( ̃𝑗 , ̃𝑘 , ̃𝑚 ) , ( ⌣ \\n𝑗 , ⌣ \\n𝑘 , ⌣ 𝑚 ) ) ∈ℤ 3 ×Λ×Λ×Λ , and Λ= ℕ 0 ×\\n{ − ⌈2 𝑗( 𝛼𝑗 −1 ) \\n2 ⌉, …, ⌈2 𝑗( 𝛼𝑗 −1 ) \\n2 ⌉} 2 ×ℤ 3 . \\nUsing the shearlet  transform,  a denoised  image 𝐼 𝑑𝑒𝑛𝑜𝑖𝑠𝑒𝑑 can be cal- \\nculated  ( Kutyniok  et al., 2016 ) \\n𝐼 𝑑𝑒𝑛𝑜𝑖𝑠𝑒𝑑 = \\ue23f \\ue234 −1 𝑇 𝛿\\ue23f\\ue234 𝐼 𝑛𝑜𝑖𝑠𝑦, \\nusing a hard thresholding  operator  𝑇 𝛿𝐼 = { 𝐼, 𝑓𝑜𝑟 𝐼 ≥ 𝛿𝑗 = 𝐾 𝑗 𝜎\\n0 , 𝑒𝑙𝑠𝑒. (assum-  \\ning 𝐼 𝑛𝑜𝑖𝑠𝑦 = 𝐼 + 𝑛 , and 𝑛 ∼\\ue23a ( 0 , 𝜎2 ) , and 𝐾 𝑗 a thresholding  factor for each \\nscale). \\nIn this study, the implementation  of the digital shearlet  transform  in \\nShearLab3D  ( Kutyniok  et al., 2016 ) was used. The decomposition  and \\nreconstruction  of the data can be calculated  at once or in a serial manner  \\nto avoid high RAM usage. Then, only the coeﬃcients  of the translations  \\nof one single shearlet  (with ﬁxed scale and shear level) are stored at one \\npoint in time ( Kutyniok  et al., 2016 ). \\nD. Vesselness  ﬁlters \\nFor the purpose  of vessel segmentation  algorithms,  multiscale  vessel \\nenhancement  ﬁlters are a commonly  used preprocessing  technique.  A \\npopular  example  is Frangi’s  ﬁlter ( Frangi et al., 1998 ) which computes  \\nthe likelihood  of a voxel to belong to a vessel. A scale space represen-  \\ntation from the imaging  data 𝐼, on which vessels are segmented  is gen- \\nerated by ﬁltering  𝐼with Gaussian  kernels  with diﬀerent  standard  de- \\nviations  𝜎according  to diﬀerent  vessel diameters.  Inspired  by Frangi’s  \\nvesselness  ﬁlter, enhancement  ﬁlters have been proposed  such as the \\nvesselness  fractional  anisotropy  tensor ( Alhasson  et al., 2018 ; Cui et al., \\n2019 ; Jerman  et al., 2015 ; Prados et al., 2010 ) that yields a close-to-  \\nuniform  response  in all vascular  structures  and enhances  the border be- \\ntween vascular  structures  and the background.  D.1. . Frangi’s  vesselness  ﬁlter \\nThe eigenvalues  of the Hessian  matrix 𝐻 𝜎of the Gaussian  scale space \\nrepresentations  are calculated,  and ordered  by the size of their absolute  \\nvalue |𝜆𝜎, 1 |≤ |𝜆𝜎, 2 |≤ |𝜆𝜎, 3 |. If vessels are represented  as dark struc- \\ntures in 𝐼, Frangi’s  vesselsness  is calculated  as: \\n\\ue242 𝐹 \\n𝜎( 𝐼 ) = { 0 , 𝑓𝑜𝑟 𝜆𝜎, 2 < 0 ∨𝜆𝜎, 3 < 0 , ( \\n1 − 𝑒 − 𝐴 2 𝜎\\n2 𝛼2 ) \\n𝑒 − 𝐵 2 𝜎\\n2 𝛽2 ⎛ \\n⎜ \\n⎜ ⎝ 1 − 𝑒 − 𝑆 2 𝜎\\n2 𝛾2 𝜎⎞ \\n⎟ \\n⎟ ⎠ , 𝑒𝑙𝑠𝑒, \\nand otherwise:  \\n\\ue242 𝐹 \\n𝜎( 𝐼 ) = { 0 , 𝑓𝑜𝑟 𝜆𝜎, 2 > 0 ∨𝜆𝜎, 3 > 0 , ( \\n1 − 𝑒 − 𝐴 2 𝜎\\n2 𝛼2 ) \\n𝑒 − 𝐵 2 𝜎\\n2 𝛽2 ⎛ \\n⎜ \\n⎜ ⎝ 1 − 𝑒 − 𝑆 2 𝜎\\n2 𝛾2 𝜎⎞ \\n⎟ \\n⎟ ⎠ , 𝑒𝑙𝑠𝑒, \\nwhere 𝐴 𝜎= |𝜆𝜎, 2 |\\n|𝜆𝜎, 3 |, 𝐵 𝜎= |𝜆𝜎, 1 |√𝜆𝜎, 2 𝜆𝜎, 3 , and 𝑆 𝜎= √ \\n𝜆2 \\n𝜎, 1 + 𝜆2 \\n𝜎, 2 + 𝜆2 \\n𝜎, 3 and \\n𝛼, 𝛽, 𝛾tuning parameters.  The ﬁnal vesselness  function  is computed  \\nas the maximum  across the scales 𝜎. \\nD.2. Vesselness  fractional  anisotropy  tensor \\nIf vessels are represented  as dark structures  in 𝐼, it is deﬁned  as \\nfollows:  \\n\\ue242 𝐹𝐴𝑇 \\n𝜎,𝜆( 𝐼 ) = √ \\n3 \\n2 √ √ √ √ √ (𝜆𝜎, 2 − 𝐷 𝜎,𝜆)2 + (𝜆𝜎,𝑝 − 𝐷 𝜎, 𝜆)2 + (𝜆𝜎,𝜈− 𝐷 𝜎, 𝜆)2 \\n𝜆2 \\n𝜎, 2 + 𝜆2 \\n𝜎,𝑝 + 𝜆2 \\n𝜎,𝜈\\nwhere 𝐷 𝜎, 𝜆= ∑3 \\n𝑖 =1 𝜆𝜎, 𝑖 \\n3 , 𝜆𝜎,𝜌,𝜈= ⎧ \\n⎪ \\n⎪ \\n⎪ \\n⎨ \\n⎪ \\n⎪ \\n⎪ ⎩ 𝜆𝜎, 3 , 𝑓𝑜𝑟 𝜆𝜎, 3 > 𝜏𝜎,𝑝,𝜈max \\n𝐼 𝜆𝜎, 3 , \\n𝜏𝜎,𝑝,𝜈max \\n𝐼 𝜆𝜎, 3 , 𝑓𝑜𝑟 0 < 𝜆𝜎, 3 \\n≤ 𝜏𝜎,𝑝,𝜈max \\n𝐼 𝜆𝜎, 3 , \\n0 , 𝑒𝑙𝑠𝑒 and \\n𝜏𝜎,𝑝 , 𝜏𝜎,𝜈∈[ 0 , 1 ] . \\nThe ﬁnal enhancement  function  is computed  as \\n\\ue242 𝜎, 𝜆,𝑝 𝑀𝐹𝐴𝑇 ( 𝐼 ) = \\ue242 𝜎−1 , 𝜆,𝑝 𝑀𝐹𝐴𝑇 ( 𝐼 ) + 𝛿tanh (𝑅 𝜎,𝜆, 𝑝 ( 𝐼 ) − 𝛿)\\n\\ue242 𝜆,𝑝 𝑀𝐹𝐴𝑇 ( 𝐼 ) = max 𝜎(\\ue242 𝜎,𝜆,𝑝 𝑀𝐹𝐴𝑇 ( 𝐼 ) , 𝑅 𝜎,𝜆, 𝑝 ( 𝐼 ) ) (D.2.1) \\nwhere \\n𝑅 𝜎,𝜆,𝑝 ( 𝐼 ) = ⎧ \\n⎪ \\n⎨ \\n⎪ ⎩ 0 , 𝑓𝑜𝑟 𝜆𝜎,𝑝 > 𝜆𝜎,𝑝 − 𝜆𝜎, 2 ∨𝜆𝜎,𝑝 ≥ 0 ∨𝜆𝜎, 2 ≥ 0 , \\n1 , 𝑓𝑜𝑟 𝜆𝜎,𝑝 − 𝜆𝜎, 2 = max \\n𝐼 (𝜆𝜎,𝑝 − 𝜆𝜎, 2 ), \\n1 − \\ue242 𝜎, 𝜆𝐹𝐴𝑇 , 𝑒𝑙𝑠𝑒, \\nand 𝛿is the step size for the calculation  of the solution.  The output is \\na probability  map, i.e. each voxel has values between  0 and 1 which is \\nthe probability  for being a vein voxel. \\nReferences  \\nAlhasson,  H.F. , Alharbi, S.S. , Obara, B. , 2018. 2D and 3D Vascular Structures  Enhancement  \\nVia Multiscale  Fractional  Anisotropy  tensor. Springer,  Munich, Germany  European  \\nConference  On Computer  Vision . \\nAshburner,  J. , Friston, K.J. , 2005. Uniﬁed segmentation.  Neuroimage  26, 839–851 . \\nBazin, P. , Plessis, V. , Fan, A.P. , Villringer,  A. , Gauthier,  C.J. , 2016. Vessel segmenta-  \\ntion from quantitative  susceptibility  maps for local oxygenation  venography.  In: Pro- \\nceedings of the IEEE 13th International  Symposium  on Biomedical  Imaging (ISBI), \\npp. 1135–1138  . \\nBeriault, S. , Xiao, Y.M. , Collins, D.L. , Pike, G.B. , 2015. Automatic  SWI venography  seg- \\nmentation  using conditional  random ﬁelds. IEEE Trans. Med. Imaging 34, 2478–2491  . \\nCui, H.F. , Xia, Y. , Zhang, Y.N. , 2019. 2D and 3D vascular structures  enhancement  \\nvia improved  vesselness  ﬁlter and vessel enhancing  diﬀusion.  IEEE Access 7, \\n123969–123980  . \\nDeistung,  A. , Dittrich, E. , Sedlacik,  J. , Rauscher,  A. , Reichenbach,  J.R. , 2009. ToF-SWI:  \\nsimultaneous  time of ﬂight and fully ﬂow compensated  susceptibility  weighted  imag- \\ning. J. Magn. Reson. Imaging 29, 1478–1484  . \\nDerdeyn,  C.P. , Videen, T.O. , Grubb, R.L. , Powers, W.J. , 2001. Comparison  of PET oxy- \\ngen extraction  fraction methods for the prediction  of stroke risk. J. Nucl. Med. 42, \\n1195–1197  . \\n10  S. Straub, J. Stiegeler, E. El-Sanosy  et al. NeuroImage  250 (2022) 118931 \\nDubuisson,  M. , Jain, A.K. , 1994. A modiﬁed  Hausdorﬀdistance  for object matching.  In: \\nProceedings  of 12th International  Conference  on Pattern Recognition,  1, Jerusalem,  \\nIsrael, pp. 566–568 . \\nDuyn, J.H. , Schenck, J. , 2017. Contributions  to magnetic  susceptibility  of brain tissue. \\nNMR Biomed. 30 . \\nEckstein,  K. , Bachrata,  B. , Hangel, G. , Widhalm,  G. , Enzinger,  C. , Barth, M. , Trattnig, S. , \\nRobinson,  S.D. , 2021. Improved  susceptibility  weighted  imaging at ultra-high  ﬁeld \\nusing bipolar multi-echo  acquisition  and optimized  image processing:  CLEAR-SWI.  \\nNeuroimage  237 . \\nEckstein,  K. , Dymerska,  B. , Bachrata,  B. , Bogner, W. , Poljanc, K. , Trattnig, S. , Robin- \\nson, S.D. , 2018. Computationally  eﬃcient combination  of multi-channel  phase data \\nfrom multi-echo  acquisitions  (ASPIRE).  Magn. Reson. Med. 79, 2996–3006  . \\nFischl, B. , 2012. FreeSurfer.  Neuroimage  62, 774–781 . \\nFrangi, A.F. , Niessen, W.J. , Vincken, K.L. , Viergever,  M.A. , 1998. Multiscale  vessel en- \\nhancement  ﬁltering. In: Proceedings  of the Medical Image Computing  and Comput- \\ner-Assisted  Intervention  - MICCAI’98,  1496, pp. 130–137 . \\nFu, W., Breininger,  K., Würﬂ, T., Ravikumar,  N., Schaﬀert,  R., Maier, A.K.J.A., 2017. \\nFrangi-net:  a neural network approach  to vessel segmentation.  abs/1711.03345.  \\nGe, Y.L. , Zohrabian,  V.M. , Osa, E.O. , Xu, J. , Jaggi, H. , Herbert, J. , Haacke, E.M. , Gross- \\nman, R.I. , 2009. Diminished  visibility of cerebral venous vasculature  in multiple scle- \\nrosis by susceptibility-weighted  imaging at 3.0 Tesla. J. Magn. Reson. Imaging 29, \\n1190–1194  . \\nGuo, K. , Labate, D. , 2007. Optimally  sparse multidimensional  representation  using shear- \\nlets. SIAM J. Math. Anal. 39, 298–318 . \\nGuo, Y.H. , Budak, U. , Sengur, A. , Smarandache,  F. , 2017. A retinal vessel detection  ap- \\nproach based on shearlet transform  and indeterminacy  ﬁltering on fundus images. \\nSymmetry  9 Basel . \\nGupta, A. , Baradaran,  H. , Schweitzer,  A.D. , Kamel, H. , Pandya, A. , Delgado, D. , Wright, D. , \\nHurtado-Rua,  S. , Wang, Y. , Sanelli, P.C. , 2014. Oxygen extraction  fraction and stroke \\nrisk in patients with carotid stenosis or occlusion:  a systematic  review and meta-anal-  \\nysis. AJNR Am. J. Neuroradiol.  35, 250–255 . \\nHaacke, E.M. , Liu, S. , Buch, S. , Zheng, W. , Wu, D. , Ye, Y. , 2015. Quantitative  susceptibility  \\nmapping:  current status and future directions.  Magn. Reson. Imaging 33, 1–25 . \\nHaacke, E.M. , Tang, J. , Neelavalli,  J. , Cheng, Y.C.N. , 2010. Susceptibility  mapping as a \\nmeans to visualize veins and quantify oxygen saturation.  J. Magn. Reson. Imaging 32, \\n663–676 . \\nHuntenburg,  J.M. , Steele, C.J. , Bazin, P.L. , 2018. Nighres: processing  tools for high-reso-  \\nlution neuroimaging.  Gigascience  7 . \\nHyder, F. , Rothman,  D.L. , Bennett, M.R. , 2013. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df2['Section'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "182516bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data availability  \\nThe Matlab code for the proposed  vein segmentation  algorithm  \\nis available  on github: https://github.com/SinaStraub/GRE  _ vessel _ \\nseg.git and example  data on Zenodo.org:  https://doi.org/10.  \\n5281/zenodo.5791233  \\nThe used data can be shared with other researchers  upon reasonable  \\nrequest.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_regex['Section'].loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05023e",
   "metadata": {},
   "source": [
    "**Fixed issues**: \n",
    "- Edited get_content_regex function to be case sensitive instead of insensitive \n",
    "    - When searching using all lowercase, results_df2['Section'].loc[2], this is cut short\n",
    "        - From 'Data and code availability  statement  \\nData used in the study are available  upon direct request.  Conditions  \\nfor its sharing  involve  the formalisation  of a research  agreement.  The \\ndata and code sharing  adopted  by the authors  comply  with the require-  \\nments of the funding  body or institute,  and with the institutional  ethics \\napproval.  Parts of the data are conﬁdential  and additional  ethical ap- \\nproval may be needed  for re-use. \\n'\n",
    "        - To: 'Data and code availability  statement  \\nData used in the study are available  upon direct request.  Conditions  \\nfor its sharing  involve  the formalisation  of a research  agreement.  The \\ndata and code sharing  adopted  by the authors  comply  with the require-  \\nments of the'\n",
    "        \n",
    "**Persisting issues**: \n",
    "- Reading the PDF \n",
    "    - By page-shift, the header is picked up (results_df_regex['Section'].loc[6], DOI  10.1016/j.neuroimage.2022.118986)\n",
    "    - Double (or more) spaces\n",
    "    - \\n characters \n",
    "- Section titles \n",
    "    - There are variations of section_start titles that I have not included in my pattern, e.g., \"Data Availability\", which I discovered in articles_groundtruth\n",
    "    - There are infinitely many undiscovered section_end titles, that I have not included in my pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09c69b",
   "metadata": {},
   "source": [
    "NB! THIS TAKES MORE THAN AN HOUR TO RUN!\n",
    "started at 16.09 - saw it was done at 18.15 - but checked at 17:40+, where it hadn't finished "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f030f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store individual results\n",
    "results = []\n",
    "\n",
    "# Read DOI values from the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    doi_data = json.load(json_file)\n",
    "\n",
    "    for doi in doi_data['DOIs']:\n",
    "        doi_replaced = doi.replace('/', '.')\n",
    "        pdf_path = os.path.join(pdf_directory, f\"{doi_replaced}.pdf\")\n",
    "\n",
    "        # Call the get_content_regex function for each DOI \n",
    "        section_content_regex, matched_pattern, start_match, end_match = get_content_regex(pdf_path, alternative_pdf_directory, section_patterns_regex)\n",
    "\n",
    "        # Create a dictionary for each result and add it to the list\n",
    "        results.append({\"DOI\": doi, \"Section\": section_content_regex, \"Matched_pattern\": matched_pattern, \"Start_pattern\": start_match, \"End_pattern\": end_match})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "articles_dataset_sections = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd72858",
   "metadata": {},
   "source": [
    "NB! The code above takes between one and two hours to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91be971d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Section</th>\n",
       "      <th>Matched_pattern</th>\n",
       "      <th>Start_pattern</th>\n",
       "      <th>End_pattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119451</td>\n",
       "      <td>Data and code availability  statements  \\nSpec...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(25302, 25330), match='...</td>\n",
       "      <td>&lt;re.Match object; span=(705, 709), match=' 3. '&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119632</td>\n",
       "      <td>Data and code availability  \\nThe data incorpo...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(39700, 39730), match='...</td>\n",
       "      <td>&lt;re.Match object; span=(689, 727), match=' \\nD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119584</td>\n",
       "      <td>Data/code  availability  statement  \\nData and...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(26185, 26209), match='...</td>\n",
       "      <td>&lt;re.Match object; span=(80, 86), match='  \\n3. '&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119550</td>\n",
       "      <td>Data and code availability  \\nAll data used in...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(104602, 104632), match...</td>\n",
       "      <td>&lt;re.Match object; span=(479, 518), match='  \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119710</td>\n",
       "      <td>Data and code availability  statement  \\nAll i...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(70621, 70650), match='...</td>\n",
       "      <td>&lt;re.Match object; span=(596, 641), match=' \\nC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>10.1016/j.neuroimage.2022.118922</td>\n",
       "      <td>Data and code availability  statement  \\nThe b...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(48329, 48358), match='...</td>\n",
       "      <td>&lt;re.Match object; span=(328, 373), match=' \\nC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119713</td>\n",
       "      <td>Data availability  \\nROI time series, along wi...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(58431, 58452), match='...</td>\n",
       "      <td>&lt;re.Match object; span=(468, 507), match='  \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119688</td>\n",
       "      <td>2.1. Participants  \\nTwelve  healthy  female p...</td>\n",
       "      <td>\\n?2\\.1\\.</td>\n",
       "      <td>&lt;re.Match object; span=(9729, 9734), match='\\n...</td>\n",
       "      <td>&lt;re.Match object; span=(721, 727), match='\\n2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>10.1016/j.neuroimage.2022.118939</td>\n",
       "      <td>Data and code availability  \\nDe-identiﬁed  da...</td>\n",
       "      <td>(?&lt;![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...</td>\n",
       "      <td>&lt;re.Match object; span=(50360, 50390), match='...</td>\n",
       "      <td>&lt;re.Match object; span=(125, 152), match=' \\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119149</td>\n",
       "      <td>2.1. Subjects  \\nThe data were 156 baseline  [...</td>\n",
       "      <td>\\n?2\\.1\\.</td>\n",
       "      <td>&lt;re.Match object; span=(8396, 8401), match='\\n...</td>\n",
       "      <td>&lt;re.Match object; span=(1345, 1351), match='\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>834 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  DOI  \\\n",
       "0    10.1016/j.neuroimage.2022.119451   \n",
       "1    10.1016/j.neuroimage.2022.119632   \n",
       "2    10.1016/j.neuroimage.2022.119584   \n",
       "3    10.1016/j.neuroimage.2022.119550   \n",
       "4    10.1016/j.neuroimage.2022.119710   \n",
       "..                                ...   \n",
       "829  10.1016/j.neuroimage.2022.118922   \n",
       "830  10.1016/j.neuroimage.2022.119713   \n",
       "831  10.1016/j.neuroimage.2022.119688   \n",
       "832  10.1016/j.neuroimage.2022.118939   \n",
       "833  10.1016/j.neuroimage.2022.119149   \n",
       "\n",
       "                                               Section  \\\n",
       "0    Data and code availability  statements  \\nSpec...   \n",
       "1    Data and code availability  \\nThe data incorpo...   \n",
       "2    Data/code  availability  statement  \\nData and...   \n",
       "3    Data and code availability  \\nAll data used in...   \n",
       "4    Data and code availability  statement  \\nAll i...   \n",
       "..                                                 ...   \n",
       "829  Data and code availability  statement  \\nThe b...   \n",
       "830  Data availability  \\nROI time series, along wi...   \n",
       "831  2.1. Participants  \\nTwelve  healthy  female p...   \n",
       "832  Data and code availability  \\nDe-identiﬁed  da...   \n",
       "833  2.1. Subjects  \\nThe data were 156 baseline  [...   \n",
       "\n",
       "                                       Matched_pattern  \\\n",
       "0    (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "1    (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "2    (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "3    (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "4    (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "..                                                 ...   \n",
       "829  (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "830  (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "831                                          \\n?2\\.1\\.   \n",
       "832  (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...   \n",
       "833                                          \\n?2\\.1\\.   \n",
       "\n",
       "                                         Start_pattern  \\\n",
       "0    <re.Match object; span=(25302, 25330), match='...   \n",
       "1    <re.Match object; span=(39700, 39730), match='...   \n",
       "2    <re.Match object; span=(26185, 26209), match='...   \n",
       "3    <re.Match object; span=(104602, 104632), match...   \n",
       "4    <re.Match object; span=(70621, 70650), match='...   \n",
       "..                                                 ...   \n",
       "829  <re.Match object; span=(48329, 48358), match='...   \n",
       "830  <re.Match object; span=(58431, 58452), match='...   \n",
       "831  <re.Match object; span=(9729, 9734), match='\\n...   \n",
       "832  <re.Match object; span=(50360, 50390), match='...   \n",
       "833  <re.Match object; span=(8396, 8401), match='\\n...   \n",
       "\n",
       "                                           End_pattern  \n",
       "0     <re.Match object; span=(705, 709), match=' 3. '>  \n",
       "1    <re.Match object; span=(689, 727), match=' \\nD...  \n",
       "2    <re.Match object; span=(80, 86), match='  \\n3. '>  \n",
       "3    <re.Match object; span=(479, 518), match='  \\n...  \n",
       "4    <re.Match object; span=(596, 641), match=' \\nC...  \n",
       "..                                                 ...  \n",
       "829  <re.Match object; span=(328, 373), match=' \\nC...  \n",
       "830  <re.Match object; span=(468, 507), match='  \\n...  \n",
       "831  <re.Match object; span=(721, 727), match='\\n2....  \n",
       "832  <re.Match object; span=(125, 152), match=' \\nS...  \n",
       "833  <re.Match object; span=(1345, 1351), match='\\n...  \n",
       "\n",
       "[834 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_dataset_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1039d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the 'Code-git/Data' directory\n",
    "data_dir = os.path.join(os.pardir, 'Data')\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(data_dir, 'articles_dataset_sections.csv')\n",
    "\n",
    "# Save the DataFrame to CSV, overwriting the file if it exists\n",
    "articles_dataset_sections.to_csv(file_path, index=False, mode='w')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964f11d",
   "metadata": {},
   "source": [
    "<a name='cleantextsections'></a>\n",
    "## 1.2. Clean text sections\n",
    "Before I continue to the extraction of the datasets from the text sections, I want to clean the current data a bit. This includes: \n",
    "- Clean the matching start patterns \n",
    "- Clean the extracted text sections, including \n",
    "    - Remove characters like '\\n' \n",
    "    - Remove double (or more) spaces \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a name='startpatterns'></a>\n",
    "### 1.2.1. Start patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "078c06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = os.path.join(os.pardir, 'Data/articles_dataset_sections.csv') \n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "articles_dataset_sections = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e800d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matched_text(text):\n",
    "    \"\"\"This function extracts matched text from a string containing a regular expression \n",
    "    match object and performs data cleaning.\n",
    "\n",
    "    Parameters:\n",
    "    :param text (str): A string containing a regular expression match object (e.g., \"<re.Match object; span=(start, end), match='text'>\").\n",
    "\n",
    "    Returns:\n",
    "    :returns: If a match is found in the input text, the function returns the matched text after performing the following operations:\n",
    "        Stripping leading and trailing spaces from the matched text.\n",
    "        Replacing '\\n' (newline) characters with empty strings.\n",
    "    :returns: If no match is found or the resulting matched text is empty, the function returns NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    match = re.search(r\"match='(.*?)'\", str(text))\n",
    "    if match:\n",
    "        matched_text = match.group(1).strip().replace('\\\\n', '').replace('  ', ' ').replace('   ', ' ')\n",
    "        if matched_text:\n",
    "            return matched_text\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e298bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to clean up the 'Start_pattern' column\n",
    "articles_dataset_sections['Start_pattern_clean'] = articles_dataset_sections['Start_pattern'].apply(extract_matched_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f9f3f",
   "metadata": {},
   "source": [
    "Overview of how many articles matches each of the section patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c8a00db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Matched_pattern  Count\n",
      "0  (?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availabi...    563\n",
      "1                                          \\n?2\\.1\\.    219\n",
      "2        \\n?Introduction\\s*?\\n? | \\s*?\\n?1\\.\\s*?\\n?      22\n",
      "3                     \\n?Resource | \\n?3\\.1\\.\\s*?\\n?      5\n",
      "4       \\n?Fig\\.\\d+ | \\n?Fig\\.\\d+\\.? | \\n?Figure \\d+      1\n",
      "5                      \\n?Tab\\.\\d+ | \\n?Table \\d+\\.?      1\n",
      "6                                                NaN     23\n",
      "Total Count: 834\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Matched_pattern' and count the number of rows in each group\n",
    "pattern_counts = articles_dataset_sections['Matched_pattern'].value_counts()\n",
    "\n",
    "# Count NaN values and add it to the pattern_counts Series\n",
    "nan_count = articles_dataset_sections['Matched_pattern'].isna().sum()\n",
    "pattern_counts['NaN'] = nan_count\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "articles_section_patterns = pd.DataFrame({\n",
    "    'Matched_pattern': pattern_counts.index,\n",
    "    'Count': pattern_counts.values\n",
    "})\n",
    "\n",
    "# Print the result DataFrame\n",
    "print(articles_section_patterns)\n",
    "\n",
    "# Calculate and print the total count\n",
    "total_count = articles_section_patterns['Count'].sum()\n",
    "print(\"Total Count:\", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c242a",
   "metadata": {},
   "source": [
    "I was only expecting to see 19 articles with NaN as a matched pattern (since there are 19 editorial board papers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "295e5dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter and display rows where 'Start_pattern_clean' is None\n",
    "no_pattern = articles_dataset_sections[articles_dataset_sections['Start_pattern_clean'].isna()]\n",
    "len(no_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2b9b5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Section</th>\n",
       "      <th>Matched_pattern</th>\n",
       "      <th>Start_pattern</th>\n",
       "      <th>End_pattern</th>\n",
       "      <th>Start_pattern_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>10.1016/j.neuroimage.2021.118776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>10.1016/j.neuroimage.2022.119154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>10.1016/j.neuroimage.2022.118921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  DOI Section Matched_pattern Start_pattern  \\\n",
       "58   10.1016/j.neuroimage.2022.119560     NaN             NaN           NaN   \n",
       "131  10.1016/j.neuroimage.2021.118776     NaN             NaN           NaN   \n",
       "517  10.1016/j.neuroimage.2022.119154     NaN             NaN           NaN   \n",
       "670  10.1016/j.neuroimage.2022.118921     NaN             NaN           NaN   \n",
       "\n",
       "    End_pattern Start_pattern_clean  \n",
       "58          NaN                 NaN  \n",
       "131         NaN                 NaN  \n",
       "517         NaN                 NaN  \n",
       "670         NaN                 NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where 'Section' is not 'Editorial board'\n",
    "no_pattern[no_pattern['Section'] != 'Editorial board']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e19617",
   "metadata": {},
   "source": [
    "There should only be 19 articles where there is no pattern-match, as there are 19 'Editorial Board' articles. The articles that were not filtered properly by my code are: \n",
    "- 10.1016/j.neuroimage.2022.119560\n",
    "    - This has a section called 'Data Availability'\n",
    "- 10.1016/j.neuroimage.2021.118776\n",
    "    - This article does not have any distinct sections. It presents all the articles in the particular volume of Neuroimaging. \n",
    "- 10.1016/j.neuroimage.2022.119154\n",
    "    - This article does not have any distinct sections. It is a commentary.     \n",
    "- 10.1016/j.neuroimage.2022.118921\n",
    "    - This article does not have any distinct sections. It is a corrigendum. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Of the four articles that did not contain one of my start patterns, only one should have been picked up. The rest seems to have been properly filtered. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 1.2.3. Clean text \n",
    "I will do a very simple initial cleaning of the extracted text sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e66acab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Data and code availability statements \\nSpeciﬁ...\n",
       "1      Data and code availability \\nThe data incorpor...\n",
       "2      Data/code availability statement \\nData and co...\n",
       "3      Data and code availability \\nAll data used in ...\n",
       "4      Data and code availability statement \\nAll ind...\n",
       "                             ...                        \n",
       "829    Data and code availability statement \\nThe bra...\n",
       "830    Data availability \\nROI time series, along wit...\n",
       "831    2.1. Participants \\nTwelve healthy female part...\n",
       "832    Data and code availability \\nDe-identiﬁed data...\n",
       "833    2.1. Subjects \\nThe data were 156 baseline [11...\n",
       "Name: Section, Length: 834, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_dataset_sections['Section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b1994f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(articles_dataset_sections['Section'])):\n",
    "    articles_dataset_sections['Section'].loc[i] = articles_dataset_sections['Section'].astype(str).loc[i].replace('   ', ' ').replace('  ', ' ').replace('\\n', '').replace('- ', '-').replace('( ', '(').replace('/ ', '/').replace(' )', ')').replace(' .', '.').replace(': /', ':/').replace(' _ ', '_').replace(' _', '_').replace('_ ', '_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac9ed8",
   "metadata": {},
   "source": [
    "<a name='getdatasets'></a>\n",
    "## 1.3. Get datasets\n",
    "I need to extract the datasets from the text sections we extracted above. \n",
    "\n",
    "Based on my previous observations, I will start the extraction with the following notions in mind: \n",
    "- Private datasets (meaning either fully private or available upon request) \n",
    "    - Markers include words such as \"request\", \"no data\", \"new data\", \"not be shared\" . E.g., \n",
    "        - \"Data and code are available upon request.\"\n",
    "        - \"Data and code availability statement All individual-level raw data used in this study cannot be shared because of the ethical code of Tokyo Metropolitan University. How-ever, the acquired metadata (e.g., group level activation maps) are available upon request. The corresponding author should be contacted by email for all data requests.\"\n",
    "        - \"No data were acquired for this study.\"\n",
    "        - \"The review summarizes data but does not contain new data.\"\n",
    "        - \"The data and code presented here are available upon request to the corresponding author.\"\n",
    "- Public datasets (meaning it's available to everyone with a link or title of the dataset)\n",
    "    - Markers include hyperlinks and camelcase \n",
    "        - Hyperlink \n",
    "        - Camelcase \n",
    "    - Word like \"code\", \"data\", or \"package\" is typically featured in the sentences with links, pointing to what the link refers to. \n",
    "    \n",
    "- Issues (**code**)\n",
    "    - The URL can be broken up by spaces due to line changes in the PDF. Do we stop at the parenthesis, comma or another symbol that might end the URL? \n",
    "        - EXAMPLES \n",
    "    - Not all links point to the dataset - some are to the code, e.g., \n",
    "        - \"Speciﬁcally, GES, PC and LiNGAM were implemented using the widely used R package pcalg , which is available at https://cran.r-project.org/web/packages/pcalg/. Notears method was implemented using Python available at https://github.com/xunzheng/notears . The proposed joint DAG method was implemented with Python and the code is available at https://github.com/gmeng92/joint-notears . The cohort data is accessible through the website (https://coins.trendscenter.org/) of COINS (COllaborative Infor-matics Neuroimaging Suite) database (Scott et al., 2011).\"\n",
    "        - \"Data and code availability The data incorporated in the primary analysis were gathered from the public UK Biobank resource and will be made pub-licly available together with the code used to generate the data through the UK Biobank Returns Catalogue (https://biobank.ndph. ox.ac.uk/showcase/docs.cgi?id = 1). ABCD study data release 3.0 is available for approved researchers in NIMH Data Archive (NDA DOI:10.151.54/1,519,007). Code for conducting discovery and replication is available at https: //github.com/robloughnan/MOSTest _ generalization . Code for simu-lations is available at https://github.com/precimed/mostest/tree/master/simu.\"    \n",
    "    \n",
    "- Issues (**analysis**)\n",
    "    - If someone uses e.g., HCP, do they use all of the data? Do I need to catch more text-sections to learn this (in relation to the discussion of significance testing - if they use different parts of the dataset, they are not testing on the same). \n",
    "        - \"Due to HCP and dHCP privacy policies, the preprocessed resting-state images of human adults and neonates (with their IDs) can only be shared upon request with qualified investigators who agree to the Restricted Data Use Terms of these two datasets.\" (from 10.1016/j.neuroimage.2022.119339)\n",
    "    - What if the article does not analyse any data? (e.g., 10.1016/j.neuroimage.2022.119295 presents a software package for the execution of RT-fMRI experiments. \n",
    "    - What if there are multiple sections and the text is slightly different (e.g., 10.1016/j.neuroimage.2022.118986)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "TO DO Columns: \n",
    "- (DONE) Section text \n",
    "- (DONE) Section pattern (multiple reasons: \n",
    "    - 1) I can get a sense of whether the data statement is common in NeuroImage, \n",
    "    2) I can go back and handle potential more difficult cases) \n",
    "- Extracted dataset \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 1.3.1. Pattern 1 \n",
    "I will start by examining and dealing with the text sections that were filtered by the first section_pattern, namely: \n",
    "\n",
    "    r'(?<![\\'\"]) \\s*?\\n?Data\\s+and\\s+code\\s+availability |(?<![\\'\"]) \\s*?\\n?Data\\s+availability |(?<![\\'\"]) \\s*?\\n?Data/code\\s+availability' \n",
    " \n",
    "The corresponding ending pattern: \n",
    "\n",
    "    r'\\s*?\\n\\n |\\s*?\\n?3\\. | \\s*?\\n?CRediT\\s+authorship\\s+contribution\\s+statement(?:s)? | \\s*?\\n?Acknowledgement(?:s)? | \\s*?\\n?Acknowledgment(?:s)? | \\s*?\\n?Reference(?:s)? | \\s*?\\n?Declaration\\s+of\\s+Competing\\s+Interest(?:s)? | \\s*?\\n?Credit\\s+authorship\\s+contribution\\s+statement(?:s)? | \\s*?\\n?Funding | \\s*?\\n?Supplementary\\s+materials | \\s*?\\n?Ethic(?:s)? statement(?:s)?'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f842e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?<![\\\\\\'\"]) \\\\s*?\\\\n?Data\\\\s+and\\\\s+code\\\\s+availability |(?<![\\\\\\'\"]) \\\\s*?\\\\n?Data\\\\s+availability |(?<![\\\\\\'\"]) \\\\s*?\\\\n?Data/code\\\\s+availability'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_dataset_sections['Matched_pattern'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f42cef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_1 = articles_dataset_sections[articles_dataset_sections['Matched_pattern'] == '(?<![\\\\\\'\"]) \\\\s*?\\\\n?Data\\\\s+and\\\\s+code\\\\s+availability |(?<![\\\\\\'\"]) \\\\s*?\\\\n?Data\\\\s+availability |(?<![\\\\\\'\"]) \\\\s*?\\\\n?Data/code\\\\s+availability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6bf8cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hg/yk6m9jqn41l9s2x1tfjqc9900000gn/T/ipykernel_16142/2212161826.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pat_1['contains_words'] = pat_1['Section'].str.contains(pattern, case=False, regex=True)\n"
     ]
    }
   ],
   "source": [
    "# List of words that seems to indicate that the authors used a private dataset \n",
    "words_to_check = ['data\\s+request', 'data\\s+re-quest' 'no data', 'no?\\s+new data', 'data\\s+not be shared']\n",
    "\n",
    "# Regex pattern to match any of the words\n",
    "pattern = '|'.join(re.escape(word) for word in words_to_check)\n",
    "\n",
    "# Check for the presence of the words\n",
    "pat_1['contains_words'] = pat_1['Section'].str.contains(pattern, case=False, regex=True)\n",
    "\n",
    "# Split the DataFrame into two sets \n",
    "pat_1_priv = pat_1[pat_1['contains_words']]\n",
    "pat_1_pub = pat_1[~pat_1['contains_words']]\n",
    "\n",
    "# Drop the temporary 'contains_words' column if you don't need it\n",
    "#pat_1.drop(columns=['contains_words'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d97c1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "563\n"
     ]
    }
   ],
   "source": [
    "print(len(pat_1_priv))\n",
    "print(len(pat_1_pub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "783da019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text containing links\n",
    "text = \"\"\"\n",
    "'Data and code availability statements Speciﬁcally, GES, PC and LiNGAM were implemented using the widely used R package pcalg , which is available at https://cran.r-project.org/web/packages/pcalg/. Notears method was implemented using Python available at https://github.com/xunzheng/notears. The proposed joint DAG method was implemented with Python and the code is available at https://github.com/gmeng92/joint-notears. The cohort data is accessible through the website (https://coins.trendscenter.org/) of COINS (COllaborative Infor-matics Neuroimaging Suite) database (Scott et al., 2011). Further information can also be found in (Gollub et al., 2013).',\n",
    "       'Data and code availability The data incorporated in the primary analysis were gathered from the public UK Biobank resource and will be made pub-licly available together with the code used to generate the data through the UK Biobank Returns Catalogue (https://biobank.ndph. ox.ac.uk/showcase/docs.cgi?id = 1). ABCD study data release 3.0 is available for approved researchers in NIMH Data Archive (NDA DOI:10.151.54/1,519,007). Code for conducting discovery and replication is available at https://github.com/robloughnan/MOSTest_generalization. Code for simu-lations is available at https://github.com/precimed/mostest/tree/master/simu.',\n",
    "       'Data/code availability statement Data and code are available upon request.',\n",
    "       'Data and code availability All data used in this project is from the Human Connec-tome Project (HCP) (www.humanconnectome.org). This data is publicly available to researchers who agree to the data use terms (www.humanconnectome.org/study/hcp-young-adult/data-use-terms). All HCP data may be downloaded through the ConnectomeDB (db.humanconnectome.org). ARCHI database is available upon request to Cyril Poupon at the email cyril.poupon@gmail.com.',\n",
    "       'Data and code availability statement All individual-level raw data used in this study cannot be shared because of the ethical code of Tokyo Metropolitan University. How-ever, the acquired metadata (e.g., group level activation maps) are available upon request. The corresponding author should be contacted by email for all data requests. The TDT (the toolbox for MVPA) and codes used for the cross-validation approach, cross-classiﬁcation ap-proach, and RSA are freely available online (https://drive.google.com/ﬁle/d/1kl6TMf7b3gndbkGDfbP2VhKcBxK_qnio/view).',\n",
    "       'Data and code availability No data were acquired for this study. The software required to gen-erate the vector spherical harmonics described in this paper is made freely available on the ﬁrst author’s GitHub page (https://github.com/tierneytim/OPM). The key function is spm_opm_vslm. Examples and tests can also be found on GitHub (https://github.com/tierneytim/OPM/blob/master/testScripts/testVSM.m).',\n",
    "       'Data availability and reproducibility All code ﬁles used in this manuscript are available at https://github. com/andyrevell/revellLab. All de-identiﬁed raw and processed data (ex-cept for patient MRI imaging) are available for download by following the links on the GitHub. Raw imaging data is available upon reasonable request from Principal Investigator K.A.D. iEEG snippets used speciﬁ-cally in this manuscript are also available, while full iEEG recordings are publicly available at https://www.ieeg.org. The Python environment for the exact packages and versions used in this study in contained in the 13 A.Y. Revell, A.B. Silva, T.C. Arnold et al. NeuroImage 254 (2022) 118986 environment directory within the GitHub. The QSIPrep docker container was used for DWI preprocessing. 5.',\n",
    "       'Data and code availability statement Data used in preparation of this article were obtained from the Hu-man Connectome Project, which are publicly available for download. Please refer Section 3.1 of the main manuscript. The codes for conduct-ing statistical analyses using CLEAN are currently available as a form of R package at https://github.com/junjypark/CLEAN.',\n",
    "       'Data and code availability statement The EEG/MEG data used in this study are openly available datasets, and are also available from the MRC Cognition And Brain Sciences’ data repository on request. The code used to analyze the EEG/MEG data is openly available on github: https://github.com/olafhauk/EEGMEG_ResolutionAtlas.',\n",
    "       'Data and code availability statements : The review summarizes data but does not contain new data.',\n",
    "       'Data availability The data and code presented here are available upon request to the corresponding author.'],\n",
    "\"\"\"\n",
    "\n",
    "# Define a regular expression pattern to match links\n",
    "pattern = r'\\b\\s*?:\\s*(?:http\\s*?://|www\\.)[^\\s)]*(?=[\\s)])'\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "# Print the extracted links\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cbdd69a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Data and code availability statements Speciﬁcally, GES, PC and LiNGAM were implemented using the widely used R package pcalg , which is available at https://cran.r-project.org/web/packages/pcalg/. Notears method was implemented using Python available at https://github.com/xunzheng/notears. The proposed joint DAG method was implemented with Python and the code is available at https://github.com/gmeng92/joint-notears. The cohort data is accessible through the website (https://coins.trendscenter.org/) of COINS (COllaborative Infor-matics Neuroimaging Suite) database (Scott et al., 2011). Further information can also be found in (Gollub et al., 2013).',\n",
       "       'Data and code availability The data incorporated in the primary analysis were gathered from the public UK Biobank resource and will be made pub-licly available together with the code used to generate the data through the UK Biobank Returns Catalogue (https://biobank.ndph. ox.ac.uk/showcase/docs.cgi?id = 1). ABCD study data release 3.0 is available for approved researchers in NIMH Data Archive (NDA DOI:10.151.54/1,519,007). Code for conducting discovery and replication is available at https://github.com/robloughnan/MOSTest_generalization. Code for simu-lations is available at https://github.com/precimed/mostest/tree/master/simu.',\n",
       "       'Data/code availability statement Data and code are available upon request.',\n",
       "       'Data and code availability All data used in this project is from the Human Connec-tome Project (HCP) (www.humanconnectome.org). This data is publicly available to researchers who agree to the data use terms (www.humanconnectome.org/study/hcp-young-adult/data-use-terms). All HCP data may be downloaded through the ConnectomeDB (db.humanconnectome.org). ARCHI database is available upon request to Cyril Poupon at the email cyril.poupon@gmail.com.',\n",
       "       'Data and code availability statement All individual-level raw data used in this study cannot be shared because of the ethical code of Tokyo Metropolitan University. How-ever, the acquired metadata (e.g., group level activation maps) are available upon request. The corresponding author should be contacted by email for all data requests. The TDT (the toolbox for MVPA) and codes used for the cross-validation approach, cross-classiﬁcation ap-proach, and RSA are freely available online (https://drive.google.com/ﬁle/d/1kl6TMf7b3gndbkGDfbP2VhKcBxK_qnio/view).',\n",
       "       'Data and code availability No data were acquired for this study. The software required to gen-erate the vector spherical harmonics described in this paper is made freely available on the ﬁrst author’s GitHub page (https://github.com/tierneytim/OPM). The key function is spm_opm_vslm. Examples and tests can also be found on GitHub (https://github.com/tierneytim/OPM/blob/master/testScripts/testVSM.m).',\n",
       "       'Data availability and reproducibility All code ﬁles used in this manuscript are available at https://github. com/andyrevell/revellLab. All de-identiﬁed raw and processed data (ex-cept for patient MRI imaging) are available for download by following the links on the GitHub. Raw imaging data is available upon reasonable request from Principal Investigator K.A.D. iEEG snippets used speciﬁ-cally in this manuscript are also available, while full iEEG recordings are publicly available at https://www.ieeg.org. The Python environment for the exact packages and versions used in this study in contained in the 13 A.Y. Revell, A.B. Silva, T.C. Arnold et al. NeuroImage 254 (2022) 118986 environment directory within the GitHub. The QSIPrep docker container was used for DWI preprocessing. 5.',\n",
       "       'Data and code availability statement Data used in preparation of this article were obtained from the Hu-man Connectome Project, which are publicly available for download. Please refer Section 3.1 of the main manuscript. The codes for conduct-ing statistical analyses using CLEAN are currently available as a form of R package at https://github.com/junjypark/CLEAN.',\n",
       "       'Data and code availability statement The EEG/MEG data used in this study are openly available datasets, and are also available from the MRC Cognition And Brain Sciences’ data repository on request. The code used to analyze the EEG/MEG data is openly available on github: https://github.com/olafhauk/EEGMEG_ResolutionAtlas.',\n",
       "       'Data and code availability statements : The review summarizes data but does not contain new data.',\n",
       "       'Data availability The data and code presented here are available upon request to the corresponding author.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_1_pub['Section'].loc[:10].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5b64e",
   "metadata": {},
   "source": [
    "The other section patterns: \n",
    "\n",
    "  (r'\\n?2\\.1\\.', r'\\n?2\\.2. | \\n\\n '),\n",
    "    (r'\\n?Resource | \\n?3\\.1\\.\\s*?\\n?', r'\\n?3\\.2.\\s*?| \\s*?\\n\\n '),\n",
    "    (r'\\n?Introduction\\s*?\\n? | \\s*?\\n?1\\.\\s*?\\n? ', r'\\s*?\\n?2\\.\\s*?\\n? | \\s*?\\n\\n '),\n",
    "    (r'\\n?Fig\\.\\d+ | \\n?Fig\\.\\d+\\.? | \\n?Figure \\d+', r'https?://[^\\s]+ | \\s*?\\n\\n '),\n",
    "    (r'\\n?Tab\\.\\d+ | \\n?Table \\d+\\.?', r'https?://[^\\s]+ | [\\w\\s-]+\\d{4} | \\s*?\\n\\n '),\n",
    "    (r'\\n?Abstract\\s*?\\n? | \\s*?\\n?1\\.\\s*?\\n? ', r'\\n?Introduction\\s*?\\n? | \\s*?\\n\\n ')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fcfc09",
   "metadata": {},
   "source": [
    "# Save datasets \n",
    "\n",
    "- Store the extracted datasets for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac298936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be94c596",
   "metadata": {},
   "source": [
    "# X. References\n",
    "\n",
    "- Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022)\n",
    "- Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_v1(article, section_patterns):\n",
    "    \"\"\"Get a section from a research paper. \n",
    "    \n",
    "    Parameters: \n",
    "    :param contents (): Text contents of the resaerch paper.\n",
    "    :param section_patterns (list): A list of strings to indicate the start and ends of the dataset section.\n",
    "    :return: returns substring of text region between section_header and a potential section_end. returns \"\" if it fails to find it.\n",
    "    \n",
    "    This function is adapted from Akkoç, A. (2023). PublicDatasets [Jupyter Notebook]. https://github.com/madprogramer/PublicDatasets (Original work published 2022) with some alterations.\n",
    "    \"\"\"\n",
    "    contents_lower = article.lower()  # Convert contents to lowercase\n",
    "    \n",
    "    \"\"\"THE CODE BELOW DOES WHAT I WANT IT TO DO\"\"\"\n",
    "    #test_start = r'data and code'\n",
    "    #test_end = r'availability'\n",
    "    #idx0 = contents_lower.find(test_start)\n",
    "    #if idx0 != -1:\n",
    "        #idxend = contents_lower.find(test_end, idx0)  # Start searching for test_end from idx0\n",
    "        #if idxend != -1:\n",
    "            #section = article[idx0:idxend]  # \"+ len(test_end)\" to include the end pattern in the extracted section\n",
    "            #print(section)\n",
    "\n",
    "    # If no match is found, return an empty string\n",
    "    return \"\" \n",
    "\n",
    "\n",
    "def get_content_v1(pdf_path, section_patterns):\n",
    "    \"\"\"Get a PDF. \n",
    "    This function is loosely interpreted from Sourget, T. (2023). TheoSourget/DDSA_Sourget: Repository used during my travel at the ITU of Copenhagen in March 2023 [Computer software]. https://github.com/TheoSourget/DDSA_Sourget\n",
    "    specifically: DDSA_Sourget/code/other/download_fulltext.ipynb, section '3. Check for dataset's organ in figures'.\n",
    "    \n",
    "    Parameters: \n",
    "    :param pdf_path (str): Path to the PDF file.\n",
    "    :param json_file_path (str): Path to the JSON file containing the DOIs of the relevant research articles. \n",
    "    \n",
    "    Returns: \n",
    "    :return: Extracted content or 'Editorial board' if not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        pdf_file = open(pdf_path, 'rb')\n",
    "        pdf_reader = pypdf.PdfReader(pdf_file)\n",
    "        # ORIGINAL \n",
    "        # Read the entire PDF content        \n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            print(page_text)\n",
    "            \n",
    "            # Search for the regex pattern in the page text\n",
    "            #if re.search(target_text_pattern, page_text, re.IGNORECASE):\n",
    "            #    print(f\"Found '{target_text_pattern}' on page {page_num + 1} of {pdf_path}\")\n",
    "\n",
    "            # Extract sections using the provided section patterns\n",
    "            content = get_section(page_text, section_patterns)\n",
    "            if content:\n",
    "                return content\n",
    "        pdf_file.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Try to open the PDF from the alternative directory\n",
    "            alternative_pdf_path = os.path.join(alternative_pdf_directory, os.path.basename(pdf_path))\n",
    "            pdf_file = open(alternative_pdf_path, 'rb')\n",
    "            return 'Editorial board'\n",
    "        except FileNotFoundError:\n",
    "            # If PDF is not found in the original or alternative directory, return 'Editorial board'\n",
    "            return 'Editorial board'\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
